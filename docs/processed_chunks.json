[
  {
    "id": "atlan_get_started_0",
    "text": "- üöÄ **Quick-start guide** Step-by-step onboarding - üîß **Secure agent** Enterprise-grade deployment options - üìù **Playbooks automation** Rule-based metadata updates at scale ## Core features - üîç **Find & understand data** Search, discover, and profile assets - üõ°Ô∏è **Govern & manage** Create data contracts & policies - üîå **Integrate** Automation, collaboration & other integrations ## Developer hub - ‚öôÔ∏è **Introductory walkthrough** Play with APIs in minutes - üíª **Client SDKs** Java, Python & more - üì¶ **Packages** Developer-built utilities and integrations **Atlan University** Get started with Atlan by building the right strategy and setting a strong foundation. **Atlan Security** A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures **Help and support** Find answers or contact our team for personalized assistance",
    "metadata": {
      "topic": "Atlan - Get started",
      "category": "General",
      "source_url": "https://docs.atlan.com/",
      "keywords": [
        "atlan",
        "general",
        "get started"
      ]
    }
  },
  {
    "id": "atlan_core_features_1",
    "text": "- üîç **Find & understand data** Search, discover, and profile assets - üõ°Ô∏è **Govern & manage** Create data contracts & policies - üîå **Integrate** Automation, collaboration & other integrations ## Developer hub - ‚öôÔ∏è **Introductory walkthrough** Play with APIs in minutes - üíª **Client SDKs** Java, Python & more - üì¶ **Packages** Developer-built utilities and integrations **Atlan University** Get started with Atlan by building the right strategy and setting a strong foundation. **Atlan Security** A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures **Help and support** Find answers or contact our team for personalized assistance",
    "metadata": {
      "topic": "Atlan - Core features",
      "category": "General",
      "source_url": "https://docs.atlan.com/",
      "keywords": [
        "atlan",
        "general",
        "core features"
      ]
    }
  },
  {
    "id": "atlan_developer_hub_2",
    "text": "- ‚öôÔ∏è **Introductory walkthrough** Play with APIs in minutes - üíª **Client SDKs** Java, Python & more - üì¶ **Packages** Developer-built utilities and integrations **Atlan University** Get started with Atlan by building the right strategy and setting a strong foundation. **Atlan Security** A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures **Help and support** Find answers or contact our team for personalized assistance",
    "metadata": {
      "topic": "Atlan - Developer hub",
      "category": "General",
      "source_url": "https://docs.atlan.com/",
      "keywords": [
        "atlan",
        "general",
        "developer hub"
      ]
    }
  },
  {
    "id": "set_up_databricks_0",
    "text": "On this page Atlan supports three authentication methods for fetching metadata from Databricks You can set up any of the following authentication methods: - Personal access token authentication - AWS service principal authentication - Azure service principal authentication ## Personal access token authentication ‚Äã Who can do this Check that you have _Admin_ and _Databricks SQL access_ for the Databricks workspace This is required for both cluster options described below If you don't have this access, contact your Databricks administrator. ### Grant user access to workspace ‚Äã To grant workspace access to the user creating a personal access token: 1 From the left menu of the account console, click **Workspaces** and then select a workspace to which you want to add the user. 2 From the tabs along the top of your workspace page, click the **Permissions** tab. 3 In the upper right of the _Permissions_ page, click **Add permissions**. 4 In the _Add permissions_ dialog, enter the following details: 1 For _User, group, or service principal_, select the user to grant access. 2 For _Permission_, click the dropdown and select workspace **User.** ### Generate a personal access token ‚Äã You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan To generate a personal access token: 1 From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click **User*Settings**. 2 Under the _Settings_ menu, click **Developer**. 3 On the _Developer_ page, next to _Access tokens_, click **Manage**. 4 On the _Access tokens_ page, click the **Generate new token** button. 5 In the _Generate new token_ dialog: 1 For _Comment_, enter a description of the token's intended use - for example, `Atlan crawler`. 2 For _Lifetime (days)_, consider removing the number This enables the token to be used indefinitely - it won't need to be refreshed. danger If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. 3 At the bottom of the dialog, click **Generate**. 6 Copy and save the generated token in a secure location, and then click **Done**. ### Select a cluster ‚Äã **Did you know?** Atlan recommends using serverless SQL warehouses for instant compute availability To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces You can set up personal access token authentication for your Databricks instance using one of the following cluster options: - Interactive cluster - SQL warehouse (formerly SQL endpoint) #### Interactive cluster ‚Äã To confirm an all-purpose interactive cluster is configured: 1 From the left menu of any page of your Databricks instance, click **Compute**. 2 Under the _All-purpose clusters_ tab, verify you have a cluster defined. 3 Click the link under the _Name_ column of the table to open your cluster. 4 Under the _Configuration_ tab, verify the _Autopilot options_ to _Terminate after ... minutes_ is enabled. 5",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_1",
    "text": "Click the link under the _Name_ column of the table to open your cluster. 4 Under the _Configuration_ tab, verify the _Autopilot options_ to _Terminate after ... minutes_ is enabled. 5 At the bottom of the _Configuration_ tab, expand the **Advanced options** expandable. 1 Under the _Advanced options_ expandable, open the **JDBC/ODBC** tab. 2 Confirm that all of the fields in this tab are populated, and copy them for use in crawling: _Server Hostname_, _Port_, and _HTTP Path_. #### SQL warehouse (formerly SQL endpoint) ‚Äã\") To confirm a SQL warehouse is configured: 1 From the left menu of any page of your Databricks instance, open the dropdown just below the _databricks_ logo and change to **SQL**. 2 From the refreshed left menu, click **SQL Warehouses**. 3 Click the link under the _Name_ column of the table to open your SQL warehouse. 4 Under the **Connection details** tab, confirm that all of the fields are populated and copy them for use in crawling: _Server hostname_, _Port_, and _HTTP path_. ## AWS service principal authentication ‚Äã Who can do this You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself You need the following to authenticate the connection in Atlan: - Client ID - Client secret ### Create a service principal ‚Äã You can create a service principal directly in your Databricks account or from a Databricks workspace. - Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. - Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. #### Identity federation enabled ‚Äã To create a service principal from your Databricks account, with identify federation enabled: 1 Log in to your Databricks account console as an account admin. 2 From the left menu of the account console, click **User management**. 3 From the tabs along the top of the _User management_ page, click the **Service principals** tab. 4 In the upper right of the _Service principals_ page, click **Add service principal**. 5 On the _Add service principal_ page, enter a name for the service principal and then click **Add**. 6 Once the service principal has been created, you can assign it to your identity federated workspace From the left menu of the account console, click **Workspaces** and then select a workspace to which you want to add the service principal. 7 From the tabs along the top of your workspace page, click the **Permissions** tab. 8 In the upper right of the _Permissions_ page, click **Add permissions**. 9 In the _Add permissions_ dialog, enter the following details: 1 For _User, group, or service principal_, select the service principal you created. 2",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_2",
    "text": "In the _Add permissions_ dialog, enter the following details: 1 For _User, group, or service principal_, select the service principal you created. 2 For _Permission_, click the dropdown and select workspace **User.** #### Identity federation disabled ‚Äã To create a service principal from a Databricks workspace, with identity federation disabled: 1 Log in to your AWS Databricks workspace as a workspace admin. 2 From the top right of your workspace, click your username, and then from the dropdown, click **Admin Settings**. 3 In the left menu of the _Settings_ page, under the _Workspace admin_ subheading, click **Identity and access**. 4 On the _Identity and access_ page, under _Management and permissions_, next to _Service principals_, click **Manage**. 5 In the upper right of the _Service principals_ page, click **Add service principal**. 6 In the _Add service principal_ dialog, click the **Add new** button. 7 For _New service principal display name_, enter a name for the service principal and then click **Add**. ### Create an OAuth secret for the service principal ‚Äã You need to create an OAuth secret to authenticate to Databricks REST APIs To create an OAuth secret for the service principal: 1 Log in to your Databricks account console as an account admin. 2 From the left menu of the account console, click **User management**. 3 From the tabs along the top of the _User management_ page, click the **Service principals** tab. 4 In the upper right of the _Service principals_ page, select the service principal you created. 5 On the service principal page, under _OAuth secrets_, click **Generate secret**. 6 From the _Generate secret_ dialog, copy the _Secret_ and _Client ID_ and store it in a secure location. danger Note that this secret is only revealed once during creation The client ID is the same as the application ID of the service principal. 7 Once you've copied the client ID and secret, click **Done**. ## Azure service principal authentication ‚Äã Who can do this You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself You need the following to authenticate the connection in Atlan: - Client ID (application ID) - Client secret - Tenant ID (directory ID) ### Create a service principal ‚Äã To use service principals on Azure Databricks, an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal To create a service principal: 01 Sign in to the Azure portal. 02 If you have access to multiple tenants, subscriptions, or directories, click the **Directories + subscriptions** (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. 03 In_Search resources, services, and docs_, search for and select **Microsoft Entra ID**. 04 Click**+ Add **and select** App registration**. 05",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_3",
    "text": "In_Search resources, services, and docs_, search for and select **Microsoft Entra ID**. 04 Click**+ Add **and select** App registration**. 05 For_Name_, enter a name for the application. 06 In the_Supported account types_section, select **Accounts in this organizational directory only (Single tenant)** and then click **Register**. 07 On the application page's_Overview_page, in the_Essentials_section, copy and store the following values in a secure location: - _Application (client) ID_ - _Directory (tenant) ID_ 08 To generate a client secret, within_Manage_, click **Certificates & secrets**. 09 On the_Client secrets_tab, click **New client secret**. 10 In the_Add a client secret_dialog, enter the following details: 11 For _Description_, enter a description for the client secret. 12 For_Expires_, select an expiry time period for the client secret and then click **Add**. 13 Copy and store the client secret's_Value_in a secure place. ### Add a service principal to your account ‚Äã To add a service principal to your Azure Databricks account: 1 Log in to your Azure Databricks account console as an account admin. 2 From the left menu of the account console, click **User management**. 3 From the tabs along the top of the _User management_ page, click the **Service principals** tab. 4 In the upper right of the _Service principals_ page, click **Add service principal**. 5 On the _Add service principal_ page, enter a name for the service principal. 6 Under _UUID_, paste the **Application (client) ID** for the service principal. 7 Click **Add**. ### Assign a service principal to a workspace ‚Äã To add users to a workspace using the account console, the workspace must be enabled for identity federation Workspace admins can also assign service principals to workspaces using the workspace admin settings page. #### Identity federation enabled ‚Äã To assign a service principal to your Azure Databricks account: 1 Log in to your Databricks account console as an account admin. 2 From the left menu of the account console, click **Workspaces** and then select a workspace to which you want to add the service principal. 3 From the tabs along the top of your workspace page, click the **Permissions** tab. 4 In the upper right of the _Permissions_ page, click **Add permissions**. 5 In the _Add permissions_ dialog, enter the following details: 1 For _User, group, or service principal_, select the service principal you created. 2 For _Permission_, click the dropdown to select workspace **User**. #### Identity federation disabled ‚Äã To assign a service principal to your Azure Databricks workspace: 1 Log in to your Azure Databricks workspace as a workspace admin. 2 From the top right of your workspace, click your username, and then from the dropdown, click **Admin Settings**. 3 In the left menu of the _Settings_ page, under the _Workspace admin_ subheading, click **Identity and access**. 4 On the _Identity and access_ page, under _Management and permissions_, next to _Service principals_, click **Manage**. 5 In the upper right of the _Service principals_ page, click **Add service principal**. 6 In the _Add service principal_ dialog, click the **Add new** button. 7",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_4",
    "text": "In the upper right of the _Service principals_ page, click **Add service principal**. 6 In the _Add service principal_ dialog, click the **Add new** button. 7 For _New service principal display name_, paste the _Application (client) ID_ for the service principal, enter a display name, and then click **Add**. ## Grant permissions to crawl metadata ‚Äã You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan To extract metadata, you can grant the BROWSE privilege, currently in public preview You no longer require the _Data Reader_ preset that granted the following privileges on objects in the catalog - `USE CATALOG`, `USE SCHEMA`, `EXECUTE`, `READ VOLUME`, and `SELECT` To grant permissions to a user or service principal: 1 Log in to your Databricks workspace as a workspace admin. 2 From the left menu of your workspace, click **Catalog**. 3 In the left menu of the _Catalog Explorer_ page, select the catalog you want to crawl in Atlan. 4 From the tabs along the top of your workspace page, click the **Permissions** tab and then click the **Grant** button. 5 In the _Grant on (workspace name)_ dialog, configure the following: 1 Under _Principals_, click the dropdown and then select the user or service principal. 2 Under _Privileges_, check the **BROWSE** privilege. 3 At the bottom of the dialog, click **Grant**. 6. (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. ### System tables extraction method ‚Äã To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse Follow these steps to extract metadata using system tables: 1 Create one of the following authentication methods: - Personal access token - AWS service principal - Azure service principal 2 Grant the following privileges to the identity you created: - `CAN_USE` on a SQL warehouse - `USE CATALOG` on `system` catalog - `USE SCHEMA` on `system.information_schema` - `SELECT` on the following tables: - `system.information_schema.catalogs` - `system.information_schema.schemata` - `system.information_schema.tables` - `system.information_schema.columns` - `system.information_schema.key_column_usage` - `system.information_schema.table_constraints` ## (Optional) Grant permissions to query and preview data ‚Äã Grant permissions to query and preview data\") danger Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method To grant permissions to query data and preview example data: 1 Log in to your Databricks workspace as a workspace admin. 2 From the left menu of your workspace, click **Catalog**. 3 In the left menu of the _Catalog Explorer_ page, select the catalog you want to query and preview data from in Atlan. 4 From the tabs along the top of your workspace page, click the **Permissions** tab and then click the **Grant** button. 5 In the _Grant on (workspace name)_ dialog, configure the following: 1 Under _Principals_, click the dropdown and then select the user or service principal. 2 Under _Privilege presets_, click the dropdown and then click **Data Reader** to enable read-only access to the catalog",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_5",
    "text": "Under _Principals_, click the dropdown and then select the user or service principal. 2 Under _Privilege presets_, click the dropdown and then click **Data Reader** to enable read-only access to the catalog Doing so automatically selects the following privileges - `USE CATALOG`, `USE SCHEMA`, `EXECUTE`, `READ VOLUME`, and `SELECT`. 3 At the bottom of the dialog, click **Grant**. 6. (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. ## (Optional) Grant permissions to import and update tags ‚Äã Grant permissions to import and update tags\") To import Databricks tags, you must have a Unity Catalog-enabled workspace and a SQL warehouse configured Atlan supports importing Databricks tags using system tables for all three authentication methods Once you have created a personal access token, an AWS service principal, or an Azure service principal, you will need to grant the following privileges: - `CAN_USE` on a SQL warehouse - `USE CATALOG` on `system catalog` - `USE SCHEMA` on `system.information_schema` - `SELECT` on the following tables: - `system.information_schema.catalog_tags` - `system.information_schema.schema_tags` - `system.information_schema.table_tags` - `system.information_schema.column_tags` To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges: - `APPLY TAG` on the object - `USE CATALOG` on the object's parent catalog - `USE SCHEMA` on the object's parent schema ## (Optional) Grant permissions to extract lineage and usage from system tables ‚Äã Grant permissions to extract lineage and usage from system tables\") You must have a Unity Catalog-enabled workspace to use system tables Atlan supports extracting the following for your Databricks assets using system tables: - lineage - usage and popularity metrics ### Enable system.access schema ‚Äã You need your account admin to enable the `system.access` schema using the SystemSchemas API This enables Atlan to extract lineage using system tables In Atlan, one Databricks connection corresponds to one metastore Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation: - List system schemas using the SystemSchemas API to check the status. - If enabled for any given schema, the state is `EnableCompleted` This confirms that the schema has been enabled for that specific metastore. - Atlan can only extract lineage using system tables when the state is marked as `EnableCompleted`. ### (Optional) enable `system.information_schema.table` ‚Äã To generate lineage with the target type set as `PATH` for a table, Atlan uses metadata from `system.information_schema.table` to resolve table paths and dependencies To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. #### Grant permissions ‚Äã Who can do this You must be a metastore admin, have the `MANAGE` privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions In Atlan, one Databricks connection corresponds to one metastore Repeat the following process for each metastore from which you want to extract lineage. 1",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_6",
    "text": "In Atlan, one Databricks connection corresponds to one metastore Repeat the following process for each metastore from which you want to extract lineage. 1 Open **Catalog Explorer** in your Databricks workspace. 2 Navigate to the catalog (for example, `main`) and then to the appropriate schema (for example, `sales`). 3 Click the **Permissions** tab. 4 Click **Grant**. 5 Enter the user or group name (principal). 6 Assign the following permissions: - `USAGE` on the catalog - `USAGE` on the schema - `SELECT` on each relevant table 7 Click **Grant** to apply the changes These privileges enable Atlan to read table definitions and other metadata from the metastore. ### (Optional) enable system.query schema ‚Äã enable system.query schema\") This is only required if you also want to extract usage and popularity metrics from Databricks You need your account admin to enable the `system.query` schema using the SystemSchemas API This enables Atlan to mine query history using system tables for usage and popularity metrics To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation If enabled for any given schema, the state is `EnableCompleted`. info üí™ **Did you know?** Can't grant `SELECT` permissions on the system tables in `system.access` and `system.query` Skip the previous steps and create cloned views in a separate catalog and schema See Create cloned views of system tables. ### Grant permissions ‚Äã Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods Once you have created a personal access token, an AWS service principal, or an Azure service principal, you will need to grant the following permissions: - `CAN_USE` on a SQL warehouse - `USE_CATALOG` on `system` catalog - `USE SCHEMA` on `system.access` schema - `USE SCHEMA` on `system.query` schema (tomine query history for usage and popularity metrics) - `SELECT` on the following tables: - `system.query.history`(to mine query history for usage and popularity metrics) - `system.access.table_lineage` - `system.access.column_lineage` You need to create a Databricks connection in Atlan for each metastore You can use the hostname of your Unity Catalog-enabled workspace as the _Host_ for the connection. info üí™ **Did you know?** Can't grant `SELECT` permissions on the system tables in `system.access` and `system.query` Skip the previous steps and create cloned views in a separate catalog and schema See Create cloned views of system tables. ### (Optional) Create cloned views of system tables ‚Äã Create cloned views of system tables\") When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema Follow these steps to set up cloned views: - Create a catalog and schema to store cloned views",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_7",
    "text": "See Create cloned views of system tables. ### (Optional) Create cloned views of system tables ‚Äã Create cloned views of system tables\") When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema Follow these steps to set up cloned views: - Create a catalog and schema to store cloned views Use meaningful and unique names‚Äîfor example, `atlan_cloned_catalog` and `atlan_cloned_schema`. - Create cloned views for the following system tables: - **Lineage tables** ```codeBlockLines_e6Vv CREATE OR REPLACE VIEW <cloned-catalog-name>.<cloned-schema-name>.column_lineage AS SELECT * FROM system.access.column_lineage; CREATE OR REPLACE VIEW <cloned-catalog-name>.<cloned-schema-name>.table_lineage AS SELECT * FROM system.access.table_lineage; ``` - Replace `<cloned-catalog-name>` and `<cloned-schema-name>` with the catalog and schema names used in your environment. - **Popularity metrics** ```codeBlockLines_e6Vv CREATE OR REPLACE VIEW <cloned-catalog-name>.<cloned-schema-name>.query_history AS SELECT * FROM system.query.history; ``` - Replace `<cloned-catalog-name>` and `<cloned-schema-name>` with the catalog and schema names used in your environment. #### Grant permissions ‚Äã Grant the following permissions to enable access to the cloned views: - `CAN_USE` on a SQL warehouse - `USE CATALOG` on the catalog (for example, `<cloned-catalog-name>`) - `USE SCHEMA` and `SELECT` on the schema (for example, `<cloned-catalog-name>.<cloned-schema-name>`) You must create a Databricks connection in Atlan for each metastore You can use the hostname of your Unity Catalog-enabled workspace as the _Host_ for the connection. ### Locate warehouse ID ‚Äã To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse To locate the warehouse ID: 1 Log in to your Databricks workspace as a workspace admin. 2 From the left menu of your workspace, click **SQL Warehouses**. 3 On the _Compute_ page, select the warehouse you want to use. 4 From the _Overview_ tab of your warehouse page, next to the _Name_ of your warehouse, copy the value for your SQL warehouse _ID_ For example, `example-warehouse (ID: 123ab4c5def67890)`, copy the value `123ab4c5def67890` and store it in a secure location. ## (Optional) Grant view permissions to access Databricks entities via APIs ‚Äã Grant view permissions to access Databricks entities via APIs\") Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines This information helps to understand which Databricks enitity was used to create a lineage between assets Use the steps below for each object type to grant **CAN VIEW** permission to the Databricks user or service principal configured in your integration: 1. **Notebook API**( `/api/2.0/workspace/list`): Grant **CAN VIEW** permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace For more information, see Manage Access Control Lists with Folders. 2. **Queries API**( `/api/2.0/sql/queries`): Grant **CAN VIEW** permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace For more information, see View Queries. 3. **Job API**( `/api/2.2/jobs/list`): Grant **CAN VIEW** permission on each job object directly Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_databricks_8",
    "text": "For more information, see View Queries. 3. **Job API**( `/api/2.2/jobs/list`): Grant **CAN VIEW** permission on each job object directly Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object For more information, see Control Access to a Job. 4. **Pipeline API**( `/api/2.0/pipelines`): Grant **CAN VIEW** permission on each Delta Live Tables (DLT) pipeline object directly For more information, see Configure Pipeline Permissions. ## (Optional) Grant permissions for views and materialized views ‚Äã Grant permissions for views and materialized views\") Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: 1 Log in to your Databricks workspace as a workspace admin. 2 From the left menu of your workspace, click **Catalog**. 3 In the _Catalog Explorer_, select the catalog you want to extract view definitions from and generate lineage for in Atlan. 4 From the tabs at the top, click the **Permissions** tab, and then click **Grant**. 5 In the **Grant on (workspace name)** dialog, configure the following: - Select the **user** or **service principal** under **Principals**. - Select the following privileges under **Privilege presets**: - `USE CATALOG` - `USE SCHEMA` - `SELECT` 6 Click **Grant** to apply the permissions. 7 Repeat steps 3‚Äì6 for each catalog you want to crawl in Atlan. **Did you know?** `SELECT` permission is required to extract the definitions of views and materialized views If you prefer not to grant `SELECT` at the catalog level, you can grant it on individual views and materialized views instead. ## (Optional) Grant permissions to mine query history ‚Äã Grant permissions to mine query history\") To mine query history using REST API, you will need to assign the `CAN MANAGE` permission on your SQL warehouses to the user or service principal To grant permissions to mine query history: 1 Log in to your Databricks workspace as a workspace admin. 2 From the left menu of your workspace, click **SQL Warehouses**. 3 On the _Compute_ page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click **Permissions**. 4 In the _Manage permissions_ dialog, configure the following: 1 In the _Type to add multiple users or groups_ field, search for and select a user or service principal. 2 Expand the _Can use_ permissions dropdown and then select **Can manage** This permission enables the service principal to view all queries for the warehouse. 3 Click **Add** to assign the `CAN MANAGE` permission to the service principal. - Personal access token authentication - AWS service principal authentication - Azure service principal authentication - Grant permissions to crawl metadata - (Optional) Grant permissions to query and preview data - (Optional) Grant permissions to import and update tags - (Optional) Grant permissions to extract lineage and usage from system tables - (Optional) Grant view permissions to access Databricks entities via APIs - (Optional) Grant permissions for views and materialized views - (Optional) Grant permissions to mine query history",
    "metadata": {
      "topic": "Set up Databricks",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
      "keywords": [
        "set",
        "databricks",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_0",
    "text": "On this page Who can do this You need your Snowflake administrator to run these commands - you may not have access yourself. ## Create user and role in Snowflake ‚Äã Create a role and user in Snowflake using the following commands: ### Create role ‚Äã Create a role in Snowflake using the following commands: ```codeBlockLines_e6Vv CREATE OR REPLACE ROLE atlan_user_role; GRANT OPERATE, USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_user_role; ``` - Replace `<warehouse-name>` with the default warehouse to use when running the Snowflake crawler Atlan requires the following privileges to: - `OPERATE` enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. - `USAGE` enables Atlan to show or list metadata from Snowflake This in turn enables the Snowflake crawler to run the `SHOW` query. ### Create a user ‚Äã Create a separate user to integrate into Atlan, using one of the following 3 options: #### With a public key in Snowflake ‚Äã See Snowflake's official guide for details on generating an RSA key-pair To create a user with a key-pair, replace the value for `rsa_public_key` with the public key and run the following: ```codeBlockLines_e6Vv CREATE USER atlan_user rsa_public_key='MIIBIjANBgkqh...' default_role=atlan_user_role default_warehouse='<warehouse-name>' display_name='Atlan' TYPE = 'SERVICE' ``` - Learn more about the `SERVICE` type property in Snowflake documentation. **Did you know?** Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure An empty passphrase results in workflow failures To generate an encrypted private key, omit the `-nocrypt` option Refer to Snowflake documentation to learn more. #### With a password in Snowflake ‚Äã **Did you know?** Snowflake recommends transitioning away from basic authentication using username and password Change to key-pair authentication for enhanced security For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method To create a user with a password, replace `<password>` and run the following: ```codeBlockLines_e6Vv CREATE USER atlan_user password='<password>' default_role=atlan_user_role default_warehouse='<warehouse-name>' display_name='Atlan' TYPE = 'LEGACY_SERVICE' ``` - Learn more about the `LEGACY_SERVICE` type property in Snowflake documentation. #### Managed through your identity provider (IdP) Private preview ‚Äã This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta: - Create a user in your identity provider (IdP) and use federated authentication in Snowflake. - The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. ### Grant role to user ‚Äã To grant the `atlan_user_role` to the new user: ```codeBlockLines_e6Vv GRANT ROLE atlan_user_role TO USER atlan_user; ``` ### Configure OAuth (client credentials flow) with Microsoft Entra ID ‚Äã with Microsoft Entra ID\") To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: 1 Follow Snowflake's documentation to: - Register a new application in Microsoft Entra ID - Collect the `client ID`, `tenant ID`, and `client secret` - Add the required API permissions 2",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_1",
    "text": "To create a user with a password, replace `<password>` and run the following: ```codeBlockLines_e6Vv CREATE USER atlan_user password='<password>' default_role=atlan_user_role default_warehouse='<warehouse-name>' display_name='Atlan' TYPE = 'LEGACY_SERVICE' ``` - Learn more about the `LEGACY_SERVICE` type property in Snowflake documentation. #### Managed through your identity provider (IdP) Private preview ‚Äã This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta: - Create a user in your identity provider (IdP) and use federated authentication in Snowflake. - The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. ### Grant role to user ‚Äã To grant the `atlan_user_role` to the new user: ```codeBlockLines_e6Vv GRANT ROLE atlan_user_role TO USER atlan_user; ``` ### Configure OAuth (client credentials flow) with Microsoft Entra ID ‚Äã with Microsoft Entra ID\") To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: 1 Follow Snowflake's documentation to: - Register a new application in Microsoft Entra ID - Collect the `client ID`, `tenant ID`, and `client secret` - Add the required API permissions 2 In Snowflake, create a security integration using the following: ```codeBlockLines_e6Vv CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '<AZURE_AD_ISSUER>' EXTERNAL_OAUTH_JWS_KEYS_URL = '<AZURE_AD_JWS_KEY_ENDPOINT>' EXTERNAL_OAUTH_AUDIENCE_LIST = ('<SNOWFLAKE_APPLICATION_ID_URI>') EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name'; ``` - Replace the placeholders with actual values from your Azure AD app: - `<AZURE_AD_ISSUER>` ‚Üí Your tenant's OAuth 2.0 issuer URL - `<AZURE_AD_JWS_KEY_ENDPOINT>` ‚Üí Azure JWKs URI - `<SNOWFLAKE_APPLICATION_ID_URI>` ‚Üí Application ID URI of the Azure app 3 Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: ```codeBlockLines_e6Vv CREATE USER oauth_svc_user WITH LOGIN_NAME = '<AZURE_AD_CLIENT_OBJECT_ID>' -- Use Azure client OBJECT ID DEFAULT_ROLE = <ROLE> DEFAULT_WAREHOUSE = <WAREHOUSE>; ``` 4 Grant the configured role to this user: ```codeBlockLines_e6Vv GRANT ROLE <ROLE> TO USER oauth_svc_user; ``` ## Choose metadata fetching method ‚Äã Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_2",
    "text": "Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: ```codeBlockLines_e6Vv CREATE USER oauth_svc_user WITH LOGIN_NAME = '<AZURE_AD_CLIENT_OBJECT_ID>' -- Use Azure client OBJECT ID DEFAULT_ROLE = <ROLE> DEFAULT_WAREHOUSE = <WAREHOUSE>; ``` 4 Grant the configured role to this user: ```codeBlockLines_e6Vv GRANT ROLE <ROLE> TO USER oauth_svc_user; ``` ## Choose metadata fetching method ‚Äã Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema You should choose one of these two methods to set up Snowflake: | | Account usage | Information schema | | --- | --- | --- | | **Overview** | Simplified grants but some limitations in functionality | Most comprehensive approach, more grant management required | | **Method** | Views in the `SNOWFLAKE` database that display object metadata and usage metrics for your account | System-defined views and table functions that provide extensive metadata for objects created in your account | | **Permissions** | User role and account, single grant for `SNOWFLAKE` database | User role and account, multiple grants per database | | **Data latency** | 45 minutes to 3 hours (varies by view) | None | | **Historical data retention** | 1 year | 7 days to 6 months (varies by view or table function) | | **Asset extraction** | `ACCOUNT_USAGE` schema | `INFORMATION_SCHEMA` schema | | **View lineage** | `ACCOUNT_USAGE` schema | `INFORMATION_SCHEMA` schema | | **Table lineage** | `ACCOUNT_USAGE` schema | `ACCOUNT_USAGE` schema | | **Tag import** | `ACCOUNT_USAGE` schema | `ACCOUNT_USAGE` schema | | **Usage and popularity** | `ACCOUNT_USAGE` schema | `ACCOUNT_USAGE` schema | | **Metadata extraction time** | Varies by warehouse size For example, 8 minutes for 10 million assets (recommended for extracting a large number of assets) | Varies by warehouse size For example, 2+ hours for 10 million assets | | **Extraction limitations** | External table location data, procedures, and primary and foreign keys | None | ## Grant permissions for account usage method ‚Äã danger If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed Once the workflow has succeeded, adjust the value to be twice the extraction time This method uses the views in `SNOWFLAKE.ACCOUNT_USAGE` (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan You can be more granular with permissions using this method, but there are limitations with this approach. ### To crawl assets, generate lineage, and import tags ‚Äã If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner, you need to grant the same permissions to import tags as required for crawling Snowflake assets",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_3",
    "text": "Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner, you need to grant the same permissions to import tags as required for crawling Snowflake assets Note that object tagging in Snowflake currently requires Enterprise Edition or higher. - To use the default `SNOWFLAKE` database and `ACCOUNT_USAGE` schema and also mine Snowflake's query history (for lineage), grant these permissions: ```codeBlockLines_e6Vv USE ROLE ACCOUNTADMIN; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role; ``` - The `ACCOUNTADMIN` role is required to grant privileges on the `SNOWFLAKE` database due to the following reasons: - By default, only the `ACCOUNTADMIN` role can access the `SNOWFLAKE` database. - To enable other roles to access the database and schemas and query the views, a user with the `ACCOUNTADMIN` role needs to grant `IMPORTED PRIVILEGES` on the `SNOWFLAKE` database to the desired roles. - To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role; ``` - Replace `<copied-database>` with the copied Snowflake database name. - Replace `<copied-schema>` with the copied Snowflake `ACCOUNT_USAGE` schema name. - The grants for the copied version can't be used on the original `SNOWFLAKE` database This is because Snowflake produces an error that granular grants can't be given to imported databases. - When using a cloned or copied version, verify that the table or view definition remains unchanged as in your `SNOWFLAKE` database If the format is different",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_4",
    "text": "This is because Snowflake produces an error that granular grants can't be given to imported databases. - When using a cloned or copied version, verify that the table or view definition remains unchanged as in your `SNOWFLAKE` database If the format is different For example, a column is missing and it no longer qualifies as a clone. ### To crawl streams ‚Äã To crawl streams, provide the following permissions: - **To crawl current streams:** ```codeBlockLines_e6Vv GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the Snowflake database name. - **To crawl future streams:** ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the Snowflake database name. ### (Optional) To preview and query existing assets ‚Äã To preview and query existing assets\") To query and preview data within assets that already exist in Snowflake, add these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) ### (Optional) To preview and query future assets ‚Äã To preview and query future assets\") To query and preview data within assets that may be created in the future in Snowflake, add these permissions",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_5",
    "text": "If the format is different For example, a column is missing and it no longer qualifies as a clone. ### To crawl streams ‚Äã To crawl streams, provide the following permissions: - **To crawl current streams:** ```codeBlockLines_e6Vv GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the Snowflake database name. - **To crawl future streams:** ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the Snowflake database name. ### (Optional) To preview and query existing assets ‚Äã To preview and query existing assets\") To query and preview data within assets that already exist in Snowflake, add these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) ### (Optional) To preview and query future assets ‚Äã To preview and query future assets\") To query and preview data within assets that may be created in the future in Snowflake, add these permissions Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. ## Grant permissions for information schema method ‚Äã This method uses views in the `INFORMATION_SCHEMA` schema in Snowflake databases to fetch metadata",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_6",
    "text": "For example, a column is missing and it no longer qualifies as a clone. ### To crawl streams ‚Äã To crawl streams, provide the following permissions: - **To crawl current streams:** ```codeBlockLines_e6Vv GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the Snowflake database name. - **To crawl future streams:** ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the Snowflake database name. ### (Optional) To preview and query existing assets ‚Äã To preview and query existing assets\") To query and preview data within assets that already exist in Snowflake, add these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) ### (Optional) To preview and query future assets ‚Äã To preview and query future assets\") To query and preview data within assets that may be created in the future in Snowflake, add these permissions Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. ## Grant permissions for information schema method ‚Äã This method uses views in the `INFORMATION_SCHEMA` schema in Snowflake databases to fetch metadata You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. ### To crawl existing assets ‚Äã Grant these permissions to crawl assets that already exist in Snowflake",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_7",
    "text": "Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. ## Grant permissions for information schema method ‚Äã This method uses views in the `INFORMATION_SCHEMA` schema in Snowflake databases to fetch metadata You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. ### To crawl existing assets ‚Äã Grant these permissions to crawl assets that already exist in Snowflake If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. - Grant permissions to crawl existing assets: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) - Grant permissions to crawl functions: ```codeBlockLines_e6Vv GRANT USAGE ON ALL FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) - For secure user-defined functions (UDFs), grant **OWNERSHIP** permissions to retrieve metadata: ```codeBlockLines_e6Vv GRANT OWNERSHIP ON FUNCTION <schema_name>.<udf_name> TO ROLE <role_name>; ``` Replace the placeholders with the appropriate values: - `<schema_name>`: The name of the schema that contains the user-defined function (UDF). - `<udf_name>`: The name of the secure UDF that requires ownership permissions. - `<role_name>`: The role that gets assigned ownership of the secure UDF. **Did you know?** The statements given on this page apply to all schemas, tables, and views in a database in Snowflake",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_8",
    "text": "You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. ### To crawl existing assets ‚Äã Grant these permissions to crawl assets that already exist in Snowflake If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. - Grant permissions to crawl existing assets: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) - Grant permissions to crawl functions: ```codeBlockLines_e6Vv GRANT USAGE ON ALL FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) - For secure user-defined functions (UDFs), grant **OWNERSHIP** permissions to retrieve metadata: ```codeBlockLines_e6Vv GRANT OWNERSHIP ON FUNCTION <schema_name>.<udf_name> TO ROLE <role_name>; ``` Replace the placeholders with the appropriate values: - `<schema_name>`: The name of the schema that contains the user-defined function (UDF). - `<udf_name>`: The name of the secure UDF that requires ownership permissions. - `<role_name>`: The role that gets assigned ownership of the secure UDF. **Did you know?** The statements given on this page apply to all schemas, tables, and views in a database in Snowflake If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. ### To crawl future assets ‚Äã To crawl assets that may be created in the future in Snowflake, add these permissions",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_9",
    "text": "If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. - Grant permissions to crawl existing assets: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) - Grant permissions to crawl functions: ```codeBlockLines_e6Vv GRANT USAGE ON ALL FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) - For secure user-defined functions (UDFs), grant **OWNERSHIP** permissions to retrieve metadata: ```codeBlockLines_e6Vv GRANT OWNERSHIP ON FUNCTION <schema_name>.<udf_name> TO ROLE <role_name>; ``` Replace the placeholders with the appropriate values: - `<schema_name>`: The name of the schema that contains the user-defined function (UDF). - `<udf_name>`: The name of the secure UDF that requires ownership permissions. - `<role_name>`: The role that gets assigned ownership of the secure UDF. **Did you know?** The statements given on this page apply to all schemas, tables, and views in a database in Snowflake If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. ### To crawl future assets ‚Äã To crawl assets that may be created in the future in Snowflake, add these permissions Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. - To grant permissions at a database level: ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON FUTURE FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_10",
    "text": "If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. ### To crawl future assets ‚Äã To crawl assets that may be created in the future in Snowflake, add these permissions Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. - To grant permissions at a database level: ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON FUTURE FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored To learn more, refer to Snowflake documentation. - To grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT REFERENCES ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database and `<schema-name>` with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) ### To mine query history for lineage ‚Äã To also mine Snowflake's query history (for lineage), add these permissions",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_11",
    "text": "Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. - To grant permissions at a database level: ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON FUTURE FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored To learn more, refer to Snowflake documentation. - To grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT REFERENCES ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database and `<schema-name>` with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) ### To mine query history for lineage ‚Äã To also mine Snowflake's query history (for lineage), add these permissions You can use either option: - To mine query history direct from Snowflake's internal tables: ```codeBlockLines_e6Vv USE ROLE ACCOUNTADMIN; GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role; ``` - To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<cloned-database>\" TO ROLE atlan_user_role; GRANT USAGE ON SCHEMA \"<cloned-database>\".\"<cloned-account-usage-schema>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN SCHEMA \"<cloned-database>\".\"<cloned-account-usage-schema>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN SCHEMA \"<cloned-database>\".\"<cloned-account-usage-schema>\" TO ROLE atlan_user_role; ``` - Replace `<cloned-database>` with the name of the cloned database, and `<cloned-account-usage-schema>` with the name of the cloned schema containing account usage details. - When using a cloned or copied version, verify that the table or view definition remains unchanged as in your `SNOWFLAKE` database If the format is different",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_12",
    "text": "You can use either option: - To mine query history direct from Snowflake's internal tables: ```codeBlockLines_e6Vv USE ROLE ACCOUNTADMIN; GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role; ``` - To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<cloned-database>\" TO ROLE atlan_user_role; GRANT USAGE ON SCHEMA \"<cloned-database>\".\"<cloned-account-usage-schema>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN SCHEMA \"<cloned-database>\".\"<cloned-account-usage-schema>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN SCHEMA \"<cloned-database>\".\"<cloned-account-usage-schema>\" TO ROLE atlan_user_role; ``` - Replace `<cloned-database>` with the name of the cloned database, and `<cloned-account-usage-schema>` with the name of the cloned schema containing account usage details. - When using a cloned or copied version, verify that the table or view definition remains unchanged as in your `SNOWFLAKE` database If the format is different For example, a column is missing and it no longer qualifies as a clone. ### (Optional) To preview and query existing assets ‚Äã To preview and query existing assets\") To query and preview data within assets that already exist in Snowflake, add these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) ### (Optional) To preview and query future assets ‚Äã To preview and query future assets\") To query and preview data within assets that may be created in the future in Snowflake, add these permissions",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_13",
    "text": "If the format is different For example, a column is missing and it no longer qualifies as a clone. ### (Optional) To preview and query existing assets ‚Äã To preview and query existing assets\") To query and preview data within assets that already exist in Snowflake, add these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) ### (Optional) To preview and query future assets ‚Äã To preview and query future assets\") To query and preview data within assets that may be created in the future in Snowflake, add these permissions Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_14",
    "text": "For example, a column is missing and it no longer qualifies as a clone. ### (Optional) To preview and query existing assets ‚Äã To preview and query existing assets\") To query and preview data within assets that already exist in Snowflake, add these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) ### (Optional) To preview and query future assets ‚Äã To preview and query future assets\") To query and preview data within assets that may be created in the future in Snowflake, add these permissions Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored To learn more, refer to Snowflake documentation. - To grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT SELECT ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database and `<schema-name>` with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. ### (Optional) To import Snowflake tags ‚Äã To import Snowflake tags\") Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_15",
    "text": "Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored To learn more, refer to Snowflake documentation. - To grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT SELECT ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database and `<schema-name>` with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. ### (Optional) To import Snowflake tags ‚Äã To import Snowflake tags\") Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema Note that object tagging in Snowflake currently requires Enterprise Edition or higher",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_16",
    "text": "To learn more, refer to Snowflake documentation. - To grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT SELECT ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` - Replace `<database-name>` with the database and `<schema-name>` with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. ### (Optional) To import Snowflake tags ‚Äã To import Snowflake tags\") Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema Note that object tagging in Snowflake currently requires Enterprise Edition or higher To import tags from Snowflake, grant these permissions: - To use the default `SNOWFLAKE` database and `ACCOUNT_USAGE` schema and also mine Snowflake's query history (for lineage), grant these permissions: ```codeBlockLines_e6Vv USE ROLE ACCOUNTADMIN; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role; ``` - The `ACCOUNTADMIN` role is required to grant privileges on the `SNOWFLAKE` database due to the following reasons: - By default, only the `ACCOUNTADMIN` role can access the `SNOWFLAKE` database. - To enable other roles to access the database and schemas and query the views, a user with the `ACCOUNTADMIN` role needs to grant `IMPORTED PRIVILEGES` on the `SNOWFLAKE` database to the desired roles. - To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role; ``` - Replace `<copied-database>` with the copied Snowflake database name. - Replace `<copied-schema>` with the copied Snowflake `ACCOUNT_USAGE` schema name. - The grants for the copied version can't be used on the original `SNOWFLAKE` database This is because Snowflake produces an error that granular grants can't be given to imported databases. ### (Optional) To push updated tags to Snowflake ‚Äã To push updated tags to Snowflake\") To push tags updated for assets in Atlan to Snowflake, grant these permissions: ```codeBlockLines_e6Vv GRANT APPLY TAG ON ACCOUNT TO ROLE <role-name>; ``` You can learn more about tag privileges from Snowflake documentation. ### (Optional) To crawl dynamic tables ‚Äã To crawl dynamic tables\") Atlan currently supports fetching metadata for dynamic tables using the `MONITOR` privilege Refer to Snowflake documentation to learn more",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_17",
    "text": "This is because Snowflake produces an error that granular grants can't be given to imported databases. ### (Optional) To push updated tags to Snowflake ‚Äã To push updated tags to Snowflake\") To push tags updated for assets in Atlan to Snowflake, grant these permissions: ```codeBlockLines_e6Vv GRANT APPLY TAG ON ACCOUNT TO ROLE <role-name>; ``` You can learn more about tag privileges from Snowflake documentation. ### (Optional) To crawl dynamic tables ‚Äã To crawl dynamic tables\") Atlan currently supports fetching metadata for dynamic tables using the `MONITOR` privilege Refer to Snowflake documentation to learn more To crawl existing dynamic tables from Snowflake: - Grant permissions at a database level: ```codeBlockLines_e6Vv GRANT MONITOR ON ALL DYNAMIC TABLES IN DATABASE \"<DATABASE_NAME>\" TO ROLE atlan_user_role; ``` - Grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT MONITOR ON ALL DYNAMIC TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` To crawl future dynamic tables from Snowflake: - Grant permissions at a database level: ```codeBlockLines_e6Vv GRANT MONITOR ON FUTURE DYNAMIC TABLES IN DATABASE \"<DATABASE_NAME>\" TO ROLE atlan_user_role; ``` - Grant permissions at a schema level: ```codeBlockLines_e6Vv GRANT MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role; ``` Replace `<database-name>` with the database and `<schema-name>` with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) ### (Optional) To crawl Iceberg tables ‚Äã To crawl Iceberg tables\") Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method To crawl Iceberg tables from Snowflake, grant the following permissions: - To crawl existing Iceberg tables in Snowflake: ```codeBlockLines_e6Vv GRANT REFERENCES ON ALL ICEBERG TABLES IN DATABASE <database-name> TO ROLE atlan_user_role; ``` - To crawl future Iceberg tables in Snowflake: ```codeBlockLines_e6Vv GRANT REFERENCES ON FUTURE ICEBERG TABLES IN DATABASE <database-name> TO ROLE atlan_user_role; ``` - To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: ```codeBlockLines_e6Vv GRANT USAGE ON INTEGRATION <integration-name> TO ROLE atlan_user_role; ``` danger You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs You must also grant permissions to all the catalogs you want to crawl in Atlan individually. ### (Optional) To crawl Snowflake stages ‚Äã To crawl Snowflake stages\") Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_snowflake_18",
    "text": "You must also grant permissions to all the catalogs you want to crawl in Atlan individually. ### (Optional) To crawl Snowflake stages ‚Äã To crawl Snowflake stages\") Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES To crawl stages from Snowflake: - Grant `USAGE` and `READ` privileges on all existing stages at the database level: ```codeBlockLines_e6Vv GRANT USAGE ON ALL STAGES IN DATABASE <database_name> TO ROLE atlan_user_role; GRANT READ ON ALL STAGES IN DATABASE <database_name> TO ROLE atlan_user_role; ``` - Replace `<database_name>` with the name of your Snowflake database - Replace `<atlan_user_role>` with the role you've granted Atlan to use for crawling. - Grant `USAGE` and `READ` privileges on all future stages at the database level: ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE STAGES IN DATABASE <database_name> TO ROLE atlan_user_role; GRANT READ ON FUTURE STAGES IN DATABASE <database_name> TO ROLE atlan_user_role; ``` - Replace `<database_name>` with the name of your Snowflake database - Replace `<atlan_user_role>` with the role you've granted Atlan to use for crawling. ## Allowlist the Atlan IP ‚Äã If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist Please raise a support ticket from within Atlan, or submit a request. (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) - Create user and role in Snowflake - Choose metadata fetching method - Grant permissions for account usage method - Grant permissions for information schema method - Allowlist the Atlan IP",
    "metadata": {
      "topic": "Set up Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
      "keywords": [
        "set",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_is_atlan_0",
    "text": "On this page We are a modern data workspace that makes collaboration among diverse users like business, analysts, and engineers easier, increasing efficiency and agility in data projects We started out as a data team, solving social good problems using data science We built Atlan for ourselves over the course of 200 data projects, which included India's national data platform used by the prime minister and monitoring the Sustainable Development Goals with the United Nations Atlan helped us build India's national data platform with an 8-member team, making it the fastest project of its kind to go live in just 12 months - instead of the projected three years. ## Why we built Atlan ‚Äã Data teams can be diverse: analysts, scientists, engineers, and business users Diverse people with diverse tools and skillsets mean diverse DNAs All of it led to chaos, which made our Slack channels look like this... !mceclip0.png ### We call this \"collaboration overheard\" ‚Äã We knew we couldn't scale like this, there had to be a better way We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams We then experimented for two years and across 200 data projects to create our own idea of what makes data teams successful We call this DataOps. ## How Atlan helps data teams ‚Äã With Atlan, analyst teams at Unilever have shipped 100+ additional data projects per quarter, while data science teams at Samsung have saved 50% of their time. ### Create self-service ecosystems by reducing dependencies ‚Äã Atlan makes all your data assets easily discoverable No more Slack messages like \"Where's that dataset?\" or long email threads for approvals With Atlan, you can simply Cmd+K your way to the right data asset. **Key capabilities**: discovery and search, Visual Query Builder, saved queries, READMEs ### Improve the agility of your data team ‚Äã Data practitioners spend 30-50% of their time finding and understanding data Atlan cuts that time by 95% Your data team will be shipping 2-3 times more projects in no time. **Key capabilities**: visibility of data quality tests and observability alerts, automated lineage, Atlan AI ### Promote governance and a sustainable data culture ‚Äã Don't lose sleep trying to figure out if your sensitive data is secure Build ecosystems of trust, make your team happy, and let Atlan manage governance and security behind the scenes. **Key capabilities**: tag sensitive data, granular access control, data products - Why we built Atlan - How Atlan helps data teams",
    "metadata": {
      "topic": "What is Atlan?",
      "category": "General",
      "source_url": "https://docs.atlan.com/get-started/what-is-atlan",
      "keywords": [
        "what",
        "atlan",
        "general"
      ]
    }
  },
  {
    "id": "tags_0",
    "text": "# Tags ## A ‚Äã - access-control19 - administration1 - agreements1 - ai1 - air-gapped1 - airflow1 - aiven1 - alerts9 - always on1 - amazon5 - amazon-athena1 - amazon-s34 - analysis1 - analytics8 - announcements2 - anomalo1 - apache3 - api73 - app3 - assests1 - asset-profile5 - assets3 - astronomer1 - atlan132 - Atlan MCP1 - atlan-ai2 - atlas1 - attributes1 - authentication52 - auto-re-attachment2 - automation24 - aws4 - aws lambda1 - azure4 ## B ‚Äã - bigid4 - bigquery1 - browse1 - browser-extension3 - business intelligence15 - business-terms17 ## C ‚Äã - calculation-view1 - capabilities10 - cassandra1 - catalog7 - categorization1 - classification1 - cloud1 - cloudera2 - collaboration4 - communication2 - compliance1 - composer1 - configuration19 - confluent2 - connect1 - connections1 - connectivity63 - connector61 - connectors290 - contracts1 - cosmosdb1 - cratedb6 - crawl227 - crawling1 - crm1 - custom metadata1 ## D ‚Äã - dagster4 - dashboards6 - data255 - data assets1 - data factory1 - data integration5 - data lake1 - data quality20 - data transformation1 - data warehouse4 - data-catalog6 - data-domains1 - data-flow1 - data-lineage57 - data-modeling1 - data-models1 - data-products1 - data-sources13 - database21 - databricks5 - datastax1 - dbt1 - definitions18 - dependencies9 - dimensions1 - discovery9 - documentation117 - domains1 - domo1 - downstream-impact8 - dynamodb1 ## E ‚Äã - ecc1 - embedded1 - enrichment1 - erd1 - erp6 - etl5 - event hubs1 ## F ‚Äã - faq31 - faq-administration2 - faq-automation2 - faq-connections6 - faq-connectors1 - faq-discovery1 - faq-governance6 - faq-insights9 - faq-integrations17 - faq-lineage16 - faq-metadata1 - faq-platform2 - faq-security1 - faq-support1 - fivetran1 ## G ‚Äã - gcp2 - gcs3 - get-started3 - glossary18 - glue1 - google2 - google-gcs3 - governance18 - graphql25 - groups1 - guides1 ## H ‚Äã - help1 - hive1 - how-to1 - hybrid bi5 ## I ‚Äã - ibm cognos1 - identity management3 - impact-analysis57 - impala2 - insights1 - integration122 - integrations13 - inventory-reports1 ## J ‚Äã - jira2 ## K ‚Äã - kafka5 ## L ‚Äã - lambda1 - lineage70 - logs1 - looker1 ## M ‚Äã - matillion1 - messaging6 - metabase1 - metadata11 - metadata-extractor1 - metrics2 - microsoft4 - microsoft teams1 - microstrategy1 - migration1 - mode1 - model13 - mongodb1 - monitoring9 - monte carlo1 - msk1 - mwaa1 - mysql1 ## N ‚Äã - nosql3 - notifications9 ## O ‚Äã - observability3 - offline1 - on-premises1 - openlineage5 - opentelemetry1 - operations1 - oracle1 - orchestration11 - organization2 - otlp1 ## P ‚Äã - permissions20 - playbooks1 - policies1 - popularity1 - postgresql1 - power bi1 - preflight-checks1 - prestosql1 - privacy4 - project management3 - properties2 ## Q ‚Äã - qlik sense2 - query1 - query history1 - quick-start3 - quicksight1 ## R ‚Äã - redash1 - redpanda1 - redshift1 - reference7 - relational4 - releases1 - reporting1 - requests1 - rest-api25 - roles2 - rules2 ## S ‚Äã - s34 - s4hana1 - salesforce10 - sap2 - sap-ecc1 - sap-hana1 - sap-s4hana1 - schema6 - schema registry1 - schema-drift5 - schema-monitoring5 - scim2 - scopes1 - search2 - security19 - servicenow2 - setup46 - siem1 - sigma1 - sisense1 - slack5 - smtp2 - snowflake10 - soda1 - spark1 - spreadsheets1 - sql4 - sql server1 - sso2 - stewardship1 - storage8 - support2 - synapse1 ## T ‚Äã - tableau2 - tags1 - teams1 - teradata1 - terminology1 - thoughtspot1 - trino1 - troubleshooting7 ## U ‚Äã - upstream-dependencies12 - usage1 - user groups2 - users1 ## V ‚Äã - visualization6 ## W ‚Äã - webhooks4 - workflow11 - workflows3",
    "metadata": {
      "topic": "Tags",
      "category": "General",
      "source_url": "https://docs.atlan.com/tags",
      "keywords": [
        "tags",
        "general"
      ]
    }
  },
  {
    "id": "how_are_product_updates_deploy_0",
    "text": "On this page As the control plane for your entire data stack, Atlan's product release strategy is centered on being fast, iterative, and adaptive to the needs of our partners All the while ensuring that product releases are mature and stable to truly empower users Atlan follows a standard CI/CD model - continuous integration (CI) and continuous delivery (CD) - for deploying product updates This includes bug fixes and making improvements regularly To minimize risks and enhance security, each component in Atlan is tracked and released individually When developers make changes, a series of automated validations are initiated, including building component images, running tests, and verifying that no vulnerabilities have been introduced Peer reviews further ensure high-quality standards Only after the changes have been approved, the new release will go through the following stages: 1 Staging - for pre-production testing 2 Beta - for limited user testing 3 Production - live environment release Atlan's engineering team actively reviews any issues with new releases to extract valuable insights and learning opportunities toward offering a better service Accordingly, Atlan recommends that any connections or integrations you would like to make must be approved by Atlan unless already tested and documented Note that any unauthorized integration may lead to breakdown of service and impact end user experience Significant feature updates are always highlighted within the application You can also subscribe to our Shipped channel for updates on product rollouts. ## View product updates ‚Äã View product updates A View product updates !View product updates from here!(Optional) Subscribe to our Shipped channel for product update emails Illustrative examples reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms To view product updates in Atlan: 1 From the top right of any screen in Atlan, click the **gift box** icon. 2 In the _Product Updates_ sidebar, view the latest product releases You can also: - (Optional) Click **Subscribe** to subscribe to our Shipped channel. - (Optional) Type a keyword in the search bar to search for specific product releases. - View product updates",
    "metadata": {
      "topic": "How are product updates deployed?",
      "category": "General",
      "source_url": "https://docs.atlan.com/get-started/faqs/how-are-product-updates-deployed",
      "keywords": [
        "how",
        "are",
        "product",
        "updates",
        "deployed",
        "general"
      ]
    }
  },
  {
    "id": "administrators_0",
    "text": "On this page ## User management ‚Äã User management is a critical part of data governance Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. ### Add and manage users from the admin center ‚Äã Admin center A Admin center !Navigate to the admin center from here!Step 3 Illustrative example reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms It's super simple to invite and remove users from Atlan from the _Admin center_ You can also manage existing users by adding them to groups, changing their roles, or set up SSO, SCIM, and SMTP configurations. ### Manage access control from the governance center ‚Äã Governance center A Governance center !Navigate to the governance center from here!Define personas for asset curation Illustrative example reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms The _Governance center_ is where you can build access control mechanisms to manage user access. _Personas_ allow you to group users into teams, such as `Financial Analysts` or `Cloud Engineers`, and set policies based on the access those personas should have. _Purposes_ are where you can build policies based on the actions or access that a user might need For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data This is a best practice for data governance Once you set these policies, Atlan will enforce them throughout your users' experience This means that users who don't have access to a particular type of data will not be able to see it. _Governance workflows_ help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. ## Asset profile ‚Äã Asset Profile A Asset Profile Replay !Step 1 reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains You can think of the asset profile as the TL;DR about your data. ## Glossary ‚Äã Glossary A Glossary Replay !Step 1 reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms The Atlan glossary is a rich tool for defining and organizing your data terminology to improve transparency and share knowledge No need to ask around for what a column name means The glossary functions as a source of truth for teams to understand their data assets Start keeping all your definitions in one searchable place",
    "metadata": {
      "topic": "Administrators",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins",
      "keywords": [
        "administrators",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "administrators_1",
    "text": "The glossary functions as a source of truth for teams to understand their data assets Start keeping all your definitions in one searchable place The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: - Owners of your data, so you know who to ask for clarification. - Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. - Linked assets that are relevant to the term, so you can explore other helpful material. **Did you know?** The glossary helps power Atlan's powerful search tool, so tagging and defining assets are critical to helping your team find what they need. ## Discovery ‚Äã Search and Discovery A Search and Discovery Sound ON 00:00/00:04 1‚úó reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms We rely on search bars to find things in almost every corner of the internet Atlan uses a similar search tool to help you explore your data assets The discovery tool is Atlan's powerful in-platform search, powered by the terms and descriptions you've added to your data assets Here are a few of the things that make Atlan's discovery awesome: - Every attribute of your data is searchable in Atlan - saved SQL queries, schemas, links, and more This lets you search far and wide to find exactly what you need. - Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. - Search assets from just about any page in Atlan using **Cmd/Ctrl+K** or by clicking **Search assets across Atlan** at the top of any page. - Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. - Sort by popularity to quickly discover what assets your teammates are using every day. - User management - Asset profile - Glossary - Discovery",
    "metadata": {
      "topic": "Administrators",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins",
      "keywords": [
        "administrators",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "api_authentication_0",
    "text": "On this page Who can do this You will need to be an admin user to create a bearer token However, you can share the token with anyone to give them programmatic access Generate API token A Generate API token !Begin by navigating to the admin center!API tokens are managed from here Interactive walkthrough To create a bearer token: 1 From the left menu of any screen, click **Admin**. 2 Under _Workspace_, click **API tokens**. 3 In the upper right of the _API tokens_ table, click the **Generate API token** button and enter the following details: 1 For _Name_, enter a name for your API token - for example, the system or application that will use this token. 2. (Optional) For _Description_, enter a description for your API token - for example, its intended use You can also add or change the description later. 3. (Optional) For _Personas_, select any asset-level permissions you want to give to the token You can also add these later. 4. (Optional) For _Collections_, select any query collections you want to provide access to the token You can also add these later. 5. (Optional) If you would like the token to be temporary, for _Expiry_, choose the time after which the token should automatically become invalid. 6 At the bottom right, click the **Save** button. 7. (Optional) If and when you no longer need your API token, on an active token's row, click the trash icon to delete your API token and then click the **Delete** button to confirm deletion. danger Remember to copy or download the token now - this is your only opportunity to do so. (If you've already forgotten, just delete the API token and create a new one.) ## Use the bearer token ‚Äã You must authenticate all requests to Atlan's APIs You can authenticate your requests by sending the following header: ```codeBlockLines_e6Vv Authorization: Bearer <token> ``` So, for example, if the API token you copied had the value `eyJhbGciOi...`, you would use the header: ```codeBlockLines_e6Vv Authorization: Bearer eyJhbGciOi... ``` danger Note that the value of the `Authorization` header is the combination of the word `Bearer`, a space, and then the token's value The token copied from Atlan does not include this `Bearer` prefix. ## Token permissions ‚Äã By default, each API token will have the permissions of an admin user, _without_ connection admin privileges This means the token is able to: - Call administrative API endpoints For example, to create users and groups. - Call governance API endpoints For example, to create governance objects like tags, custom metadata, personas and purposes. danger The API token will only be able to access connections (and assets within them) that the token itself created Even connections with _All Admins_ set as connection admins will not be accessible by the token, without a persona assigned to the token To provide access to any connections and assets, you need to add one or more personas to the token that have access to that connection's assets",
    "metadata": {
      "topic": "API authentication",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/get-started/references/api-authentication",
      "keywords": [
        "api",
        "authentication",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "our_3_pro_tips_for_saving_time_0",
    "text": "On this page There are a lot of incredibly time-saving functionalities built into Atlan Here are three pro tips that you can use to save time for you and your team! ## Search from anywhere in Atlan ‚Äã Atlan supports powerful, intelligent search Start your search from anywhere in Atlan to find the data asset you're looking for You can also enter a keyword in the search bar and filter your results by a specific type of asset For instance, enter the keyword `product` in the search bar and then click the **Table** filter to view table results for your searched keyword. **Did you know?** You can use **Cmd/Ctrl + K** to open the search page from _anywhere_ in Atlan Search from anywhere in Atlan A Search from anywhere in Atlan !Start your search from anywhere in Atlan!Enter a keyword in the search bar Interactive walkthrough ## Collaborate with your team ‚Äã Use Atlan's seamless Slack or Microsoft Teams integration to share updates with your team Don't waste time adding the correct link - just click the **Slack** or **Teams** icon in Atlan to post directly on a Slack or Microsoft Teams channel You can also add your message as a resource to the asset Collaborate with your team A Collaborate with your team !Share on Slack from anywhere in Atlan!Select the Teams channel where you'd like to share!Select the Slack channel where you'd like to share Interactive walkthrough ## Refine your search using filters ‚Äã Use the filters in the left panel of the _Assets_ workspace to refine your search Check out our three best pro tips for using filters Refine your search using filters A Refine your search using filters !Certificates can be used to quickly filter data assets!Choose your preferred certification option Interactive walkthrough ### Search by certification status ‚Äã Certificates in Atlan are a useful tool for letting your team know the status of an asset You can search by certification status to only find the assets you need. ### Search by owners or data contributors ‚Äã Want to find out what your teammate is working on Looking for a dashboard that your manager is delegating to you Use the _Owners_ filter to search by asset owner in Atlan You can also search by other user roles To do this, you first need to set up custom metadata properties to represent those other roles Then you can filter by those custom metadata properties to find assets with any Atlan user who has that role for the asset. ### Search by lineage ‚Äã Want to only find assets with a completed lineage You can filter your search by assets that have data lineage using the _Has lineage_ filter listed under _Properties_ Try using these 3 pro tips to make Atlan a powerful part of your team's tool kit. - Search from anywhere in Atlan - Collaborate with your team - Refine your search using filters",
    "metadata": {
      "topic": "Our 3 pro tips for saving time with Atlan",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/get-started/references/our-3-pro-tips-for-saving-time-with-atlan",
      "keywords": [
        "our",
        "pro",
        "tips",
        "saving",
        "time",
        "atlan",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "product_release_stages_0",
    "text": "On this page The following release stages are part of the lifecycle of an Atlan feature: | Stage | Access | Status | | --- | --- | --- | | Alpha | By invitation only to few data teams | Experimental | | Private preview | By invitation only to a larger cohort of data teams | Near production grade | | Public preview | Open to all data teams | Production grade with known limitations | | General availability | Open to all data teams | Production grade | ## Atlan release stages ‚Äã While features are stable, Atlan may on occasion deprecate legacy functionality to support product updates. ### Alpha ‚Äã - Enabled for select customers on request to test, validate, and provide feedback. - Experimental in nature, user feedback critical in preparing for a wider release. - Formal support and documentation unavailable. - Standard SLAs, terms, and warranties don't apply. ### Private preview ‚Äã - Enabled for a larger cohort of customers on request to test, validate, and provide feedback. - Partially production ready - breaking changes may occur. - Informal support and documentation from product and engineering teams. - Standard SLAs, terms, and warranties don't apply. - To get access to private preview features, raise a support request. ### Public preview ‚Äã - Enabled for all customers, usually includes a `Preview` label in the product or requires admins to enable the feature from _Labs_ in the admin center. - Production grade is high, but with known limitations. - Formal support and documentation provided on best available basis. - Feature release may be announced on the Shipped channel. - Standard SLAs, terms, and warranties do not apply. ### General availability ‚Äã - Enabled for all users. - Production grade is of the highest quality to ensure the best possible user experience. - Formal support and documentation available. - Feature release announced on the Shipped channel. - Standard SLAs, terms, and warranties apply. - Atlan release stages - Alpha - Private preview - Public preview - General availability",
    "metadata": {
      "topic": "Product release stages",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/get-started/references/product-release-stages",
      "keywords": [
        "product",
        "release",
        "stages",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "the_dataops_culture_code_0",
    "text": "On this page We experimented for two years, across 200 data projects, to create our own viewpoint of what makes data teams successful We've codified these learnings into what we call the \"DataOps Culture Code\" The data team is the most interdisciplinary team in any organization. **Data Scientists, Analysts, Engineers, Business Users** These are diverse people, with diverse tools, skillsets, and DNA All doing diverse things Sometimes they're asking open-ended questions to get to the bottom of ‚Äúwhy‚Äù, just like a scientist in a research lab Sometimes they're working on scaling petabyte-sized data processing systems, like a software engineer Add to all this the living and breathing thing that is... **data** Unlike code or design, it's constantly changing. ## How do you make a data team successful? ‚Äã There's no easy answer We started as a data team ourselves, on a quest to make ourselves as agile as we could We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams We then experimented for two years, across 200 data projects, to create our own idea of what makes data teams successful These principles are the foundation of everything we build at Atlan. ## The DataOps Culture Code ‚Äã ### ü§ù It‚Äôs a team sport, and collaboration is key ‚Äã Data teams will always have a variety of roles, each with their own skills, favorite tools and DNA Embrace the diversity, and create mechanisms for effective collaboration. ### üóÑ Treat data, code, models and dashboards as assets. ‚Äã All data assets - from code and models to data and dashboards - are assets, and they should be treated like assets. - Assets should be easily discoverable. - Assets should be maintained. - Assets should be easily reusable. ### üöÄ Optimize for agility ‚Äã In today‚Äôs world, as business needs evolve rapidly, data teams need to be a step ahead, not deluged with three months of backlog Constantly measure your team‚Äôs velocity, and invest in foundational initiatives to improve cycle times. - Reduce dependencies between business, analysts and engineers. - Enable a documentation-first culture. - Automate whatever is repetitive. ### üë• Create systems of trust ‚Äã With the inherent diversity of data teams, it's all too easy to misunderstand other team members' roles But that creates trust deficiencies - especially when things go wrong Intentionally create systems of trust in your team. - Make everyone‚Äôs work accessible and discoverable to break down \"tool\" silos. - Create transparency in data pipelines and lineage so everyone can see and troubleshoot issues. - Set up monitoring and alerting systems to proactively know when things break. ### üñáÔ∏è Create a plug-and-play data stack ‚Äã The data ecosystem will rapidly evolve The tools, technology and infrastructure you use today will (and should) be different from the tools you use two years later",
    "metadata": {
      "topic": "The DataOps Culture Code",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/get-started/references/the-dataops-culture-code",
      "keywords": [
        "dataops",
        "culture",
        "code",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "the_dataops_culture_code_1",
    "text": "Intentionally create systems of trust in your team. - Make everyone‚Äôs work accessible and discoverable to break down \"tool\" silos. - Create transparency in data pipelines and lineage so everyone can see and troubleshoot issues. - Set up monitoring and alerting systems to proactively know when things break. ### üñáÔ∏è Create a plug-and-play data stack ‚Äã The data ecosystem will rapidly evolve The tools, technology and infrastructure you use today will (and should) be different from the tools you use two years later Your data stack should allow your team to experiment and innovate as technology evolves, without creating lock-ins. - Embrace tools that are open and extensible. - Leverage a strong metadata layer to tie diverse tooling together. ### ‚ú® User experience defines adoption velocity ‚Äã Employees at Airbnb famously said, \"Designing the interface and user experience of a data tool should not be an afterthought.\" Without good user experience, the best tools or most thoughtful processes won't be adopted in your team Invest in user experience, even for internal tools It will define adoption velocity! - Invest in simple and intuitive tools. - Software shouldn't need training programs. - How do you make a data team successful? - The DataOps Culture Code",
    "metadata": {
      "topic": "The DataOps Culture Code",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/get-started/references/the-dataops-culture-code",
      "keywords": [
        "dataops",
        "culture",
        "code",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "what_is_atlan_0",
    "text": "On this page We are a modern data workspace that makes collaboration among diverse users like business, analysts, and engineers easier, increasing efficiency and agility in data projects We started out as a data team, solving social good problems using data science We built Atlan for ourselves over the course of 200 data projects, which included India's national data platform used by the prime minister and monitoring the Sustainable Development Goals with the United Nations Atlan helped us build India's national data platform with an 8-member team, making it the fastest project of its kind to go live in just 12 months - instead of the projected three years. ## Why we built Atlan ‚Äã Data teams can be diverse: analysts, scientists, engineers, and business users Diverse people with diverse tools and skillsets mean diverse DNAs All of it led to chaos, which made our Slack channels look like this... !mceclip0.png ### We call this \"collaboration overheard\" ‚Äã We knew we couldn't scale like this, there had to be a better way We borrowed the principles of Agile from product teams, DevOps from engineering teams, and Lean Manufacturing from supply chain teams We then experimented for two years and across 200 data projects to create our own idea of what makes data teams successful We call this DataOps. ## How Atlan helps data teams ‚Äã With Atlan, analyst teams at Unilever have shipped 100+ additional data projects per quarter, while data science teams at Samsung have saved 50% of their time. ### Create self-service ecosystems by reducing dependencies ‚Äã Atlan makes all your data assets easily discoverable No more Slack messages like \"Where's that dataset?\" or long email threads for approvals With Atlan, you can simply Cmd+K your way to the right data asset. **Key capabilities**: discovery and search, Visual Query Builder, saved queries, READMEs ### Improve the agility of your data team ‚Äã Data practitioners spend 30-50% of their time finding and understanding data Atlan cuts that time by 95% Your data team will be shipping 2-3 times more projects in no time. **Key capabilities**: visibility of data quality tests and observability alerts, automated lineage, Atlan AI ### Promote governance and a sustainable data culture ‚Äã Don't lose sleep trying to figure out if your sensitive data is secure Build ecosystems of trust, make your team happy, and let Atlan manage governance and security behind the scenes. **Key capabilities**: tag sensitive data, granular access control, data products - Why we built Atlan - How Atlan helps data teams",
    "metadata": {
      "topic": "What is Atlan?",
      "category": "General",
      "source_url": "https://docs.atlan.com/get-started/what-is-atlan\\",
      "keywords": [
        "what",
        "atlan",
        "general"
      ]
    }
  },
  {
    "id": "authentication_and_authorizati_0",
    "text": "On this page Atlan supports the following authentication methods: ## Basic authentication ‚Äã Atlan initially comes with basic or username-password authentication Admins can invite new users to log into Atlan When a new user opens the invitation link, they will be able to set up their user profile, including username and password However, Atlan does not recommend using basic authentication Instead, admins should configure and enforce SSO authentication. ## SSO authentication ‚Äã ### SSO using SAML 2.0 ‚Äã Atlan supports single sign-on (SSO), allowing admins to configure SSO authentication Atlan currently supports the following SSO providers: - Azure AD - Google - JumpCloud - Okta - OneLogin - Custom IdP ### SSO using SCIM ‚Äã System for Cross-domain Identity Management (SCIM) provisioning works in combination with SSO Atlan currently supports SCIM provisioning for the following SSO providers: - Azure AD - Okta ## Authorization ‚Äã ### Role-based access control (RBAC) ‚Äã\") Atlan implements role-based access control (RBAC) to ensure that users have the minimum level of access required to perform their tasks Access rights are assigned based on roles, and users are granted permissions according to their responsibilities A system owner or an authorized party must approve any additional permissions Atlan adheres to the principle of least privilege, ensuring that users are only granted the level of access necessary to perform their job functions. ### User access review (UAR) ‚Äã\") Atlan recommends that admins perform access reviews of users, admins, and service accounts on a quarterly basis to ensure that appropriate access levels are maintained Access reviews should also be documented. ## Identity and access management ‚Äã For centralized management of groups and users, Atlan uses granular access policies Admins can define policies to control both which actions a user can take and against which assets These can be as broad as entire databases down to individual columns Organizations can even build policies based on asset classification This opens up the ability to restrict access to sensitive data like Personally Identifiable Information (PII) - an essential feature in the GDPR era Atlan denies access by default, and explicit denials override any grants You can even deny admin users access to assets, if you want. ### Roles ‚Äã You must assign every user in Atlan a user role These control basic levels of access. ### Groups ‚Äã You can also add users to groups Groups provide a more maintainable mechanism for applying access controls. ### Policies ‚Äã You can define access policies for both users and groups Through these policies you can restrict which users can take which actions on which assets For example, you can set up tags such as PII and apply this to data assets like tables You can also configure the tag to propagate downstream to any columns or tables created from them You can then define access controls based on these tags to restrict access to tagged assets",
    "metadata": {
      "topic": "Authentication and authorization",
      "category": "General",
      "source_url": "https://docs.atlan.com/platform/concepts/authentication-and-authorization",
      "keywords": [
        "authentication",
        "authorization",
        "general"
      ]
    }
  },
  {
    "id": "data_and_metadata_persistence_0",
    "text": "On this page Atlan is a fully virtualized solution that does not involve moving _data_ from existing storage layers Atlan crawls _metadata_ from upstream data sources and stores it in a secure VPC (virtual private cloud) Atlan pushes any queries to existing processing layers For example, directly to your database, warehouse, or a processing layer such as Athena or Presto on top of blob storage So the _data_ itself stays put - Atlan does not move or store it Not sure on the difference between _data_ and _metadata_ Try our helpful primer. ## Data previews and queries ‚Äã Atlan gives users the ability to see sample data previews for a data asset as well as the results for any queries run on Atlan In both cases, Atlan pushes the request upstream to the data source, and shows a 100-row sample of the result to Atlan users Atlan does not cache any of this data So each time a user previews or queries data, it is re-queried from the source Every time a user runs a query, Atlan streams query results in batches directly from your data source Since the data is streamed in real-time from the data source, there is no need to persist the query results in Atlan's cache or storage layer This ensures that the data displayed is always up to date and accurate, eliminating the need for storing intermediate query results. ## Metadata storage ‚Äã Atlan stores the metadata it collects and creates in applications and databases within the VPC This includes: - asset metadata - user data ### Asset metadata ‚Äã Atlan stores asset metadata, including lineage, in: - Apache Atlas, a graph database layer that stores entity relationships and attributes - Elasticsearch, to optimize search on the product - Cassandra, as the persistence back-end ### User data ‚Äã Atlan stores data on users, roles, and groups in its own PostgreSQL database Keycloak uses this information for access and identity management Atlan hashes all sensitive fields like passwords and stores them securely Any user data transmitted over the internet is SSL-encrypted over HTTPS. - Data previews and queries - Metadata storage",
    "metadata": {
      "topic": "Data and metadata persistence",
      "category": "General",
      "source_url": "https://docs.atlan.com/platform/concepts/data-and-metadata-persistence",
      "keywords": [
        "data",
        "metadata",
        "persistence",
        "general"
      ]
    }
  },
  {
    "id": "encryption_and_key_management_0",
    "text": "On this page Atlan has adopted global industry standards in security practices and solutions Amazon S3 server-side encryption secures the S3 bucket launched by Atlan Atlan uses AES-256 as the SSE algorithm in the S3 bucket All the EBS (Elastic Block Storage) launched by Atlan is encrypted Atlan uses encrypted storage classes to provision persistent volumes to the microservices running inside the Kubernetes cluster. ## Key and credential management ‚Äã Atlan uses HashiCorp Vault to manage the following: - Keys - Vault manages encryption keys to encrypt sensitive data at rest and in transit. - Secrets - Vault encrypts and securely stores secrets such as API keys, tokens, and credentials. - Passwords - passwords are hashed and stored encrypted. ## Data in transit ‚Äã Atlan uses standard encryption to protect data in transit Atlan uses hypertext transfer protocol secure (HTTPS) for secure communication when data is in transit This protocol is encrypted using Transport Layer Security (TLS) Two-factor authentication (2FA) is also supported for accessing resources. ## Data at rest ‚Äã Data-At-Rest Encryption (DARE) is the encryption of data stored in different storage components and not moving through networks. ### Cloud storage ‚Äã Atlan encrypts the data at rest in different cloud resources like volumes and cloud storage using cloud provider-managed keys. - Amazon S3 - Atlan uses server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data at rest in Amazon S3 This encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode (AES-GCM) to encrypt all uploaded objects. - Azure Blob Storage - Atlan uses Microsoft-managed keys to encrypt the data at rest in Azure Blob Storage This encryption uses 256-bit AES encryption and is FIPS 140-2 compliant. - Google Cloud Storage - Atlan uses Google-managed keys to encrypt the data at rest in Google Cloud Storage This encryption uses AES-256 using Galois/Counter Mode (GCM) to encrypt all uploaded objects. ### Volumes ‚Äã Volumes are used by the StatefulSet running in the tenants These volumes are encrypted at rest by the cloud provider-managed keys. - Amazon Web Services (AWS) - Atlan uses the default Amazon Elastic Block Store (EBS) encryption for encrypting the data at rest in all the volumes Amazon EBS encrypts volume with a data key using industry-standard AES-256 data encryption. - Microsoft Azure - Atlan uses Azure Storage encryption, which uses server-side encryption (SSE), for encrypting the data at rest in all the volumes Data in Azure managed disks is encrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant. - Google Cloud Platform (GCP) - Atlan uses Google-managed encryption to encrypt the data at rest in all the volumes This encryption uses the Advanced Encryption Standard (AES) algorithm, AES-256. ### Over the internet ‚Äã Communication between the client and Atlan public endpoints is always conducted over hypertext transfer protocol secure (HTTPS) HTTPS is encrypted in order to increase the security of data transfer",
    "metadata": {
      "topic": "Encryption and key management",
      "category": "General",
      "source_url": "https://docs.atlan.com/platform/concepts/encryption-and-key-management",
      "keywords": [
        "encryption",
        "key",
        "management",
        "general"
      ]
    }
  },
  {
    "id": "high_availability_and_disaster_0",
    "text": "On this page ## High availability (HA) ‚Äã\") Atlan uses Amazon Elastic Kubernetes Service (EKS) for high availability (HA) Amazon EKS runs and scales the Kubernetes control plane across multiple AWS Availability Zones to ensure high availability It automatically scales control plane instances based on load, detects and replaces unhealthy control plane instances, and patches the control plane The benefits of using this concept are: - Scalability and reliability help the system remain stable - Promotes self-healing to ensure that containers are running in a healthy state - Handles node failures gracefully - Auto-scaling enables automated cluster creation ### Application HA ‚Äã Atlan ensures application HA through the following: - Multiple replicas for both stateless and stateful applications - Load balancing with services - Rolling updates to maintain the availability threshold - Pod-to-node distribution to ensure that critical application pods are running on dedicated nodes - Usage of inter-Availability Zone (AZ) data transfers to avoid single-zone failure impact ## Disaster recovery (DR) ‚Äã\") Atlan follows industry best practices for disaster recovery Atlan uses Argo Workflows for orchestration to successfully implement a disaster recovery strategy and reduce production downtime, so that business impact is minimized in the event of an outage If a disaster is detected, the Disaster Assessment Team - comprising key stakeholders from IT, platform, operations, and support - will be promptly notified through Atlan‚Äôs established communication channels The team will conduct a thorough evaluation to determine the extent of the damage and prioritize remediation based on an internal list of critical services and applications In case of a disaster, a tenant will be recreated, and the following actions performed to restore the tenant: - Onboard a new tenant with no data and a different domain. - Use Argo Workflows to restore the data from last backup. - Scale down the previous tenant. - Change the domain of the new tenant to that of the previous one. - Update the Cloudflare record with the load balancer of the newly onboarded tenant All the aforementioned action items are automated The entire process of restoring all the components of a tenant from backup takes around 3-4 hours In case of data loss for any particular component, it can also be recovered from the last backup Here are a few parameters that help reduce downtime and expedite the process of disaster recovery: ### Infrastructure ‚Äã Single-tenant SaaS is the default deployment option for most Atlan users In this model, Atlan manages the infrastructure needs and ensures that all instances are spread across multiple Availability Zones (AZ) in each AWS Region where the user instance is deployed Availability Zones are multiple, isolated locations within a single AWS Region Multi-AZ deployments provide enhanced availability for instances within a single AWS Region With multi-AZ, your data is synchronously replicated to standby in a different Availability Zone. danger Atlan currently does not support multi-region deployment. ### Atlan service overview ‚Äã The diagram below illustrates the relationships and communication flows between each service",
    "metadata": {
      "topic": "High availability and disaster recovery (HA/DR)",
      "category": "General",
      "source_url": "https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr",
      "keywords": [
        "high",
        "availability",
        "disaster",
        "recovery",
        "general"
      ]
    }
  },
  {
    "id": "high_availability_and_disaster_1",
    "text": "Multi-AZ deployments provide enhanced availability for instances within a single AWS Region With multi-AZ, your data is synchronously replicated to standby in a different Availability Zone. danger Atlan currently does not support multi-region deployment. ### Atlan service overview ‚Äã The diagram below illustrates the relationships and communication flows between each service The bottom-most layer shows the services that are entirely independent, such as Cassandra, Postgres, and more Most of the other services depend on these to function. ![](https://ask.atlan.com/hc/article_attachments/7887835262749) ### Backups and restore ‚Äã Atlan runs an automated daily backup of each tenant By default, the backup is scheduled at 3:00 AM UTC, configurable as per the requirement of an organization The backup of each tenant is stored in its respective cloud storage The backups are encrypted at rest by the default cloud provider key This key uses the Advanced Encryption Standard (AES) 256 algorithm Since Atlan uses the cloud provider key, the key is rotated by the cloud provider Atlan controls access to the cloud storage where the backup is stored, and only provides access in case of troubleshooting an issue Each backup process captures a full backup of all the data, with no incremental backups being performed Atlan also monitors the backup to ensure that backups are not skipped Alerts are generated in case a backup run fails for the support team to examine the issue The lifecycle policy for backups in the cloud provider is set to 15 days, which means Atlan will retain backups for all the components for 15 days Backups of the following components are taken on a daily basis: - Argo Workflows - Elasticsearch - Cassandra - Redis - Postgres Atlan can restore a single component in case of data corruption for any single point of failure, such as a metastore and its components like Elasticsearch and Cassandra It is also possible to do a full-cluster restore in case of an unintended operation or a data loss or corruption event. **Did you know?** Argo Workflows powers all the backup and restore packages in Atlan It includes a retry mechanism in case of any errors while completing the steps in the workflow It also sends alerts in case of entire package failure as part of observability. ### Migration ‚Äã Atlan has an easy process to migrate the application to other AWS Regions In case of total region outage and the need for migrating an instance to another region or account, this migration activity will be performed via Atlan‚Äôs backup and restore packages. ## RTO, RPO, and retention ‚Äã Greater RTOs and RPOs as well as system recovery are crucial for ensuring that multiple mission-critical applications are quickly restored",
    "metadata": {
      "topic": "High availability and disaster recovery (HA/DR)",
      "category": "General",
      "source_url": "https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr",
      "keywords": [
        "high",
        "availability",
        "disaster",
        "recovery",
        "general"
      ]
    }
  },
  {
    "id": "high_availability_and_disaster_2",
    "text": "It also sends alerts in case of entire package failure as part of observability. ### Migration ‚Äã Atlan has an easy process to migrate the application to other AWS Regions In case of total region outage and the need for migrating an instance to another region or account, this migration activity will be performed via Atlan‚Äôs backup and restore packages. ## RTO, RPO, and retention ‚Äã Greater RTOs and RPOs as well as system recovery are crucial for ensuring that multiple mission-critical applications are quickly restored It is now possible to minimize the impact of a disruption and perform a recovery within a few hours of an outage. - Atlan carries out a daily backup of all critical services once every 24 hours, so in a worst case scenario provides an RPO of 24 hours. - For all critical applications, RTO is less than 3 hours. - Atlan retains daily backups for 15 days. ## Post-recovery validation ‚Äã The following post-recovery actions are performed: - Post restoration, Atlan conducts data integrity checks to ensure that the restored data is accurate and complete. - Atlan performs system tests to confirm that all components of the tenant are functioning correctly after restoration. - High availability (HA) - Disaster recovery (DR) - RTO, RPO, and retention - Post-recovery validation",
    "metadata": {
      "topic": "High availability and disaster recovery (HA/DR)",
      "category": "General",
      "source_url": "https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr",
      "keywords": [
        "high",
        "availability",
        "disaster",
        "recovery",
        "general"
      ]
    }
  },
  {
    "id": "generate_har_files_and_console_0",
    "text": "On this page Atlan is built on REST APIs, so you can see the requests being sent by the UI to the API gateway through your browser's developer console To help Atlan troubleshoot issues, you may be asked to create and send a HAR file and browser console logs: - **Console or network logs** frequently provide critical error details that are required to determine the underlying cause of the issue or bug that you are experiencing. - **HAR files** include all the network traffic from when you started recording, including sensitive information like passwords and private keys To avoid including such information in a HAR file, Atlan recommends using a text editor to manually edit the file and remove any sensitive content before sending it to Atlan support. ## Generate in Google Chrome ‚Äã 01 Launch _Google Chrome_ and navigate to the relevant webpage in Chrome. 02 In the upper-right corner of your screen, click the three vertical dots. 03 From the _Chrome_ menu, click **More Tools** and then click **Developer Tools**. 04 In the left menu, click on the **Network** tab and then select **Fetch/XHR** as your filtering option. 05 Under the _Network_ tab, click **Preserve log** A red circle will appear on the left to show that you have started recording the network log If you see a black circle, click on it to turn it red and start recording. 06 To allow Google Chrome to record the interaction between the browser and website, refresh the page Confirm if you can view new entries in the console. 07 Next to the red circle icon, click the circle slash icon to clear logs. 08 Replicate the issue that you experienced in the browser For example, if it's a particular click that triggers an error, perform this action so that the error is recorded in the console. 09 Once the page has loaded, navigate to the _Console_ tab and right-click in the console box Select **Save as...** and enter a name for the file. 10 Return to the _Network_ tab and right-click the element that triggered an error This is typically marked in red in the console Click **Save as HAR with content** to save the HAR file. 11. (Recommended) To remove any sensitive information from the HAR file: 12 Open the HAR file in a text editor of your choice. 13 Search for all instances of passwords and replace the values with a placeholder value such as `**` For example, in the following sample password content, you can replace `<YourPrivateKey>` and `<YourPrivateKeyPassword>` with placeholder values: ```codeBlockLines_e6Vv \"headersSize\": -1, \"bodySize\": 3762, \"postData\": { \"mimeType\": \"application/json\", \"text\": \"{\"host\":\"<YourHostName>\",\"port\":<port>,\"authType\":\"keypair\",\"username\":\"PRD_SDCSHOP_TU_ETL_CATALOG\",\"password\":\"-----BEGIN ENCRYPTED PRIVATE KEY-----<YourPrivateKey> -----END ENCRYPTED PRIVATE KEY-----\",\"extra\":{\"role\":\"GLOBAL_TALEND_CATALOG_DBADMIN\",\"warehouse\":\"PRD_SDCSHOP_ETL\",\"private_key_password\":\"<YourPrivateKeyPassword>\"},\"connectorConfigName\":\"atlan-connectors-snowflake\",\"query\":\"show atlan schemas\",\"schemaExcludePattern\":[\"INFORMATION_SCHEMA\"]}\" } } ``` 14 Save the HAR file. 15 Upload the HAR file and browser console log to your Atlan support ticket. - Generate in Google Chrome",
    "metadata": {
      "topic": "Generate HAR files and console logs",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/platform/how-tos/generate-har-files-and-console-logs",
      "keywords": [
        "generate",
        "har",
        "files",
        "console",
        "logs",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "atlan_architecture_0",
    "text": "On this page Atlan is a cloud-first solution Single-tenant SaaS is the recommended deployment model Atlan currently supports hosting tenants on the following cloud platforms: ### Amazon Web Services (AWS) ‚Äã\") !Atlan Architecture Diagrams v3 - AWS v3.png ### Microsoft Azure ‚Äã !Atlan Architecture Diagrams v3 - Azure v3.png ### Google Cloud Platform (GCP) ‚Äã\") !Atlan Architecture Diagrams v3 - GCP v3.png The components of Atlan are isolated, across both compute and data For more details, see How are resources isolated? ## Platform components ‚Äã - Kong is an API gateway It handles rate limiting and token verification on all incoming API requests. - Apache Keycloak is an identity and access management component It manages everything to do with users, login, SSO and so on. - Heracles is Atlan's API service It houses the business logic used by the frontend and APIs to interact with other platform components. - PostgreSQL is a SQL database Many services on the platform use it for storage. - HashiCorp Vault is a secret manager It stores sensitive credentials provided by the user. - Apache Ranger is the policy engine It provides fine-grained access control over data in the metastore. - Argo Workflows is a workflow orchestrator for k8s It runs and manages long-running jobs in a container and k8s-native fashion. - Admission Controller is a k8s admission controller It performs certain actions when Argo Workflows are updated such as workflow alerts. - Metastore stores metadata as data in a graph store It is based on Apache Atlas and has fine-grained access control on top. - Apache Zookeeper manages consensus and coordination for the metastore services. - Elasticsearch indexes data and drives search functionality. - Apache Cassandra is an object-oriented database used to store the metastore's data. - Apache Kafka is an event stream It enables event-driven use cases across the platform. - Heka is Atlan's SQL component It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite. - Redis is a cache layer used by Heracles. ## Platform management components ‚Äã - Velero performs cluster backups. - Kibana explores and filters log data stored in Elasticsearch. - Fluent Bit is a logging and metrics processor It parses and pushes logs from pods to various destinations. - Elasticsearch stores and indexes logs. ## Central components ‚Äã - Zenduty is used for incident response Alerts are sent when something goes wrong in one of the clusters. - Argo CD is used for continuous deployment Changes in git repositories lead to upgrades in the clusters. - Github Actions update the Docker container images as part of the development process. - Sendgrid is used to send emails. - The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. - Alertmanager sends alerts generated by metrics stored in Prometheus. - Grafana provides observability dashboards. - VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database",
    "metadata": {
      "topic": "Atlan architecture",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/atlan-architecture",
      "keywords": [
        "atlan",
        "architecture",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "cloud_logging_and_monitoring_0",
    "text": "On this page Atlan exports IAM service event logs in the OpenTelemetry Protocol (OTLP) specification and securely delivers them to the Amazon S3 or Google Cloud Storage (GCS) bucket of your organization This enables you to monitor login events and integrate logs with security information and event management (SIEM) systems for real-time security monitoring and alerts. ## Key aspects ‚Äã - Log format and structure: The OTLP format ensures seamless integration with SIEM systems, and logs are organized by date and event type Logs are stored in a compressed format in your organization's preferred object storage (S3 or GCS) Once uncompressed, the logs will be available in a JSON file format containing multiple log entries. - Each file is saved for an hour in the following folder structure in gzip: ```codeBlockLines_e6Vv /year=YYYY/month=MM/day=DD/hour=HH/logs_<rnd-9-digit-int>.json.gz ``` - The JSON file structure is as follows: ```codeBlockLines_e6Vv { \"resourceLogs\": [ { \"resource\": { \"attributes\": [ // k8s metadata ] }, \"scopeLogs\": [ { \"scope\": {}, \"logRecords\": [ { \"timeUnixNano\": \"1725861538220747913\", \"observedTimeUnixNano\": \"1726071786185095727\", \"body\": { \"stringValue\": \"//redacted logline\" }, \"traceId\": \"\", \"spanId\": \"\" } ] }, {...}, ] }, { \"resource\": {...}, \"scopeLogs\": [...] } ] } ``` - Secure delivery- Logs are encrypted in transit and at rest, with mechanisms to validate data integrity. - Customer access: Logs are easily accessible through S3 or GCS, allowing for a flexible monitoring and alerting setup. ## Enabling event logs in AWS ‚Äã !AWS Keycloak Event Logs ### Prerequisites ‚Äã - Enable bucket versioning Both source and destination buckets must have versioning enabled See AWS documentation. - Customer-provided bucket details: account ID, bucket name, and region. - Atlan will use these details to create an IAM role on the Atlan side and then provide you with the bucket policy to be attached. - Once you have confirmed that the bucket policy has been attached, Atlan will complete the final step of setting up log replication Atlan support will complete the configuration on the Atlan side You will need to attach the following policy to your destination bucket: ```codeBlockLines_e6Vv { \"Version\": \"2012-10-17\", \"Id\": \"\", \"Statement\": [ { \"Sid\": \"Set-permissions-for-objects\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"<Atlan Role ARN>\" }, \"Action\": [ \"s3:ReplicateObject\", \"s3:ReplicateDelete\", \"s3:GetBucketVersioning\", \"s3:PutBucketVersioning\" ], \"Resource\": [ \"arn:aws:s3:::<Customer S3 Bucket Name>/*\", \"arn:aws:s3:::<Customer S3 Bucket Name>\" ] } ] } ``` ### Continuous replication to S3 bucket ‚Äã Application audit logs are streamed to Atlan's S3 bucket in near real time ‚Äî within 10 seconds of being generated This is a continuous process Once the logs are available in Atlan's bucket, the logs will be replicated to your organization's S3 bucket within 15 minutes The replication is ongoing and occurs without delays This ensures that logs are continuously transferred as they are generated, with no waiting period between replications. ## Enabling event logs in GCP ‚Äã !GCP Keycloak Events Logs For Google Cloud Platform (GCP), Atlan utilizes Logs Router to transfer logs from the GCS bucket of your Atlan tenant to a destination bucket of your choice",
    "metadata": {
      "topic": "Cloud logging and monitoring",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/cloud-logging-and-monitoring",
      "keywords": [
        "cloud",
        "logging",
        "monitoring",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "compliance_standards_and_asses_0",
    "text": "Atlan adheres to various industry standards and regulations to ensure the security, privacy, and integrity of the platform This entails conducting both external audits and internal assessments to continuously improve compliance standards Following is an overview of Atlan's key compliance certifications and internal assessment practices: | Compliance | Description | Status | Frequency | | --- | --- | --- | --- | | ISO 27001 | The Information Security Management System (ISMS) standard ensures data confidentiality, integrity, and availability. | Certified | Annual | | ISO 27701 | The Privacy Information Management System (PIMS) standard manages PII and ensures compliance with privacy regulations like GDPR and CCPA. | Certified | Annual | | SOC 2 Type II | The SOC (System and Organization Controls) 2 Type II report attests to the security, availability, confidentiality, and privacy controls for service organizations. | Certified | Annual | | GDPR | The General Data Protection Regulation (GDPR) is an EU regulation that ensures the protection of personal data by enforcing strict privacy and security measures, along with giving individuals control over their data Atlan adheres to GDPR through ongoing compliance, including breach notifications, data subject rights, and consent management. | Certified | Annual | | EU-U.S Data Privacy Framework | The Data Privacy Framework outlines policies and controls that govern how Atlan handles personal information to ensure data protection and compliance with privacy regulations like GDPR. | Compliant | Annual | | HIPAA | HIPAA, or the Health Insurance Portability and Accountability Act, safeguards protected health information (PHI). | Certified | Annual | | VAPT assessments | Annual third-party Vulnerability Assessment and Penetration Testing (VAPT) assessments help identify and mitigate potential vulnerabilities within the Atlan platform. | Ongoing | Annual |",
    "metadata": {
      "topic": "Compliance standards and assessments",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/compliance-standards-and-assessments",
      "keywords": [
        "compliance",
        "standards",
        "assessments",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "incident_response_plan_0",
    "text": "On this page Atlan's incident response plan for any potential network security incidents is as follows: ## Incident response process ‚Äã For any critical issues, the Incident Response Team at Atlan will follow a structured process designed to investigate, contain, and remediate the threat as well as recover systems and services The process includes: 1 Event reported - initial notification of the incident. 2 Triage and analysis - assessment of the incident's severity and potential impact. 3 Investigation - detailed examination to understand the cause and scope. 4 Containment and neutralization - actions to limit the impact and prevent further exploitation. 5 Recovery and vulnerability remediation - restoration of systems and addressing vulnerabilities. 6 Hardening and detection improvements - enhancing security measures and detection capabilities to prevent future incidents Key details about this process are as follows: - Incident manager - oversees incident response efforts. - War room - a central location, either physical or virtual (for example, Slack), dedicated to managing the incident. - Recurring meetings - regular meetings to review the incident status until resolution. - Notification - legal and executive staff will be informed as required. ## Incident severity levels ‚Äã | Severity | Category | Description | | --- | --- | --- | | P0 | Critical | Actively exploited risk involves the engagement of a malicious actor Identifying such active exploitation is essential Major data breach, widespread system outage, critical vulnerability being actively exploited. | | P1 | High | Active exploitation is not yet confirmed but is highly probable The vulnerability presents a high risk, potentially causing severe performance degradation or unauthorized access to sensitive data. | | P2/P3 | Medium/Low | Suspicious or unusual behavior that has not yet been verified and requires further investigation This includes moderate performance issues, non-critical vulnerabilities, and isolated incidents affecting a small group of users. | ## Incident reporting ‚Äã Atlan will report any breaches to customers, consumers, data subjects, and regulators without undue delay and in accordance with all contractual commitments and applicable legislation If any users become aware of an information security incident, potential incident, imminent incident, unauthorized access, policy violation, security weakness, or suspicious activity, please notify Atlan support immediately. - Incident response process - Incident severity levels - Incident reporting",
    "metadata": {
      "topic": "Incident response plan",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/incident-response-plan",
      "keywords": [
        "incident",
        "response",
        "plan",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "infrastructure_security_0",
    "text": "On this page See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more Atlan is deployed using Kubernetes in an Atlan-managed VPC (virtual private cloud) Atlan also carries out: - **Vulnerability management through frequent releases** - Atlan makes weekly releases to minimize vulnerability at a product and operating system level. - **Application Penetration Testing (APT)** - Atlan uses a third-party tool to conduct industry standard APT A penetration test is an authorized simulated cyber attack on a computer system, performed to evaluate the security of the system The test is performed to identify both weaknesses (including the potential for unauthorized parties to gain access to the system's features and data) and strengths, enabling a full risk assessment to be completed. - **Event logging and monitoring** - Atlan has many tools to support monitoring and event logging: - Prometheus and Grafana for monitoring - Fluent Bit and Loki for event logging ## Network access to the control plane ‚Äã We restrict access to the Kubernetes control plane by IP address to cluster administrators We deny public internet access to the control plane. ## Network access to nodes ‚Äã Nodes are configured to only accept connections (via network access control lists): - from the control plane on the specified ports - for services in Kubernetes of type `NodePort` and `LoadBalancer` Each component of the Kubernetes cluster has security measures configured These security measures are at the following levels: - Cluster security - Node security - Pod security - Container security - Network security - Code security - Secret management - Data encryption in transit - Network access to the control plane - Network access to nodes",
    "metadata": {
      "topic": "Infrastructure security",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/infrastructure-security",
      "keywords": [
        "infrastructure",
        "security",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "quality_assurance_framework_0",
    "text": "On this page Atlan has a robust quality assurance (QA) framework for delivering reliable, high-quality products that exceed user expectations and build trust The QA strategies and processes implemented throughout the product development lifecycle have the following benefits to offer: - Rigorous testing and quality checks ensure that the product is reliable, functional, and user-friendly. - Thorough testing helps identify and mitigate potential defects and risks early in the development process - minimizing rework and reducing the time and effort for bug fixes. - Clear communication, sharing test results, and involving all stakeholders throughout the process fosters effective collaboration and alignment on quality goals. ## Manual testing process ‚Äã Atlan uses Testmo for the manual testing process and Jira for project management: ### Test planning ‚Äã The objectives, scope, and test approach are defined for the manual testing phase Testable requirements are identified and test cases prioritized based on risk analysis. ### Test case design ‚Äã Once a feature is ready for testing, it is handed over to the QA team Test scenarios are created based on user requirements The product owner reviews these test scenarios and approves them for testing. ### Manual testing execution ‚Äã Manual testing is performed using the approved test scenarios Testing is conducted in different environments, including `beta`, `staging`, and `production` Comprehensive testing takes place in the staging environment Any issues identified during testing are documented using a bug reporting tool Reported issues are then assigned to the development team for resolution Manual testing covers the following areas: - **Functional testing** - to verify that the feature functions according to specified requirements. - **Security testing** - to identify vulnerabilities and ensure the security of user data and system resources. - **Integration testing** - to validate the seamless integration of the feature with other components or systems. - **Usability testing** - to evaluate the user-friendliness and intuitiveness of the feature. ### Defect reporting ‚Äã Any deviations or defects encountered during testing are recorded Steps to reproduce the issue are documented with screenshots or logs, if applicable, and a severity level is assigned to each issue. ### Issue retesting and sanity testing ‚Äã After the development team has fixed the reported issues, the QA team retests the scenarios A round of sanity tests is conducted to ensure the overall stability of the feature. ### Test cycle closure ‚Äã Test results are evaluated, including defect metrics, test coverage, and overall quality A test summary report is prepared that highlights the testing activities, challenges faced, and recommendations for improvement. ## Automation testing process ‚Äã Atlan uses mabl for the test automation process: ### Increased test coverage ‚Äã Automation testing allows Atlan to execute a large number of test cases across different scenarios, configurations, and environments By automating repetitive and time-consuming tasks, Atlan is able to achieve a broader test coverage, ensuring that critical functionalities and edge cases are thoroughly validated. ### Faster time-to-market ‚Äã Automation testing significantly reduces the time required to execute test cases compared to manual testing",
    "metadata": {
      "topic": "Quality assurance framework",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/quality-assurance-framework",
      "keywords": [
        "quality",
        "assurance",
        "framework",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "quality_assurance_framework_1",
    "text": "A test summary report is prepared that highlights the testing activities, challenges faced, and recommendations for improvement. ## Automation testing process ‚Äã Atlan uses mabl for the test automation process: ### Increased test coverage ‚Äã Automation testing allows Atlan to execute a large number of test cases across different scenarios, configurations, and environments By automating repetitive and time-consuming tasks, Atlan is able to achieve a broader test coverage, ensuring that critical functionalities and edge cases are thoroughly validated. ### Faster time-to-market ‚Äã Automation testing significantly reduces the time required to execute test cases compared to manual testing Atlan can accelerate the testing process and obtain quicker feedback on the quality of the product This helps in meeting tight deadlines and release features faster. ### Improved accuracy and consistency ‚Äã Automated tests are executed with precision and consistency, minimizing human errors and ensuring accurate results By eliminating manual intervention, Atlan reduces the risk of human-induced mistakes, resulting in reliable and repeatable test outcomes. ### Regression testing ‚Äã Automation testing is particularly effective for regression testing, which involves retesting previously validated functionalities to ensure that new changes or fixes do not introduce unintended issues By automating regression tests, Atlan can quickly and accurately verify that existing features are working as expected after modifications. ### Continuous integration and deployment ‚Äã Automation testing seamlessly integrates with Atlan's continuous integration and continuous delivery (CI/CD) pipeline This enables Atlan to automate the execution of tests at various stages of the development process, such as after each code commit or prior to a deployment By automating tests as part of the CI/CD process, Atlan ensures that software updates are thoroughly validated before being released. ### Maintenance and reusability ‚Äã Automation testing scripts can be maintained and reused across multiple test cycles, saving time and effort As the product evolves, the scripts can be updated to accommodate changes This ensures that the automation suite is up-to-date and aligned with the latest functionalities. ### Label-based testing ‚Äã Label-based testing incorporates mabl's automated testing capabilities into the CI/CD pipeline using labels Here's how it works: - **Triggering tests based on labels** - tests can be triggered based on specific labels in the CI/CD pipeline For example, a particular set of mabl tests labeled as `Regression` or `Smoke` can be executed during different stages of the pipeline, such as pre-production or post-deployment. - **Test result reporting** - after executing the mabl tests, results are reported back to the CI/CD pipeline This information includes the test outcomes, such as pass, fail, or blocked, along with any associated logs, screenshots, or other artifacts for further analysis. ## Deployment process ‚Äã Once a feature has successfully passed manual and automation testing, it is ready for deployment: ### Environment setup ‚Äã Target environments required for testing are set up and a base user is created for the environment to perform the test cases",
    "metadata": {
      "topic": "Quality assurance framework",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/quality-assurance-framework",
      "keywords": [
        "quality",
        "assurance",
        "framework",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "quality_assurance_framework_2",
    "text": "For example, a particular set of mabl tests labeled as `Regression` or `Smoke` can be executed during different stages of the pipeline, such as pre-production or post-deployment. - **Test result reporting** - after executing the mabl tests, results are reported back to the CI/CD pipeline This information includes the test outcomes, such as pass, fail, or blocked, along with any associated logs, screenshots, or other artifacts for further analysis. ## Deployment process ‚Äã Once a feature has successfully passed manual and automation testing, it is ready for deployment: ### Environment setup ‚Äã Target environments required for testing are set up and a base user is created for the environment to perform the test cases Each environment closely resembles the intended production environment to ensure accurate testing and validation. ### Build generation ‚Äã Once the code changes are ready for deployment, a build or package is generated that encapsulates the necessary files and resources for the software update The build ensures that all components are packaged correctly and ready for deployment. ### Automation testing ‚Äã Before deploying the software update to the target environment, a suite of automated tests is executed These tests validate the functionality, performance, and stability of the software, ensuring that it meets Atlan's quality standards. ### Monitoring and rollback ‚Äã Once the deployment is completed, the application is closely monitored in the target environment Monitoring tools and logs are used to identify any issues or anomalies In case of critical failures or any unexpected issues, there is a rollback plan in place to revert to the previous version. ### Continuous improvement ‚Äã The deployment process is continuously evaluated to identify areas for improvement This includes capturing feedback, analyzing deployment metrics, and implementing changes to enhance efficiency, reliability, and quality. ## Daily test reports ‚Äã Tests are executed on a daily schedule to ensure the proper functioning of the product and promptly address any issues The daily test reports include the following: ### Test case execution status ‚Äã Overall status of the test case execution for the day - including the number of cases that were executed, passed, failed, or blocked. ### Test coverage ‚Äã Insights into the coverage of test cases across different functional areas, features, or modules These help stakeholders understand the parts of the system that were tested and to what extent. ### Defects or issues ‚Äã Information on any defects or issues discovered during testing",
    "metadata": {
      "topic": "Quality assurance framework",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/quality-assurance-framework",
      "keywords": [
        "quality",
        "assurance",
        "framework",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "quality_assurance_framework_3",
    "text": "The daily test reports include the following: ### Test case execution status ‚Äã Overall status of the test case execution for the day - including the number of cases that were executed, passed, failed, or blocked. ### Test coverage ‚Äã Insights into the coverage of test cases across different functional areas, features, or modules These help stakeholders understand the parts of the system that were tested and to what extent. ### Defects or issues ‚Äã Information on any defects or issues discovered during testing This can encompass newly identified defects, severity and priority levels, steps to reproduce the issues, and any relevant supporting details. ### Metrics and statistics ‚Äã Metrics and statistics related to test case execution, such as the pass/fail ratio, defect density, test coverage percentage, or any other relevant metrics agreed upon by the QA team. ### Summary and recommendations ‚Äã Key findings, observations, and recommendations for further actions or improvements to help with decision making and prioritizing testing efforts. ## Protocol for test failures ‚Äã When a test fails, Atlan adheres to a specific protocol to address and resolve the failure: ### Failure identification ‚Äã Once a test fails, the specific test case or test suite that encountered the failure is promptly identified This involves reviewing test logs, error messages, and any available diagnostic information to pinpoint the cause of the failure. ### Root cause analysis ‚Äã A thorough root cause analysis is conducted to determine why the test failed This may involve examining the test environment, reviewing test data, analyzing code changes, or investigating any external factors that could have influenced the failure. ### Bug reporting ‚Äã If the test failure is determined to be due to a software defect, the bug is reported following a standard bug reporting process The QA team provides detailed information regarding the failure, including steps to reproduce, relevant test data, and any supporting evidence or screenshots. ### Bug resolution ‚Äã Once the bug is reported, the development team takes appropriate action to address and resolve the defect This includes analyzing the bug report, reproducing the issue, and implementing the necessary fixes The bug resolution process follows Atlan's established software development lifecycle and bug prioritization guidelines. ### Retesting ‚Äã After the bug fixes are implemented, the QA team retests to verify that the issue has been resolved and the test case has passed successfully This is to ensure that the fix does not introduce any new issues or regressions. - Manual testing process - Automation testing process - Deployment process - Daily test reports - Protocol for test failures",
    "metadata": {
      "topic": "Quality assurance framework",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/quality-assurance-framework",
      "keywords": [
        "quality",
        "assurance",
        "framework",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "tenant_access_management_0",
    "text": "On this page For any Atlan tenant, there are three types of access required for troubleshooting an issue: - access to cloud resources - access to vCluster or dedicated Kubernetes (K8s) cluster - access to product ## Access to cloud resources ‚Äã 1 Identify need for access - access to the cloud resources of a tenant is generally required to debug an issue related to infrastructure components For any tenant, cloud resources include cloud storage, secrets, Kubernetes cluster, network rules, and more. 2 Request access - if an Atlan engineer troubleshooting an issue requires access to any cloud resource, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification. 3 Approval process - the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control. 4 Access granting - once approved, the IT team at Atlan will grant access to the resource for a specified period of time. 5 Access revocation - after the specified period of time is over or troubleshooting has been completed, the IT team will revoke access to avoid any unauthorized or prolonged access. ## Access to cluster ‚Äã 1 Identify need for access - access to vCluster or dedicated Kubernetes (K8s) cluster is required to troubleshoot an issue related to Kubernetes resources. 2 Management of Kubernetes clusters - Atlan uses Loft for managing K8s clusters and vclusters All K8s clusters and vclusters are added to and managed from Loft. 3 Request access - if an Atlan engineer troubleshooting an issue requires access to a vcluster or dedicated cluster, the engineer must raise a formal request using Atlan's service request ticketing system with the appropriate justification. 4 Approval process - the request will be reviewed and require approval from a relevant authority, likely a manager or another team member responsible for access control. 5 Access granting - once approved, the IT team at Atlan will grant access to the resource for a specified period of time. 6 Access revocation - after the specified period of time is over or troubleshooting has been completed, the IT team will revoke access to avoid any unauthorized or prolonged access. ## Access to product ‚Äã 1 Identify need for access - access to the product may be required to troubleshoot an issue with product features or connector workflows. 2 Creation of support user - Atlan creates a support user named `atlansupport` while creating a tenant The credentials for this user is stored with the IT team. 3 Security of credentials - the credentials for this user are securely stored in 1Password, and managed by the IT team Access to these credentials is tightly controlled, requiring explicit permission from the IT team to access them, and support user passwords are reset every 90 days. 4",
    "metadata": {
      "topic": "Tenant access management",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/tenant-access-management",
      "keywords": [
        "tenant",
        "access",
        "management",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "tenant_logs_0",
    "text": "On this page Atlan can help you understand the events that occur in your tenants, including user and administrative actions Learn more about logging and retention as follows: ## Tenant logs ‚Äã Note the following: - Load balancer logs for Azure and GCP tenants are currently not enabled. - AuditSearch and SearchLog records are persisted forever in Elasticsearch The 30-day retention period pertains to application logs written to logging Elasticsearch. - An example of block storage mentioned below is Amazon Elastic Block Store (EBS) for AWS. ### Production tenants ‚Äã | Log types | Retention | Storage | AWS | Azure | GCP | | --- | --- | --- | --- | --- | --- | | Active tenant overall backup | 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Offboarded tenant overall backup | AWS - 30 days, Azure and GCP - 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Load balancer logs | 30 days | Object storage | ‚úÖ | ‚ùå | ‚ùå | | Audit - user events | 60 days | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Audit - admin events | Unlimited | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Application logs | 30 days | Elasticsearch and object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow logs | 90 days | ClickHouse | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow artifacts | 180 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Application metrics | 60 days | VictoriaMetrics (block storage) | ‚úÖ | ‚úÖ | ‚úÖ | ### Proof of value (POV) tenants ‚Äã tenants\") | Log types | Retention | Storage | AWS | Azure | GCP | | --- | --- | --- | --- | --- | --- | | Active tenant overall backup | 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Offboarded tenant overall backup | AWS - 3 days, Azure and GCP - 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Load balancer logs | 30 days | Object storage | ‚úÖ | ‚ùå | ‚ùå | | Audit - user events | 60 days | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Audit - admin events | Unlimited | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Application logs | 30 days | Elasticsearch and object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow artifacts | 180 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Application metrics | 60 days | VictoriaMetrics (block storage) | ‚úÖ | ‚úÖ | ‚úÖ | ## Atlan logs ‚Äã | Service | Type | Logging pipeline | Destination | | --- | --- | --- | --- | | Heracles | application | Fluent Bit | S3 | | Argo | application, server | Argo, Fluent Bit | S3 | | Atlas | application, audit, perf | Fluent Bit | S3 | | Numaflow | application | Fluent Bit | S3 | | Kube events | application | Fluent Bit | S3 | | Wisdom | application, audit | Fluent Bit | S3 | | Chronos | application | Fluent Bit | S3 | | Redis | application | Fluent Bit | S3 | | Kong | application, audit | Fluent Bit, PostgreSQL, Keycloak REST API | S3 | | Keycloak | application | Fluent Bit | S3 | | Elasticsearch | application | Fluent Bit | S3 | | Cassandra | application | Fluent Bit | S3 | | Heka | application | Fluent Bit | S3 | | Pgpool | application, server | Fluent Bit | S3 | | Kafka | events | Fluent Bit | S3 | ## Cloud storage lifecycle ‚Äã The cloud storage created for each tenant has its own lifecycle",
    "metadata": {
      "topic": "Tenant logs",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/tenant-logs",
      "keywords": [
        "tenant",
        "logs",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "tenant_logs_1",
    "text": "Learn more about logging and retention as follows: ## Tenant logs ‚Äã Note the following: - Load balancer logs for Azure and GCP tenants are currently not enabled. - AuditSearch and SearchLog records are persisted forever in Elasticsearch The 30-day retention period pertains to application logs written to logging Elasticsearch. - An example of block storage mentioned below is Amazon Elastic Block Store (EBS) for AWS. ### Production tenants ‚Äã | Log types | Retention | Storage | AWS | Azure | GCP | | --- | --- | --- | --- | --- | --- | | Active tenant overall backup | 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Offboarded tenant overall backup | AWS - 30 days, Azure and GCP - 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Load balancer logs | 30 days | Object storage | ‚úÖ | ‚ùå | ‚ùå | | Audit - user events | 60 days | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Audit - admin events | Unlimited | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Application logs | 30 days | Elasticsearch and object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow logs | 90 days | ClickHouse | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow artifacts | 180 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Application metrics | 60 days | VictoriaMetrics (block storage) | ‚úÖ | ‚úÖ | ‚úÖ | ### Proof of value (POV) tenants ‚Äã tenants\") | Log types | Retention | Storage | AWS | Azure | GCP | | --- | --- | --- | --- | --- | --- | | Active tenant overall backup | 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Offboarded tenant overall backup | AWS - 3 days, Azure and GCP - 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Load balancer logs | 30 days | Object storage | ‚úÖ | ‚ùå | ‚ùå | | Audit - user events | 60 days | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Audit - admin events | Unlimited | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Application logs | 30 days | Elasticsearch and object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow artifacts | 180 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Application metrics | 60 days | VictoriaMetrics (block storage) | ‚úÖ | ‚úÖ | ‚úÖ | ## Atlan logs ‚Äã | Service | Type | Logging pipeline | Destination | | --- | --- | --- | --- | | Heracles | application | Fluent Bit | S3 | | Argo | application, server | Argo, Fluent Bit | S3 | | Atlas | application, audit, perf | Fluent Bit | S3 | | Numaflow | application | Fluent Bit | S3 | | Kube events | application | Fluent Bit | S3 | | Wisdom | application, audit | Fluent Bit | S3 | | Chronos | application | Fluent Bit | S3 | | Redis | application | Fluent Bit | S3 | | Kong | application, audit | Fluent Bit, PostgreSQL, Keycloak REST API | S3 | | Keycloak | application | Fluent Bit | S3 | | Elasticsearch | application | Fluent Bit | S3 | | Cassandra | application | Fluent Bit | S3 | | Heka | application | Fluent Bit | S3 | | Pgpool | application, server | Fluent Bit | S3 | | Kafka | events | Fluent Bit | S3 | ## Cloud storage lifecycle ‚Äã The cloud storage created for each tenant has its own lifecycle The lifecycle policy is attached to paths in the cloud storage",
    "metadata": {
      "topic": "Tenant logs",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/tenant-logs",
      "keywords": [
        "tenant",
        "logs",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "tenant_logs_2",
    "text": "The 30-day retention period pertains to application logs written to logging Elasticsearch. - An example of block storage mentioned below is Amazon Elastic Block Store (EBS) for AWS. ### Production tenants ‚Äã | Log types | Retention | Storage | AWS | Azure | GCP | | --- | --- | --- | --- | --- | --- | | Active tenant overall backup | 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Offboarded tenant overall backup | AWS - 30 days, Azure and GCP - 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Load balancer logs | 30 days | Object storage | ‚úÖ | ‚ùå | ‚ùå | | Audit - user events | 60 days | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Audit - admin events | Unlimited | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Application logs | 30 days | Elasticsearch and object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow logs | 90 days | ClickHouse | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow artifacts | 180 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Application metrics | 60 days | VictoriaMetrics (block storage) | ‚úÖ | ‚úÖ | ‚úÖ | ### Proof of value (POV) tenants ‚Äã tenants\") | Log types | Retention | Storage | AWS | Azure | GCP | | --- | --- | --- | --- | --- | --- | | Active tenant overall backup | 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Offboarded tenant overall backup | AWS - 3 days, Azure and GCP - 15 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Load balancer logs | 30 days | Object storage | ‚úÖ | ‚ùå | ‚ùå | | Audit - user events | 60 days | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Audit - admin events | Unlimited | PostgreSQL | ‚úÖ | ‚úÖ | ‚úÖ | | Application logs | 30 days | Elasticsearch and object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Workflow artifacts | 180 days | Object storage | ‚úÖ | ‚úÖ | ‚úÖ | | Application metrics | 60 days | VictoriaMetrics (block storage) | ‚úÖ | ‚úÖ | ‚úÖ | ## Atlan logs ‚Äã | Service | Type | Logging pipeline | Destination | | --- | --- | --- | --- | | Heracles | application | Fluent Bit | S3 | | Argo | application, server | Argo, Fluent Bit | S3 | | Atlas | application, audit, perf | Fluent Bit | S3 | | Numaflow | application | Fluent Bit | S3 | | Kube events | application | Fluent Bit | S3 | | Wisdom | application, audit | Fluent Bit | S3 | | Chronos | application | Fluent Bit | S3 | | Redis | application | Fluent Bit | S3 | | Kong | application, audit | Fluent Bit, PostgreSQL, Keycloak REST API | S3 | | Keycloak | application | Fluent Bit | S3 | | Elasticsearch | application | Fluent Bit | S3 | | Cassandra | application | Fluent Bit | S3 | | Heka | application | Fluent Bit | S3 | | Pgpool | application, server | Fluent Bit | S3 | | Kafka | events | Fluent Bit | S3 | ## Cloud storage lifecycle ‚Äã The cloud storage created for each tenant has its own lifecycle The lifecycle policy is attached to paths in the cloud storage The lifecycle policy applied to a production tenant is as follows: ### Amazon Web Services (AWS) ‚Äã\") | Lifecycle policy | Path | Action | | --- | --- | --- | | `DeleteClusterLogsAfter30Days` | `logs/` | Expires | | `DeleteArgoArtifactsAfter180Days` | `argo-artifacts/` | Transitions to S3 Glacier Flexible Retrieval, then expires | | `DeleteArgoBackupAfter15Days` | `backup/argo/` | Expires | | `DeleteAltanScheduleQuery` | `argo-artifacts/default/schedule-query/` | Expires | | `DeletePostgresBackupAfter15Days` | `backup/postgres/` | Expires | | `DeleteRedisBackupAfter15Days` | `backup/redis/` | Expires | | `DeleteCassandraBackupAfter15Days` | `backup/cassandra/` | Expires | | `DeletePrometheusBackupAfter15Days` | `backup/prometheus/` | Expires | | `DeleteALBLogsAfter30Days` | `AWSLogs/` | Expires | ### Microsoft Azure ‚Äã | Lifecycle policy | Path | Action | | --- | --- | --- | | `DeleteClusterLogsAfter30Days` | `logs/` | Delete | | `DeleteArgoArtifactsAfter180Days` | `argo-artifacts/` | Moves to archive after 90 days and delete after 180 days | | `DeleteArgoBackupAfter15Days` | `backup/argo/` | Delete | | `DeletePostgresBackupAfter15Days` | `backup/postgres/` | Delete | | `DeleteRedisBackupAfter15Days` | `backup/redis/` | Delete | | `DeleteCassandraBackupAfter15Days` | `backup/cassandra/` | Delete | | `DeleteAltanScheduleQuery` | `argo-artifacts/default/schedule-query/` | Delete if blobs not modified in 1 day | | `DeleteSparkEventLogsAfter15Days` | `spark-event-logs/` | Delete | ### Google Cloud Platform (GCP) ‚Äã\") | Lifecycle policy | Path | Action | | --- | --- | --- | | `DeleteClusterLogsAfter15Days` | `logs/` | Delete | | `DeleteArgoArtifactsAfter180Days` | `argo-artifacts/` | Archive | | `DeleteArgoArtifactsAfter270Days` | `argo-artifacts/` | Delete | | `DeleteArgoBackupAfter3Days` | `backup/argo/` | Delete | | `DeletePostgresBackupAfter3Days` | `backup/postgres/` | Delete | | `DeletePrometheusBackupAfter3Days` | `backup/prometheus/` | Delete | | `DeleteRedisBackupAfter3Days` | `backup/redis/` | Delete | | `DeleteScheduleQueryAfter1Day` | `argo-artifacts/default/schedule-query/` | Delete | | `DeleteSparkEventLogsAfter15Days` | `spark-event-logs/` | Delete | | `DeleteCassandraBackupAfter3Days` | `backup/cassandra/` | Delete | - Tenant logs - Atlan logs - Cloud storage lifecycle",
    "metadata": {
      "topic": "Tenant logs",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/tenant-logs",
      "keywords": [
        "tenant",
        "logs",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "tenant_offboarding_0",
    "text": "On this page An Atlan tenant consists of multiple infrastructure resources, such as EC2 nodes, S3 buckets, ALB, EKS, and IAM roles, Loft vcluster, and more Offboarding an Atlan vcluster tenant involves removing all the infrastructure resources associated with that particular tenant or instance While most of the resources can be deleted immediately without any data loss, we handle the cleanup of storage resources like S3 buckets with care S3 buckets associated with a tenant are not immediately removed - they are retained for 30 days after a tenant is offboarded This ensures that there is a disaster recovery mechanism in place if a tenant is offboarded by mistake or faces any unrecoverable errors. ## Tenant decommissioning ‚Äã You can raise a support ticket for tenant decommissioning The customer success team at Atlan will notify the infrastructure support team responsible for executing tenant offboarding These requests are usually resolved within the first 48 hours to ensure that no unnecessary costs are incurred for infra resources. ## Frequently asked questions ‚Äã #### Where is customer data stored? ‚Äã Any customer-specific data in Atlan - for example, query logs, assets, service logs, and more - are stored in EBS volumes and S3 buckets. #### How long does customer data persist after tenant offboarding? ‚Äã EBS volumes are immediately deleted during offboarding However, S3 buckets are retained for a period of 30 days. - Tenant decommissioning - Frequently asked questions",
    "metadata": {
      "topic": "Tenant offboarding",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/platform/references/tenant-offboarding",
      "keywords": [
        "tenant",
        "offboarding",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "install_on_aws_eks_0",
    "text": "On this page This guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster. ## System requirements ‚Äã To deploy the Secure Agent on AWS EKS, ensure the following system requirements are met: 1 Configure network access between your Secure Agent and Atlan tenant For more information, see Whitelisting Secure Agent. 2 You need Kubernetes version 1.19 or higher. 3 You need to install Helm and kubectl on the machine you're using to connect to the AWS EKS cluster. 4 You need at least 1 node for base services with a disk space of 20 GB and instance configuration as below: | Environment | Minimum instance type | Recommended instance type | | --- | --- | --- | | Production | t3.large | Custom based on workload | | Non-production | t3.large | t3.xlarge | info üí™ **Did you know?** For optimal autoscaling, scale nodes based on the number of concurrent workflows. ## Permissions required ‚Äã Before installing the Secure Agent, make sure the following permissions are in place: ### Permissions for the Installer ‚Äã The user, service or system account performing the installation needs access to the EKS cluster and permissions to manage Custom Resource Definitions (CRDs). 1 Ensure the kubeconfig is correctly configured for your target EKS cluster If needed, use the following command to configure or update your kubeconfig file. ```codeBlockLines_e6Vv aws eks update-kubeconfig --region ``<region>`` --name ``<cluster-name>`` ``` - Replace `<region>` with your AWS region (for example, us-east-1) and `<cluster-name>` with the name of your EKS cluster. 2 The installer needs permission to create, update, and delete Custom Resource Definitions (CRDs) If not using the cluster-admin role, grant the following: - Create a file named `agent-crd-permissions.yaml` on your machine. - Copy the following content into the file: ```codeBlockLines_e6Vv apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # Use a descriptive name name: helm-crd-installer-role rules: - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] # Recommended verbs for Helm CRD management verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"] apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: # Use a descriptive name name: helm-crd-installer-binding subjects: # IMPORTANT: Modify this section based on who is running Helm # Choose ONE of the following options and replace placeholders. # Option 1: Bind to a specific User - kind: User name: \"your-kubernetes-username\" # Replace with the installing user's K8s username recognized by the cluster apiGroup: rbac.authorization.k8s.io # Option 2: Bind to a specific Group # - kind: Group # name: \"your-kubernetes-groupname\" # Replace with the installing user's K8s group name # apiGroup: rbac.authorization.k8s.io # Option 3: Bind to a Service Account (e.g., for CI/CD pipelines) # - kind: ServiceAccount # name: \"installer-sa-name\" # Replace with the installer SA's name # namespace: \"installer-sa-namespace\" # Replace with the installer SA's namespace roleRef: # This refers to the ClusterRole created above kind: ClusterRole name: helm-crd-installer-role apiGroup: rbac.authorization.k8s.io ``` Follow the comments in the file to replace the placeholders",
    "metadata": {
      "topic": "Install on AWS EKS",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
      "keywords": [
        "install",
        "aws",
        "eks",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_aws_eks_1",
    "text": "The installer needs permission to create, update, and delete Custom Resource Definitions (CRDs) If not using the cluster-admin role, grant the following: - Create a file named `agent-crd-permissions.yaml` on your machine. - Copy the following content into the file: ```codeBlockLines_e6Vv apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # Use a descriptive name name: helm-crd-installer-role rules: - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] # Recommended verbs for Helm CRD management verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"] apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: # Use a descriptive name name: helm-crd-installer-binding subjects: # IMPORTANT: Modify this section based on who is running Helm # Choose ONE of the following options and replace placeholders. # Option 1: Bind to a specific User - kind: User name: \"your-kubernetes-username\" # Replace with the installing user's K8s username recognized by the cluster apiGroup: rbac.authorization.k8s.io # Option 2: Bind to a specific Group # - kind: Group # name: \"your-kubernetes-groupname\" # Replace with the installing user's K8s group name # apiGroup: rbac.authorization.k8s.io # Option 3: Bind to a Service Account (e.g., for CI/CD pipelines) # - kind: ServiceAccount # name: \"installer-sa-name\" # Replace with the installer SA's name # namespace: \"installer-sa-namespace\" # Replace with the installer SA's namespace roleRef: # This refers to the ClusterRole created above kind: ClusterRole name: helm-crd-installer-role apiGroup: rbac.authorization.k8s.io ``` Follow the comments in the file to replace the placeholders In the above file: - Resource: `customresourcedefinitions` - needed for managing CRDs in the cluster. - API Group: `apiextensions.k8s.io` - required to work with CRDs. - Verbs: create, get, list, update, delete - necessary for installing, inspecting, updating, and cleaning up CRDs using Helm. - ClusterRoleBinding: needed to assign the role to the user or group performing the installation. - Once you‚Äôve updated the placeholders, use the below `kubectl` command to apply the configuration: 4 Once you‚Äôve updated the placeholders, use the below kubectl command to apply the configuration: ```codeBlockLines_e6Vv kubectl apply -f agent-crd-permissions.yaml ``` ### Permissions for the Secure Agent Pod (Runtime) ‚Äã\") The Secure Agent runs as pods in your EKS cluster and requires permissions to interact with AWS services like S3 These permissions are granted through IAM Roles for Service Accounts (IRSA). 1 Create a new IAM role for the Secure Agent pod. 2 Configure the trust policy to enable the Secure Agent‚Äôs Kubernetes service account to assume the role Make sure the argo-workflow service account exists in the same namespace where you plan to install the agent",
    "metadata": {
      "topic": "Install on AWS EKS",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
      "keywords": [
        "install",
        "aws",
        "eks",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_aws_eks_2",
    "text": "Configure the trust policy to enable the Secure Agent‚Äôs Kubernetes service account to assume the role Make sure the argo-workflow service account exists in the same namespace where you plan to install the agent For more information, see the AWS documentation on IAM roles for service accounts (IRSA). - Example: Trust policy for the argo-workflow service account: ```codeBlockLines_e6Vv \"Condition\": { \"StringEquals\": { \":sub\": \"system:serviceaccount::argo-workflow\" } } ``` Replace `<namespace>` with the namespace where you plan to install agent. - Create an S3 bucket (or use an existing one), and attach the following permissions to the IAM role used by the Secure Agent: - `s3:PutObject`: Needed to write logs and artifacts - `s3:GetObject`: Needed to read logs and artifacts. - `s3:ListBucket`: Needed by Argo artifact repository for listing objects. **Did you know?** The Helm chart automatically configures the necessary Kubernetes RBAC for Argo Workflows, which the Secure Agent uses No additional configuration is required for the Secure Agent pod.. ## Prerequisites ‚Äã Before proceeding, complete the following setup steps to prepare your Atlan tenant and AWS EKS cluster. ### Configure Atlan tenant ‚Äã In your Atlan tenant: 1 Sign in as an Atlan admin. 2 Go to **Admin** from the left menu. 3 Under **Workspace**, click **Labs**. 4 Navigate to **Workflow Center**. 5 Enable the **Crawl assets using Secure Agent** toggle. ### Configure Secure Agent settings ‚Äã The `agent_config_values.yaml` file is used to configure the Secure Agent, Argo Workflows, and storage for the AWS EKS cluster Follow these instructions on the machine where you're performing the installation. 1 Create a file named `agent_config_values.yaml` file. 2 Copy the configuration below into the file: ```codeBlockLines_e6Vv # ----------------------------------------------------------------------------------------- # Agent core settings - Follow the comments to update: # 1 Image registry settings - To be updated only if you are using a private image registry # 2 Atlan connection settings - To be updated only if you want agent to use the S3 bucket # 3 Argo Private repository settings - To be updated only if you are using private repository for Argo workflows # 4 Kubernetes Pod Annotation settings - To be updated only if you want to customize the Kubernetes pod‚Äôs metadata # 5 Argo Private repository settings - To be updated only if you are using private repository for Argo workflows # 6 S3 storage settings - To be updated with S3 bucket details. # ----------------------------------------------------------------------------------------- agent: enabled: true enableStorageProxy: false ca: crt: \"\" #Provide a base64-encoded string of a JSON object, e.g., {\"client_id\": 123, \"client_secret\": 1243}. #Set this only if you need to include custom headers in API calls made by the agent. restAPIHeaders: \"\" versions: k3s: \"\" k8s: \"\" helm: \"\" # 1",
    "metadata": {
      "topic": "Install on AWS EKS",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
      "keywords": [
        "install",
        "aws",
        "eks",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_aws_eks_3",
    "text": "Argo Private repository settings - To be updated only if you are using private repository for Argo workflows # 6 S3 storage settings - To be updated with S3 bucket details. # ----------------------------------------------------------------------------------------- agent: enabled: true enableStorageProxy: false ca: crt: \"\" #Provide a base64-encoded string of a JSON object, e.g., {\"client_id\": 123, \"client_secret\": 1243}. #Set this only if you need to include custom headers in API calls made by the agent. restAPIHeaders: \"\" versions: k3s: \"\" k8s: \"\" helm: \"\" # 1 Image Registry Settings image: # Only update if you're using a private image registry registry: \"public.ecr.aws\" repository: \"atlanhq\" # Only update if you're using custom images restImageName: \"rest-2\" restImageTag: \"1.0\" # Only update if you're using custom images jdbcImageName: \"jdbc-metadata-extractor-with-jars\" jdbcImageTag: \"1.0\" # Only update if you're using custom images credentialImageName: \"connector-auth\" credentialImageTag: \"1.0\" # Only update if you're using custom images csaScriptsImageName: \"marketplace-csa-scripts\" csaScriptsImageTag: \"1.0\" # Marketplace scripts image details - keep these values as is unless using custom images marketplaceScriptsImageName: \"marketplace-scripts-agent\" marketplaceScriptsImageTag: \"1.0\" pullPolicy: IfNotPresent pullSecrets: [] # Add pull secrets if using private registry annotations: {} labels: {} serviceAccountName: \"\" automountServiceAccountToken: true resources: {} # 2 Atlan connection settings - Only update if you want to agent to use the S3 bucket atlan: argoToken: \"\" vaultEnvEnabled: false # Set to true only if the agent should store metadata # in your bucket instead of sending it to Atlan via presigned URL. useAgentBucket: false metadataBucket: \"\" persistentVolume: scripts: enabled: false data: enabled: false minio: enabled: false argo-workflows: images: pullPolicy: IfNotPresent pullSecrets: [] crds: install: true keep: true annotations: {} singleNamespace: true workflow: serviceAccount: create: true rbac: create: true controller: # 3 Argo Private repository settings - Only update if you are using a private image repository for Argo image: # update the private image repository details registry: quay.io repository: argoproj/workflow-controller tag: \"\" parallelism: 10 resourceRateLimit: limit: 10 burst: 5 rbac: create: true secretWhitelist: [] accessAllSecrets: false writeConfigMaps: false configMap: create: true name: \"\" namespaceParallelism: 10 workflowDefaults: # 4 Kubernetes Pod Annotation settings - Only update if you want to customize the Pod metadata. ## For example, the annotation might be used by external systems such as proxies, or monitoring tools, and more. spec: podMetadata: annotations: argo.workflow/agent-type: \"atlan-agent-service\" labels: app.kubernetes.io/name: \"atlan-agent\" podGC: strategy: OnPodSuccess serviceAccountName: argo-workflow automountServiceAccountToken: true ttlStrategy: secondsAfterCompletion: 84600 templateDefaults: container: securityContext: allowPrivilegeEscalation: false resources: {} env: - name: CA_CERT valueFrom: configMapKeyRef: name: cert-config key: ca.crt optional: true - name: REST_API_HEADERS valueFrom: configMapKeyRef: name: agent-registry-settings key: restAPIHeaders optional: true serviceAccount: create: true name: workflow-controller workflowNamespaces: - default replicas: 1 revisionHistoryLimit: 10 nodeEvents: enabled: false server: enabled: true # 5 Argo Private repository settings - Only update if you are using a private image repository for Argo image: registry: quay.io repository: argoproj/argocli tag: \"\" rbac: create: true serviceAccount: create: true replicas: 1 autoscaling: enabled: false ingress: enabled: false annotations: ingress.kubernetes.io/ssl-redirect: \"false\" resources: {} executor: securityContext: {} resources: {} artifactRepository: archiveLogs: true useStaticCredentials: false # 6",
    "metadata": {
      "topic": "Install on AWS EKS",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
      "keywords": [
        "install",
        "aws",
        "eks",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_aws_eks_4",
    "text": "Kubernetes Pod Annotation settings - Only update if you want to customize the Pod metadata. ## For example, the annotation might be used by external systems such as proxies, or monitoring tools, and more. spec: podMetadata: annotations: argo.workflow/agent-type: \"atlan-agent-service\" labels: app.kubernetes.io/name: \"atlan-agent\" podGC: strategy: OnPodSuccess serviceAccountName: argo-workflow automountServiceAccountToken: true ttlStrategy: secondsAfterCompletion: 84600 templateDefaults: container: securityContext: allowPrivilegeEscalation: false resources: {} env: - name: CA_CERT valueFrom: configMapKeyRef: name: cert-config key: ca.crt optional: true - name: REST_API_HEADERS valueFrom: configMapKeyRef: name: agent-registry-settings key: restAPIHeaders optional: true serviceAccount: create: true name: workflow-controller workflowNamespaces: - default replicas: 1 revisionHistoryLimit: 10 nodeEvents: enabled: false server: enabled: true # 5 Argo Private repository settings - Only update if you are using a private image repository for Argo image: registry: quay.io repository: argoproj/argocli tag: \"\" rbac: create: true serviceAccount: create: true replicas: 1 autoscaling: enabled: false ingress: enabled: false annotations: ingress.kubernetes.io/ssl-redirect: \"false\" resources: {} executor: securityContext: {} resources: {} artifactRepository: archiveLogs: true useStaticCredentials: false # 6 S3 bucket settings - needed by the secure agent to store logs and artifacts s3: # S3 bucket name - Update with the bucket name you created in the Permissions required section. bucket: \"atlan--bucket\" # S3 endpoint endpoint: \"s3.us-east-2.amazonaws.com\" # AWS region - Update with the region where you created bucket in the Permissions required section. region: \"us-east-2\" # Artifact path format keyFormat: \"argo-artifacts/{{workflow.namespace}}/{{workflow.name}}/{{pod.name}}\" # Whether to use insecure connections insecure: false # Use AWS SDK credentials (IAM role) useSDKCreds: true ``` 3 In the configuration file, follow the comments to replace the necessary attributes You may want to update the below configurations if: - _You are using a private image registry (Image registry settings)_ - _You want the agent to use an S3 bucket (Atlan connection settings)_ - _You are using a private repository for Argo workflows (Argo Private repository settings)_ - _You want to customize the Kubernetes pod's metadata (Kubernetes Pod Annotation settings)_ - _You need specific S3 storage configuration (S3 storage settings)_ ## Install using Helm chart ‚Äã Follow these steps to install the Secure Agent and its dependencies into your AWS EKS cluster using Helm charts. 1 Install the Argo Custom Resource Definitions (CRDs) required by the Secure Agent This step installs only the CRDs The Secure Agent is installed in the subsequent step using a Helm upgrade. ```codeBlockLines_e6Vv helm install <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent --version 0.1.0 -n <namespace> --create-namespace -f <path/to/agent_config_values.yaml> --set agent.name=\"<secure-agent-name>\" --set agent.atlan.domain=\"<atlan-tenant-domain>\" --set agent.atlan.token=\"<atlan-api-token>\" --set argo-workflows.controller.workflowNamespaces={<namespace>} --set IsUpgrade=false ``` Replace the placeholders: - `<namespace>`: The Kubernetes namespace where you want to deploy the Secure Agent. - `<path/to/agent_config_values.yaml>`: The path to the YAML config file. - `<secure-agent-name>`: Unique name, like agent-us-east-cdw. - `<helm-app-name>`: Unique Helm release name, like atlan-agent-v1. - `<atlan-tenant-domain>`: Your Atlan domain, e.g., mycompany.atlan.com. - `<atlan-api-token>`: Token used for authentication See Create a bearer token. 2 Use the following kubectl command to associate the IAM role with the service account This enables the Secure Agent to access the S3 bucket securely using IAM Roles for Service Accounts (IRSA)",
    "metadata": {
      "topic": "Install on AWS EKS",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
      "keywords": [
        "install",
        "aws",
        "eks",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_aws_eks_5",
    "text": "Use the following kubectl command to associate the IAM role with the service account This enables the Secure Agent to access the S3 bucket securely using IAM Roles for Service Accounts (IRSA) Make sure the IAM role‚Äôs trust policy enables the argo-workflow service account to assume the role. ```codeBlockLines_e6Vv kubectl annotate serviceaccount argo-workflow -n eks.amazonaws.com/role-arn=arn:aws:iam:::role/ ``` Replace the placeholders: - `<namespace`: The Kubernetes namespace where you want to deploy the Secure Agent. - `<AWS_ACCOUNT_ID>`: Your AWS Account ID. - `<YourAgentIAMRoleName>`: The IAM role name you created for the Secure Agent using IRSA. 3 Install the Secure Agent by upgrading the Helm release This step performs the actual Secure Agent installation after CRDs are in place. ```codeBlockLines_e6Vv helm upgrade <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent --version 0.1.0 -n <namespace> --create-namespace -f <path/to/agent_config_values.yaml> --set agent.name=\"<secure-agent-name>\" --set agent.atlan.domain=\"<atlan-tenant-domain>\" --set agent.atlan.token=\"<atlan-api-token>\" --set argo-workflows.controller.workflowNamespaces={<namespace>} --set IsUpgrade=true ``` Replace the placeholders: - `<namespace>`: The Kubernetes namespace where you want to deploy the Secure Agent. - `<path/to/agent_config_values.yaml>`: The path to the YAML config file. - `<secure-agent-name>`: Unique name, like agent-us-east-cdw. - `<helm-app-name>`: Unique Helm release name, like atlan-agent-v1. - `<atlan-tenant-domain>`: Your Atlan domain, e.g., mycompany.atlan.com. - `<atlan-api-token>`: Token used for authentication See Create a bearer token. 4 While the installation is in progress, you can run the following command to verify the progress: ```codeBlockLines_e6Vv kubectl get pods -n <namespace> ``` Replace `<namespace>` with the Kubernetes namespace used for deployment. ## Verify installation ‚Äã To confirm successful installation: 1 Sign in to your Atlan tenant as an admin For example, `https://<tenant>.atlan.com`. 2 Navigate to the **Agent** tab. 3 Search for your Secure Agent name. 4 If the agent appears in the list and is marked **Active**, installation is complete. - System requirements - Permissions required - Prerequisites - Install using Helm chart - Verify installation",
    "metadata": {
      "topic": "Install on AWS EKS",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
      "keywords": [
        "install",
        "aws",
        "eks",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "configure_workflow_execution_0",
    "text": "On this page When using Secure Agent for extraction, source system credentials (secrets) required for workflow execution are stored in a Secret Manager This guide provides steps to set up workflows with Secure Agent and specify the secret details it uses during workflow execution. ### Before you begin ‚Äã Before configuring Secure Agent for workflow execution, ensure you have: - A registered and active Secure Agent. - Access to one of the supported secret stores: AWS Secrets Manager, Azure Key Vault, GCP Secret Manager, environment variable-based secret injection technique, or a custom secret store. ### Configure secrets retrieval for workflow execution ‚Äã Follow these steps to configure Secure Agent to retrieve secrets from a secret store required for the workflow execution This is necessary for secure data access while running your workflows. üí™ **Did you know?** For each field, you can enter either the name of a secret stored in your secret manager or the actual value Use secret names when using a secret store with Secure Agent, or enter values directly if no secret is required. - AWS - Azure - GCP - Environment variables - Custom store Secure Agent retrieves the required secrets from AWS Secrets Manager during workflow execution Follow these steps to configure retrieval under the Secure Agent configuration section: - **Secret path in Secret Manager:** Provide the Amazon Resource Name (ARN) or the path of the secret that contains the sensitive configuration details required for the connector These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution. - **AWS region:** Select the region where your AWS Secrets Manager is located. - **AWS authentication method:** Select how you want the Secure Agent to authenticate when executing the workflow Choose one: - **IAM (Recommended)**: Use this method if the secure agent was configured to use the AWS IAM permissions to access secrets. - **IAM Assume Role**: Use this method if the agent was configured to access secrets via cross-account roles. - **AWS Assume Role ARN**: Provide the IAM Role ARN that grants the Secure Agent permission to retrieve secrets. - **Access Key & Secret Key**: Use this method if the agent was configured to use the AWS Access Key ID and Secret Access Key via environment variables or Kubernetes secrets. ### Next steps ‚Äã After configuring the Secure Agent, return to your connector‚Äôs setup guide and continue the workflow setup. - Before you begin - Configure secrets retrieval for workflow execution - Next steps",
    "metadata": {
      "topic": "Configure workflow execution",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/configure-secure-agent-for-workflow-execution",
      "keywords": [
        "configure",
        "workflow",
        "execution",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_virtual_machine_k3s_0",
    "text": "On this page **Did you know?** Secure Agent installation can be done by a non-root user Root access is **only** needed for setting up system prerequisites before installation This page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying K3s in a rootless execution mode%E2%80%8B). ## System requirements ‚Äã Before installing the Secure Agent, ensure that the virtual machine (VM) meets the following requirements: - At least 80GB of available disk space. - A Linux-based OS running on an amd64 (x86_64) architecture with `systemd` enabled. - The Secure Agent requires the following ports for internal services Ensure these ports are open and accessible: - Kubernetes API: `6443` - Internal K3s proxy: `10443`, `10080` - MinIO storage: `9000`, `32075` - MinIO console: `9001`, `30614` - Traefik ingress: `31037`, `32547` ## Prerequisites ‚Äã Before installing the Secure Agent, complete the following setup steps to prepare your Atlan tenant and virtual machine. ### Configure Atlan tenant ‚Äã In Atlan, complete the following steps to configure the tenant: 1 Sign in to your tenant as an Atlan admin. 2 From the left menu of any screen, click **Admin**. 3 Under _Workspace_ click **Labs**. 4 Navigate to _Workflow Center_. 5 Enable the **Crawl assets using Secure Agent** toggle. ### Configure virtual machine ‚Äã On the virtual machine, complete the following steps to configure it: 1 Log in as a root user. 2 Create the required directory to configure cgroup delegation with: ```codeBlockLines_e6Vv sudo mkdir -p /etc/systemd/system/user@.service.d ``` 3 Use the below `cat` command to create the delegation file with required configuration: ```codeBlockLines_e6Vv cat <<EOF | sudo tee /etc/systemd/system/user@.service.d/delegate.conf [Service] Delegate=cpu cpuset io memory pids EOF ``` 4 Use the below command to reload systemd: ```codeBlockLines_e6Vv sudo systemctl daemon-reload && sudo reboot ``` 5 To keep the Secure Agent running after logout, the root user must enable service persistence for the user installing it by running the following command: ```codeBlockLines_e6Vv sudo loginctl enable-linger ``<user_installing_secure_agent>`` ``` - Replace `<user_installing_secure_agent>` with the actual username of the user installing the Secure Agent. 6 Run the following commands to enable IP forwarding so Secure Agent can communicate with other Secure Agent instances and make network requests to the Atlan tenant. - IPv4 forwarding: ```codeBlockLines_e6Vv sudo sysctl -w net.ipv4.ip_forward=1 ``` - IPv6 forwarding: ```codeBlockLines_e6Vv sudo sysctl -w net.ipv6.conf.all.forwarding=1 ``` 7 To manage containerized workloads, install fuse-overlayfs with: ```codeBlockLines_e6Vv sudo yum install fuse-overlayfs ``` 8 The VM must have access to the source system‚Äôs secret manager to retrieve secrets",
    "metadata": {
      "topic": "Install on Virtual Machine (K3s)",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/k3s/install-secure-agent-on-virtual-machine-k3s",
      "keywords": [
        "install",
        "virtual",
        "machine",
        "k3s",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_virtual_machine_k3s_1",
    "text": "To manage containerized workloads, install fuse-overlayfs with: ```codeBlockLines_e6Vv sudo yum install fuse-overlayfs ``` 8 The VM must have access to the source system‚Äôs secret manager to retrieve secrets For more information, see how to provide access for some popular secret managers listed below: - **AWS:** Configure access for AWS Secrets Manager. - **Azure:** Configure access for Azure Key Vault. - **GCP:** Configure access for GCP Secret Manager. ## Permissions required ‚Äã Before installing the Secure Agent, the user must have the following permissions: - Create and modify directories in the user‚Äôs home directory: `~/.config/systemd/user`, `~/bin`, `~/.local/bin`, and `~/.rancher`. - Create and write log files. - Execute standard Linux commands: `mkdir`, `chmod`, `tar`, and `sed`. ## Download Agent packages ‚Äã Follow these steps to download the necessary packages for setting up the Secure Agent. **Did you know?** The steps require Internet access to download files In case the VM has no Internet connectivity, one can download them separately and copy the files to the VM. - Create a folder for deployment and navigate to it: ```codeBlockLines_e6Vv mkdir -p atlan-secure-agent && cd atlan-secure-agent ``` - Run the following commands to download the required packages: - Download the _Kubernetes install package_, which contains files to run K3s on an air-gapped VM: ```codeBlockLines_e6Vv curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/k3s_offline_package_main.tar ``` - Download the _Container images package_ if an image registry isn't available: ```codeBlockLines_e6Vv curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_images_main.tar ``` - Download the _Secure Agent install package_, which contains files for running the Secure Agent: ```codeBlockLines_e6Vv curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_install_config_main.tar.gz ``` - Verify that all the files are downloaded. ## Install Secure Agent ‚Äã Follow these steps to install and configure the Secure Agent on the virtual machine. **Did you know?** The installation can be performed by both root (administrative) and non-root (standard) users. - Navigate to the deployment folder (if not already): ```codeBlockLines_e6Vv cd atlan-secure-agent ``` - Run the following command to extract the Secure Agent install package: ```codeBlockLines_e6Vv tar -xvf atlan_install_config_main.tar.gz ``` - The `rootless-install` folder is extracted from the Secure Agent install package Run the following command to create an environment file using the `env.sample` file located in the `rootless-install` folder: ```codeBlockLines_e6Vv cp ./rootless-install/.env.sample .env ``` - Open the `.env` file and update these variables: ```codeBlockLines_e6Vv VAR_ATLAN_SECURE_AGENT_NAME=prod-atlan-agent-vm VAR_ATLAN_DOMAIN=tenant.atlan.com VAR_ATLAN_TOKEN=<atlan-api-token> VAR_ATLAN_DATA_PATH=</absolute/path/to/atlan-secure-agent> ``` - Replace the environment variable values: - `VAR_ATLAN_SECURE_AGENT_NAME:` Specify a meaningful and unique name for the Secure Agent For example, `prod-atlan-agent-vm`. - `VAR_ATLAN_DOMAIN:` Enter your Atlan tenant domain For example, `tenant.atlan.com`. - `VAR_ATLAN_TOKEN:` Provide the API key (Bearer token) For more information on generating an API key, see _Create a bearer token_. - `VAR_ATLAN_DATA_PATH:` Specify the path where the `atlan-secure-agent` directory is located. - Run the following command to grant execution permission for the setup script: ```codeBlockLines_e6Vv chmod +x rootless-install/setup.sh ``` - The extracted `setup.sh` file installs the Secure Agent and K3s",
    "metadata": {
      "topic": "Install on Virtual Machine (K3s)",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/k3s/install-secure-agent-on-virtual-machine-k3s",
      "keywords": [
        "install",
        "virtual",
        "machine",
        "k3s",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "install_on_virtual_machine_k3s_2",
    "text": "For example, `tenant.atlan.com`. - `VAR_ATLAN_TOKEN:` Provide the API key (Bearer token) For more information on generating an API key, see _Create a bearer token_. - `VAR_ATLAN_DATA_PATH:` Specify the path where the `atlan-secure-agent` directory is located. - Run the following command to grant execution permission for the setup script: ```codeBlockLines_e6Vv chmod +x rootless-install/setup.sh ``` - The extracted `setup.sh` file installs the Secure Agent and K3s Run the following command to execute the installer: ```codeBlockLines_e6Vv ./rootless-install/setup.sh .env ``` - While the installation is in progress, you can run the following command to verify the progress: ```codeBlockLines_e6Vv kubectl get pods -A ``` ## Verify installation ‚Äã After installing the Secure Agent, verify that it's running correctly You can check its status through the Atlan UI or by accessing the Agent UI on K3s. 1 Log in as an Atlan admin or a similar role to access your tenant For example: `https://<tenant>.atlan.com`. 2 Navigate to the **Agent** tab. 3 In the _Secure Agents_ list, use the _Search for agents_ box to enter your Secure Agent name. 4 If the agent appears in the list and is marked **Active**, installation is complete. ## Troubleshooting ‚Äã If you encounter issues during installation, follow these steps: - Check the logs using the following command for detailed error messages that may indicate the root cause: ```codeBlockLines_e6Vv tail -f logs/k3s.log ``` - For K3s rootless mode issues, follow the K3s official documentation for troubleshooting rootless issues If you continue to face issues, contact Atlan support by creating a ticket. - System requirements - Prerequisites - Permissions required - Download Agent packages - Install Secure Agent - Verify installation - Troubleshooting",
    "metadata": {
      "topic": "Install on Virtual Machine (K3s)",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/secure-agent/how-tos/k3s/install-secure-agent-on-virtual-machine-k3s",
      "keywords": [
        "install",
        "virtual",
        "machine",
        "k3s",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "deployment_architecture_0",
    "text": "On this page The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. ## High-level architecture ‚Äã This section describes how the Secure Agent is structured and deployed It explains the core components that enable metadata extraction, job execution, and communication with Atlan. !Secure agent acts as a gateway **Figure 1:** Atlan Secure Agent deployment architecture. ## Core components ‚Äã The Secure Agent runs as a Kubernetes-based application within a customer's private cloud or on-premises environment It consists of several key components that work together to execute metadata extraction tasks. ### Argo Workflows ‚Äã - An Argo Workflow server is deployed to coordinate all activities and launch Kubernetes workloads. - The Secure Agent uses Argo Workflows to orchestrate and manage metadata extraction jobs. - Each workflow represents a unit of work, such as extracting metadata from a source system. ### Agent orchestrator ‚Äã - A scheduled job that runs every five minutes to check for jobs that need to be executed. - It connects to the Atlan tenant, retrieves job details, and initiates workflows accordingly. ### Auxiliary services ‚Äã Additional services that support agent operations: - **Health monitoring service** sends periodic heartbeats to Atlan to confirm the agent is active. - **Logging service** uploads execution logs to Atlan for monitoring and debugging. ### Metadata extraction workflows ‚Äã - Connector-specific jobs that extract metadata from source systems. - Workflows run in isolated containers, ensuring security and reliability. ## Data flow ‚Äã The Secure Agent supports two modes of metadata transfer Each mode determines how extracted metadata is delivered to Atlan. ### Bucket relay ‚Äã Bucket relay is the recommended mode for metadata extraction In this mode, metadata is first stored in enterprise-managed cloud storage before Atlan retrieves it. !Secure agent acts as a gateway **Figure 2:** Data flow in bucket relay mode. - The Secure Agent extracts metadata and writes it to a storage bucket in the customer‚Äôs cloud environment (such as AWS S3, Azure Blob Storage, or Google Cloud Storage) This is managed by providing the agent write access to cloud storage. - Atlan retrieves metadata from the storage bucket and processes it further This is managed by providing Atlan read access to list and read files in cloud storage. - This mode ensures the extracted data remains within the customer‚Äôs infrastructure until Atlan explicitly fetches it Customers can also use this data for auditing. ### Direct ingestion ‚Äã In direct ingestion mode, metadata is transferred directly from the Secure Agent to Atlan. !Secure agent acts as a gateway **Figure 3:** Data flow in direct ingestion mode. - The Secure Agent uses pre-signed URLs to upload metadata directly to Atlan",
    "metadata": {
      "topic": "Deployment architecture",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/secure-agent/references/deployment-architecture",
      "keywords": [
        "deployment",
        "architecture",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "security_0",
    "text": "On this page The Secure Agent is designed with multiple security controls to protect metadata, credentials, and communication between systems This document outlines its security mechanisms across authentication, encryption, container security, network security, and logging and monitoring. ## Authentication and authorization ‚Äã The Secure Agent implements security measures for authentication, encryption, and access control This section details authentication mechanisms, including API key management and secret handling. ### API key management ‚Äã The Secure Agent uses API keys for authentication when communicating with Atlan These keys verify the agent‚Äôs identity and define its access scope. - **Authentication:** API keys authenticate the Secure Agent, allowing it to interact securely with Atlan Each key is associated with a specific tenant and grants access based on permissions. - **Storage:** API keys are stored in enterprise-managed vaults, such as AWS Secrets Manager, Azure Key Vault, or Kubernetes Secrets The Secure Agent retrieves the key dynamically during operation, eliminating manual configuration. - **Expiration:** API keys can have an expiration period, such as 90-180 days, or be configured based on internal security policies. - **Rotation:** When an API key nears expiration, a new key can be generated and stored in the secret vault The Secure Agent automatically fetches the latest key from the vault. - **Revocation:** If an API key is compromised, it can be revoked Once revoked, the Secure Agent retrieves a newly assigned key from the vault without requiring manual intervention. ### Secret management ‚Äã The Secure Agent retrieves credentials securely without storing them locally. - **Enterprise-managed vaults:** The Secure Agent integrates with AWS Secrets Manager, Azure Key Vault, and other vaults to securely store credentials, keeping them within the organization‚Äôs security perimeter. - **Just-in-time access:** Credentials, such as database secrets, are retrieved dynamically from enterprise vaults when needed and are never stored locally. - **No credential transmission:** Secrets are never transmitted to or stored on Atlan, ensuring complete isolation of sensitive information. ## Data security and encryption ‚Äã The Secure Agent protects metadata using encryption and strict access controls. - **Compliance with security standards:** The Secure Agent aligns with ISO 27001 and SOC 2 security standards, ensuring strong encryption, data protection, and access control measures. - **Data in transit:** All communication between the Secure Agent and Atlan is encrypted using TLS 1.2 over HTTPS For network-level protections, see Network security. - **Data at rest:** Metadata stored in customer-managed storage or Atlan‚Äôs tenant bucket is encrypted using AES-256. - **Data minimization:** Only essential metadata is extracted and transmitted Customers can configure data filters to exclude specific metadata fields from processing. - **Retention control:** Atlan doesn't require metadata post-ingestion, and customers can delete metadata from their storage buckets based on internal security policies. ## Container security ‚Äã The Secure Agent implements security measures to protect container images, ensuring their integrity and mitigating security risks. - **Container image hosting:** Secure Agent container images are hosted on public repositories, such as Docker Hub and Amazon ECR",
    "metadata": {
      "topic": "Security",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/secure-agent/references/security",
      "keywords": [
        "security",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "security_1",
    "text": "For network-level protections, see Network security. - **Data at rest:** Metadata stored in customer-managed storage or Atlan‚Äôs tenant bucket is encrypted using AES-256. - **Data minimization:** Only essential metadata is extracted and transmitted Customers can configure data filters to exclude specific metadata fields from processing. - **Retention control:** Atlan doesn't require metadata post-ingestion, and customers can delete metadata from their storage buckets based on internal security policies. ## Container security ‚Äã The Secure Agent implements security measures to protect container images, ensuring their integrity and mitigating security risks. - **Container image hosting:** Secure Agent container images are hosted on public repositories, such as Docker Hub and Amazon ECR Organizations can deploy the Secure Agent from a private container registry to meet their compliance and security requirements. - **Vulnerability scanning:** Trivy scans container images for known vulnerabilities, outdated dependencies, misconfigurations, and exposed secrets Scans are conducted weekly and whenever new changes are checked in. - **Image signing and verification:** Cosign signs container images to ensure authenticity Image verification includes: - Validating the image signature against Sigstore's transparency log. - Verifying the signer‚Äôs identity through GitHub workflows. - Confirming the certificate issued by GitHub‚Äôs OpenID Connect (OIDC) provider. - **License compliance:** Trivy scans for software license compliance to ensure proper licensing for all components within the container images. ## Network security ‚Äã The Secure Agent operates within a controlled network environment to facilitate secure metadata extraction and communication with Atlan. ### SSL certificates ‚Äã The Secure Agent encrypts communications with Atlan, source systems, proxy servers, and secret managers. - **Encryption in transit:** All data communication between the Secure Agent and Atlan is encrypted using TLS 1.2 over HTTPS. - **Certificate management:** - If trusted or well-known certificate authorities are used, no additional configuration is needed The Default Trusted Certificate Authorities store contains certificates from the most common and trusted CAs, which the Secure Agent uses to secure connections. - If internal or private certificate authorities are used, the Secure Agent trusts these custom certificate authorities through the infrastructure‚Äôs default certificate store. ### Whitelisting ‚Äã Configuring network access ensures only trusted communication between the Secure Agent and Atlan. - **Domain whitelisting:** The Secure Agent requires outbound access to Atlan through the domain `tenant.atlan.com` Domain-based whitelisting simplifies network configurations while maintaining security. - **DNS resolution:** The Secure Agent relies on standard DNS resolution to reach Atlan domains Network configurations must allow name resolution for `tenant.atlan.com`. - **IP-based whitelisting:** If domain-based whitelisting isn‚Äôt feasible and specific IP ranges must be allowed, refer to the list of required IP ranges to be whitelisted If you need further assistance, contact Atlan Support. ## Logging and monitoring ‚Äã The Secure Agent captures logs for workflow execution, system orchestration, and Kubernetes operations while also providing monitoring capabilities. ### Types of logs ‚Äã - **Workflow logs:** Capture job execution details, including start and completion status, connections to source systems and secret managers, metadata extraction results, and authentication status",
    "metadata": {
      "topic": "Security",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/secure-agent/references/security",
      "keywords": [
        "security",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "security_2",
    "text": "Network configurations must allow name resolution for `tenant.atlan.com`. - **IP-based whitelisting:** If domain-based whitelisting isn‚Äôt feasible and specific IP ranges must be allowed, refer to the list of required IP ranges to be whitelisted If you need further assistance, contact Atlan Support. ## Logging and monitoring ‚Äã The Secure Agent captures logs for workflow execution, system orchestration, and Kubernetes operations while also providing monitoring capabilities. ### Types of logs ‚Äã - **Workflow logs:** Capture job execution details, including start and completion status, connections to source systems and secret managers, metadata extraction results, and authentication status These logs are sent to Atlan and accessible from the workflow status page. - **Orchestration logs:** Track the Secure Agent‚Äôs scheduled operations, including connection attempts to Atlan, retrieval of workflow requests, and workflow submission to the Argo engine Logs also include error messages and performance metrics. - **Argo logs:** Provide visibility into workflow execution, including job scheduling, resource allocation, state transitions, and error handling. - **Kubernetes logs:** Capture system-level events, such as pod lifecycle changes, container startup and shutdown, resource allocation, network connectivity, and health checks. ### Monitoring ‚Äã - **Health checks:** Secure Agent components run periodic health checks to verify connectivity, resource availability, and system integrity. - **Resource utilization:** CPU, memory, and storage usage are monitored to track system load and detect potential bottlenecks Logs can be viewed in Atlan or integrated with external monitoring systems. - Authentication and authorization - Data security and encryption - Container security - Network security - Logging and monitoring",
    "metadata": {
      "topic": "Security",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/secure-agent/references/security",
      "keywords": [
        "security",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "customer_support_0",
    "text": "On this page One of Atlan's core values is to help you and your team do your life's best work. üíô That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data Atlan's customer support is a combination of several teams in Atlan: - Product support personnel - Cloud support personnel - DevOps/engineering support personnel - Vast repository of self-service resources ## Service-level commitment ‚Äã Atlan's Technical Support team provides support globally with high response commitment levels This includes 24/7 SRE support for critical (P0) issues Customers get a service-level commitment, including the following: - 99.5% uptime for Atlan - Dedicated support center, available from within the Atlan product - Commitments for aggressive response times for business critical issues - Designated Customer Success Manager to assist with escalations ## Ways to contact support ‚Äã - ‚úâÔ∏è Email support at a dedicated customer support email account ( ask@atlan.com) - üë®‚Äçüíª In-product support widget to log tickets and a help desk portal to log and track tickets You can sign up to track support tickets on the help desk portal You must use your organizational email address as the username and create a password. - üìù Submit a support request via the online form To track your support tickets: 1 Navigate to https://atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. 2 From the top right, click your avatar, and then from the dropdown, click **My activities**. 3 On the _My activities_ page, you can do the following: - _My requests_ and _Requests I'm CC'd on_ - view and edit the support tickets you either created or were copied on, respectively. - _Organization requests_ - to access all other support tickets for your organization, please reach out to your customer success manager Atlan provides you with read access to all the support tickets for your organization To be able to comment on or close them, you must be CC'd on all tickets. ## Hours of operation ‚Äã 24x7 availability for all requests and issues ## Severity levels ‚Äã The Atlan Technical Support team determines the severity of an issue The customer's position is considered, and these guidelines are followed to determine priority Below are the response time SLAs: | Severity | Description | Basic support | Advanced support | | --- | --- | --- | --- | | S0 | Production software is unavailable; all customers are blocked and productivity halted | 2 hours | 1 hour | | S1 | Production software is available; functionality or performance is severely impaired | 4 hours | 2 hours | | S2 | Production software is available and usable with partial, noncritical loss of functionality Or, production software has an occasional issue that customer requests identification and resolution",
    "metadata": {
      "topic": "Customer support",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/support/references/customer-support",
      "keywords": [
        "customer",
        "support",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "customer_support_1",
    "text": "Below are the response time SLAs: | Severity | Description | Basic support | Advanced support | | --- | --- | --- | --- | | S0 | Production software is unavailable; all customers are blocked and productivity halted | 2 hours | 1 hour | | S1 | Production software is available; functionality or performance is severely impaired | 4 hours | 2 hours | | S2 | Production software is available and usable with partial, noncritical loss of functionality Or, production software has an occasional issue that customer requests identification and resolution Also includes requests for help with administrative tasks | 16 hours | 4 hours | | S3 | Cosmetic issues or request for general information about the software, documentation, processes, or procedures | 24 hours | 14 hours | ## Escalation procedure ‚Äã If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood You may further escalate by contacting: 1. **1st level of escalation**: Technical Support Engineer 2. **2nd level of escalation**: Director, Support 3. **3rd level of escalation**: Head of Customer Experience - Service-level commitment - Ways to contact support - Hours of operation - Severity levels - Escalation procedure",
    "metadata": {
      "topic": "Customer support",
      "category": "Reference",
      "source_url": "https://docs.atlan.com/support/references/customer-support",
      "keywords": [
        "customer",
        "support",
        "reference",
        "documentation",
        "specifications"
      ]
    }
  },
  {
    "id": "allow_members_to_view_reports_0",
    "text": "On this page Who can do this You will need to be an admin user in Atlan to allow member users to view the reporting center Admin users can control access to the reporting center for member users in their organization If enabled, member users will be able to view the following dashboards: - Assets dashboard to monitor assets - Glossary dashboard to track metrics for glossaries, categories, and terms - Insights dashboard to track metrics for queries - Usage and cost dashboard to track asset usage and associated costs danger Permission to view the governance and automations dashboards in the reporting center is reserved for admin users only To allow member users to view the reporting center , follow these steps. ## Enable member users to view reports ‚Äã To enable member users to view the reporting center: 1 From the left menu in Atlan, click **Admin**. 2 Under _Workspace_, click **Labs**. 3 Under the _Access control_ heading of the _Labs_ page, turn on **Allow member users to access Reporting Center** Your member users will now be able to view the assets, glossary, Insights, and usage and cost dashboards in the reporting center! üéâ If you would like to revoke access, follow the steps above to turn it off. - Enable member users to view reports",
    "metadata": {
      "topic": "Allow members to view reports",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-members-to-view-reports",
      "keywords": [
        "allow",
        "members",
        "view",
        "reports",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "enable_discovery_of_process_as_0",
    "text": "On this page Who can do this You will need to be an admin user in Atlan to enable discovery and tracking of process assets Processes represent the movement or transformation of assets in Atlan By default, process assets are hidden on the assets page and reporting center to ensure an efficient asset search, filtering, and discovery experience To create a more customizable experience for your users, you can turn on discovery and tracking of process assets Enable process discovery A Enable process discovery !Begin by navigating to the admin center!Enable user permissions from here Interactive walkthrough reCAPTCHA **Did you know?** Even if discovery of process assets is turned off, users will still be able to view process assets on the lineage graph and sidebar. ## Enable process asset discovery ‚Äã To enable discovery and tracking of process assets: 1 From the left menu in Atlan, click **Admin**. 2 Under _Workspace_, click **Labs**. 3 Under the _Assets_ heading of the _Labs_ page, turn on **Discover and track processes** Your users will now be able to search, filter, discover, and track process assets! üéâ If you'd like to disable this feature, follow the steps above to turn it off. - Enable process asset discovery",
    "metadata": {
      "topic": "Enable  discovery of process assets",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-discovery-of-process-assets",
      "keywords": [
        "enable",
        "discovery",
        "process",
        "assets",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "restrict_asset_visibility_0",
    "text": "On this page Who can do this You will need to be an admin user in Atlan to restrict asset visibility Note that glossary access works slightly differently To build effective data governance, you need to control data access across your organization Atlan handles this through access control policies By default in Atlan, all users can see the existence of all assets The _All assets_ view includes all assets in your Atlan data estate To change this default setting, see Disable all assets view Generally, you may want to limit specific teams from being able to see all assets in Atlan All you need to do is turn off the default behavior, so that your member and guest users will only have access to the assets curated through their personas and purposes Admin users will still have full access to all assets, even when this default behavior is turned off. ## Summary ‚Äã - View all assets in Labs is OFF: To view any assets, a persona is required Personas are necessary to grant access for viewing. - View all assets in Labs is ON: All assets are visible by default, even without a persona However, only the default metadata will be displayed and the rest will be locked Personas are needed to restrict access to view specific assets. ## Example ‚Äã Imagine a user who belongs to an `Insurance Member` purpose Once you turn off the default behavior, this user will have access to: - View details only about the curated assets in the `Insurance Member` purpose. - Search and discover only the curated assets in the `Insurance Member` purpose. - View details only about the related assets in the `Insurance Member` purpose. - View only the linked assets in the `Insurance Member` purpose for glossary terms. - View all assets in the lineage graph, but only view details in the sidebar for assets in the `Insurance Member` purpose If the user is a member of multiple purposes and personas, the assets they can access will be a superset of assets across all those purposes and personas. danger Turning off the default visibility of all assets currently does not apply to the _Explorer_ section in _Insights_ or the requests widget Member and guest users will still be able to view all assets in _Insights_ and the requests widget View assets by persona or purpose A View assets by persona or purpose !In this example, we have logged into Atlan as a member user with the default all assets view turned off!Member and guest users will only be able to view the recently verified assets for their persona or purpose Illustrative examples reCAPTCHA Disable all assets view A Disable all assets view !Begin by navigating to the admin workspace!Manage user permissions from here Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Disable all assets view ‚Äã The _All assets_ view on the _Assets_ page is enabled for all users by default",
    "metadata": {
      "topic": "Restrict asset visibility",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-asset-visibility",
      "keywords": [
        "restrict",
        "asset",
        "visibility",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "restrict_asset_visibility_1",
    "text": "View assets by persona or purpose A View assets by persona or purpose !In this example, we have logged into Atlan as a member user with the default all assets view turned off!Member and guest users will only be able to view the recently verified assets for their persona or purpose Illustrative examples reCAPTCHA Disable all assets view A Disable all assets view !Begin by navigating to the admin workspace!Manage user permissions from here Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Disable all assets view ‚Äã The _All assets_ view on the _Assets_ page is enabled for all users by default To turn it off for your member and guest users, complete the following steps To disable all assets view: 1 From the left menu in Atlan, click **Admin**. 2 Under _Workspace_, click **Labs**. 3 Under the _Access Control_ heading of the _Labs_ page, turn off **View \"All assets\" in Assets Discovery** Your member and guest users will now only have access to the curated assets for their persona or purpose by default! üéâ If you'd like to enable the all assets view, follow the steps above and then turn it on. **Did you know?** If a user does not belong to any persona or purpose and the _All assets_ view is disabled, then the user will be prompted to reach out to their Atlan administrator and request to be added to a persona or purpose For more questions on restricting asset visibility, head over here. - Summary - Example - Disable all assets view",
    "metadata": {
      "topic": "Restrict asset visibility",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-asset-visibility",
      "keywords": [
        "restrict",
        "asset",
        "visibility",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "restrict_glossary_visibility_0",
    "text": "On this page Who can do this You will need to be an admin user in Atlan to restrict glossary visibility Note that asset access works slightly differently Once you have restricted glossary visibility: - If a glossary policy has been configured for a selected persona, only the user belonging to that persona be able to view the glossaries curated through glossary policies. - If a user isn't part of any persona, they're unable to view any glossaries in Atlan. - If a user is part of a persona that doesn't have glossary policies specifically or any policies at all configured, the user is unable to view all glossaries. - The restriction is applicable to the _Assets_ page, glossary tree on the _Glossary_ page, terms filter on the _Assets_ page, terms in an asset sidebar, linked assets, and related terms The _All glossaries_ view on the _Glossary_ page is enabled for all users by default To turn it off for your member and guest users, complete the following steps To disable all glossaries view: 1 From the left menu in Atlan, click **Admin**. 2 Under _Workspace_, click **Labs**. 3 Under the _Access control_ heading on the _Labs_ page, turn on **Persona switcher in Glossary** Your member and guest users now only have access to the curated glossaries for their persona! üéâ If you'd like to restore the default all glossaries view, repeat steps 1 to 3 and then turn off the toggle. ## Summary ‚Äã - Persona switcher in Glossary in Labs is OFF: Users can see all glossaries. - Persona switcher in Glossary in Labs is ON: Users can't see any glossaries until provided in a persona. **Did you know?** If a user doesn't belong to any persona and the _All glossaries_ view is disabled, then the user is prompted to reach out to their Atlan administrator and request to be added to a persona. - Summary",
    "metadata": {
      "topic": "Restrict glossary visibility",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-glossary-visibility",
      "keywords": [
        "restrict",
        "glossary",
        "visibility",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "view_event_logs_0",
    "text": "On this page Who can do this You will need to be an admin user in Atlan to view event logs Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan Event logs are currently stored in Atlan for 7 days You will first need to configure any of the following supported sources to receive events: - Airflow/OpenLineage - Amazon MWAA/OpenLineage - Astronomer/OpenLineage - Google Cloud Composer/OpenLineage - Apache Spark/OpenLineage - Anomalo Once you have configured a supported source, you can view event logs for your events from the admin center: - View a list of the 20 most recently received events For every event, you can also view the timestamp for when it was received in Atlan based on your local timezone and 24-hour time notation, name of the connector configured, and event details. - Filter events by connectors ‚Äî Airflow (also includes all other supported distributions, Amazon MWAA, Astronomer, and Google Cloud Composer), Apache Spark, and Anomalo. - Expand any event to view the full JSON code. ## View event logs ‚Äã You can view event logs in Atlan through two methods, depending on your role and workflow preferences: ### Via admin panel ‚Äã To view event logs: View event logs A View event logs !Begin by navigating to the admin center!Track OpenLineage events from the event logs Interactive walkthrough 1 From the left menu of any screen in Atlan, click **Admin**. 2 Under the _Logs_ heading of your admin _Workspace_, click **Event logs**. 3 On the _Event logs_ page, you can view a list of up to 20 most recently received events in Atlan. 4. (Optional) Click the **Connector** dropdown to filter events by the connector configured: - Click **Airflow** to view events received through Airflow, Amazon MWAA, Astronomer, or Google Cloud Composer connections. - Click **Spark** to view events received through Apache Spark connections. - Click **Anomalo** to view events received through Anomalo connections. 5. (Optional) Click the refresh button to refresh event logs and view the latest events. 6 For any event listed in the event logs, you can view the timestamp for when it was received in Atlan, name of the connector configured, and event details. (Optional) Click any event to view more details in the _Event details_ sidebar: - View the JSON code, connector name, and timestamp for the event received When viewing the code, you can also click the brackets to collapse or expand them. - Click the copy icon to copy the event details. - Click the expand icon to view the JSON code in fullscreen mode. ### Via connection profile ‚Äã caution ü§ì **Who can do this?** You must be an admin user in Atlan to view event logs View event logs A View event logs !Begin by navigating to the workflows center!Select the Manage tab to track events Interactive walkthrough 1 Navigate to **Workflow > Manage**. 2 Select the **Listeners** tab. 3",
    "metadata": {
      "topic": "view event logs",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/logs/how-tos/view-event-logs",
      "keywords": [
        "view",
        "event",
        "logs",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "view_query_logs_0",
    "text": "Who can do this You will need to be an admin user in Atlan to view query logs The query log helps you track all queries run in Atlan, including: - saved and unsaved queries in the Insights query editor - queries run through both the Atlan UI and API - sample data previews from asset profiles You can also view additional details and run status for each query and use filters to track specific queries Query logs are persisted throughout the lifecycle of the Atlan instance for your organization View query logs View query logs !Begin by navigating to the admin center!Track queries from the query logs Interactive walkthrough reCAPTCHA To view query logs: 1 From the left menu of any screen in Atlan, click **Admin**. 2 Under the _Logs_ heading of your admin _Workspace_, click **Query logs**. 3 On the _Query logs_ page, you can view all the queries that your users have run or are running in Atlan. 4. (Optional) Click the funnel icon to filter queries and then: - Click **Status** to filter queries by run status ‚Äî _Succeeded_, _Failed_, or _Aborted_. - Click **Users** to filter queries by Atlan users. 5. (Optional) Use the search bar to search for queries using specific keywords. 6 The default date range is set to 30 days Use the date filter to view query logs for the last 7 days, past 3 or 6 months, or a custom date range of your choice. 7 For any query listed in the query logs, you can view the query name, connection, execution details, user that run the query, and timestamp for when the query was run. (Optional) Click any query to view more details in the _Query details_ sidebar: - In the _Query details_ sidebar, you can view the full query, connection, database, schema, and asset name, query status, and query run time. - Click the copy icon to copy the query and use it as a template for writing your own queries. - Click the expand icon to see the full query. - For _Query Source_, click **Copy ID** to copy the query ID",
    "metadata": {
      "topic": "view query logs",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/logs/how-tos/view-query-logs",
      "keywords": [
        "view",
        "query",
        "logs",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "create_readme_templates_0",
    "text": "On this page Who can do this You will need to be an admin user in Atlan to create and manage README templates Admin users in Atlan can create, curate, and manage README templates from the governance center Once admin users have created the templates, other users will be able to select these templates and enrich their assets with READMEs They will also be able to see a rich preview of each template before adding the relevant documentation. ## Create a README template ‚Äã Create a README template A Create a README template !Begin by navigating to the governance center!Add and manage README templates from here Interactive walkthrough To create a README template: 1 In the left menu in Atlan, click **Governance**. 2 Under the _Governance_ heading, click **Readme templates**. 3 On the _Readme templates_ page, click **Get started**. 4 In the _Untitled Template_ dialog box, enter the following details: 1 For template name, enter a name for your template. 2. (Optional) For _Add description_, add a description for your template. 3. (Optional) To add an icon, click on the **image** icon. 5 Click **Create** to proceed. 6 In the text editor, add your template. 7 Click **Save**. 8. (Optional) To edit the template, in the upper right of the screen, click the **pencil** icon Your README template is now available in the templates manager! üéâ - Create a README template",
    "metadata": {
      "topic": "Create README templates",
      "category": "How-to Guides",
      "source_url": "https://docs.atlan.com/product/administration/readme-templates/how-tos/create-readme-templates",
      "keywords": [
        "create",
        "readme",
        "templates",
        "how-to guides",
        "tutorial",
        "guide",
        "instructions"
      ]
    }
  },
  {
    "id": "atlan_ai_security_0",
    "text": "On this page **Did you know?** Atlan uses Azure OpenAI Service to power Atlan AI Atlan does **not** send any data to the AI service and only uses metadata for supported capabilities For questions about data security, see below Learn more about how Atlan AI processes and stores your data: #### What services does Atlan AI use? ‚Äã Atlan uses Azure OpenAI Service to power Atlan AI Specifically, Atlan uses GPT-4o, a large, pretrained AI model. #### What data does Atlan send to the AI service? ‚Äã Atlan does **not** send any data to the AI service Atlan only sends metadata for supported capabilities For example: - Atlan AI-suggested asset descriptions - table, view, column, database, or schema name. - Atlan AI-suggested term descriptions - glossary name and description, category name and description, and term name. - Atlan AI-suggested lineage explanations - SQL transformations in lineage with upstream and downstream asset names. - Atlan AI-suggested aliases - table, view, column, database, or schema name. - Atlan AI-suggested READMEs for terms - glossary, category, and term name and description, and any existing READMEs within the same glossary. #### Does Atlan use any metadata or data to train Atlan AI? ‚Äã No, Atlan does **not** use your metadata or data for fine-tuning or training AI models. #### Is the data processed through Atlan AI encrypted? ‚Äã Atlan makes HTTPS requests from your tenant, applicable to all supported cloud platforms The data is encrypted in transit using TLS 1.2, AWS PrivateLink, or Azure virtual network peering Atlan uses 256-bit Advanced Encryption Standard (AES-256) algorithm to encrypt data at rest. #### How does Atlan ensure security development of Atlan AI? ‚Äã Atlan AI follows OWASP Top 10 that includes application security reviews and Static Application Security Testing (SAST) tools. #### Does Atlan AI comply with any governance or legal frameworks? ‚Äã While Atlan is HIPAA and GDPR compliant, Atlan AI is currently not As Atlan AI matures, compliance continues to be our key focus. #### Does Atlan AI process PII or other sensitive data? ‚Äã Atlan AI only processes user input and metadata, which typically do not contain PII or sensitive data However, it is the organization's responsibility to ensure that PII or other sensitive data is not available in the metadata or shared via user input. #### What is the data retention policy for Atlan AI? ‚Äã Atlan does **not** store any data for Atlan AI This is enforced in the following two ways: - Atlan has an exemption from Microsoft to not store any data",
    "metadata": {
      "topic": "Atlan AI security",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/concepts/security",
      "keywords": [
        "atlan",
        "security",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "atlan_ai_0",
    "text": "On this page ‚ûï **Premium feature!** This feature will be a paid addition Reach out to your customer success manager for more information Atlan AI is the first-ever copilot for all humans of data! üöÄ You can use Atlan AI to supercharge the documentation of your data assets and gain meaningful insights from your data estate in Atlan. **Did you know?** Atlan uses Azure OpenAI Service to power Atlan AI Atlan does **not** send any data to the AI service and only uses metadata for supported capabilities For questions about data security, see Atlan AI security. ## Enable Atlan AI ‚Äã Who can do this You will need to be an admin user in Atlan to enable Atlan AI Only admin users in Atlan can enable Atlan AI for their organization Once enabled, each user's existing permissions and access policies in Atlan will determine how they can use Atlan AI For example, a user must have the permission to edit metadata to use Atlan AI for updating asset descriptions Enable Atlan AI A Enable Atlan AI !Begin by navigating to the admin workspace!Manage workspace permissions from here Interactive walkthrough reCAPTCHA To enable Atlan AI for your Atlan users: 1 From the left menu of any screen in Atlan, click **Admin**. 2 Under the _Workspace_ heading, click **Labs**. 3 On the _Labs_ page, under the _Atlan AI_ heading: - Turn on **Enrich metadata** to enable your users to use Atlan AI for documenting assets, explaining lineage transformations, and generating aliases. 4. (Optional) For _Customize Atlan AI_, click the **Add instructions** button to make Atlan AI suggestions more relevant to your organization In the _Enhance suggestions_ dialog, for _General instructions_, describe your organization and add details about your product, mission, and more, and then click **Save** You have now unleashed the power of Atlan AI for your users! üéâ If you'd like to disable Atlan AI from your organization's Atlan workspace, follow the steps above to turn it off. **Did you know?** If Atlan AI is disabled, the feature will no longer be available in your workspace However, any descriptions previously generated by Atlan AI and added to your assets or saved SQL queries will still be available. ## Current capabilities ‚Äã ### Use Atlan AI to document assets ‚Äã Atlan AI puts you in control of your data estate in Atlan, helping you curate meaningful context for your data assets Accept, reject, or edit any AI-powered suggestions, the choice is _yours_ You can use Atlan AI to: - Document tables and views with AI-generated descriptions - Document columns with AI-generated descriptions - Document terms and categories with AI-generated descriptions - Document terms with AI-generated READMEs - Add an Atlan AI-generated alias to supported assets **Did you know?** To ensure full transparency, any changes made using Atlan AI will be marked as _Updated using Atlan AI_ in the activity log. ### Use Atlan AI for lineage analysis ‚Äã Atlan AI can help you understand lineage transformations using natural language",
    "metadata": {
      "topic": "Atlan Ai",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/concepts/what-is-atlan-ai",
      "keywords": [
        "atlan",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "implement_atlan_mcp_server_0",
    "text": "On this page The **Model Context Protocol** (MCP) is an open standard that enables AI agents to access contextual metadata from external systems Atlan provides a reference implementation of MCP through the **Atlan MCP server** This server acts as a bridge between Atlan‚Äôs metadata platform and AI tools such as Claude and Cursor You can use the Atlan MCP server to support AI-driven use cases like searching for assets, understanding lineage, or updating metadata, all using real-time context from Atlan. ## Get started ‚Äã The Atlan MCP server can be configured in multiple environments depending on your preferred development setup and integration target Follow the instructions below to set up the server with your desired tool: - Cursor - Claude - Local development Set up the Atlan MCP server in Cursor: - Using uv: `uv` is a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker. - Using Docker: Use Docker to run the MCP server in an isolated, containerized environment. ## Available tools ‚Äã The Atlan MCP server exposes a set of tools that enable AI agents to interact with your metadata programmatically. - **Search assets**: Search for assets in Atlan using filters such as name, type, tags, and domains. - **Query by DSL**: Retrieve specific assets using Atlan‚Äôs DSL query language. - **Explore lineage**: Explore upstream or downstream lineage for a given asset. - **Update assets**: Modify asset metadata, including descriptions and certification status. ## Need help? ‚Äã - For troubleshooting and feature requests, see the GitHub repo. - Contact Atlan support for help with setup or integration. ## See also ‚Äã - Atlan MCP Server README on GitHub - Get started - Available tools - Need help? - See also",
    "metadata": {
      "topic": "Implement Atlan MCP Server",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/implement-the-atlan-mcp-server",
      "keywords": [
        "implement",
        "atlan",
        "mcp",
        "server",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "use_atlan_ai_for_documentation_0",
    "text": "On this page ‚ûï **Available to customers in Enterprise and Business-Critical platform editions** Who can do this Before using Atlan AI, your admin user must enable Atlan AI in your Atlan workspace Atlan AI helps you automate the process of documenting your data assets You can use Atlan AI to generate meaningful context for your assets and then simply review the content for accuracy and relevance Accept, reject, or edit any AI-powered suggestions, the choice is yours You can use Atlan AI to: - Document tables and views with AI-generated descriptions - Document child assets with AI-generated descriptions - Document terms and categories with AI-generated descriptions - Document terms with AI-generated READMEs - Add an Atlan AI-generated alias to supported assets **Did you know?** To ensure full transparency, any changes made using Atlan AI will be marked as _Updated using Atlan AI_ in the activity log. ## Add a description to a table or view ‚Äã You can use Atlan AI to add descriptions to your assets in Atlan and provide helpful context to your teams Supported asset types include: - Amazon QuickSight analyses, dashboards, and datasets - dbt Cloud and dbt Core models, sources, and tests - Looker dashboards, explores, looks, models, tiles, and views - Microsoft Power BI workspaces, dashboards, datasets, data sources, pages, reports, tables, and tiles - Mode charts, queries, and reports - Redash dashboards, queries, and visualizations - Salesforce objects, dashboards, and reports - Sigma workbooks, datasets, pages, and data elements - Snowflake streams - SQL tables, views, databases, and schemas - Tableau workbooks, worksheets, dashboards, data sources, and metrics - ThoughtSpot answers and liveboards Use Atlan AI to document tables A Use Atlan AI to document tables !Begin by navigating to your assets!(Optional) Filter assets by table asset type Interactive walkthrough To add a description to a table or view using Atlan AI: 1 From the left menu on any screen, click **Assets**. 2. (Optional) Under the search bar on the _Assets_ page, click the **Table** tab. 3. (Optional) In the _Filters_ menu on the left, click **Properties** to expand the menu and select **Description** to search for assets without a description. 4 Click an asset to view the _Overview_ tab in the sidebar. 5 Under _Description_, you can either: - For assets without a description, navigate to the text box and click **use Atlan AI** to add an Atlan AI-suggested description. - For assets with an existing description, click the text box and then click the **Improve using Atlan AI** button to replace the existing description with an Atlan AI-suggested one. 6. (Optional) At the bottom of the _Atlan AI is writing..._ box, click **Enhance now** to briefly describe your organization and help Atlan AI make more relevant suggestions - this option is only visible to admin users. 7",
    "metadata": {
      "topic": "Use Atlan AI for documentation",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
      "keywords": [
        "use",
        "atlan",
        "documentation",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "use_atlan_ai_for_documentation_1",
    "text": "Click an asset to view the _Overview_ tab in the sidebar. 5 Under _Description_, you can either: - For assets without a description, navigate to the text box and click **use Atlan AI** to add an Atlan AI-suggested description. - For assets with an existing description, click the text box and then click the **Improve using Atlan AI** button to replace the existing description with an Atlan AI-suggested one. 6. (Optional) At the bottom of the _Atlan AI is writing..._ box, click **Enhance now** to briefly describe your organization and help Atlan AI make more relevant suggestions - this option is only visible to admin users. 7 Once Atlan AI has generated a description, you can: - Click anywhere in the text box to edit the text and then click **Apply**. - Click **Apply** to add the description to your asset. - Click **Discard** to discard the AI-generated description. - Click the retry button to generate a new description, compare the two to select the most relevant option, and then click **Apply**. 8. (Optional) From the sidebar tabs on the right, click the **Activity** tab to view the changelog - including the _Updated using Atlan AI_ stamp, user information, and timestamp for the update Your AI-suggested table or view description is now live! üéâ Use Atlan AI to document columns A Use Atlan AI to document columns !Let's add more context to a column in the form of a description!Simply use Atlan AI to add a description to the column Interactive walkthrough ## Add a description to a column ‚Äã You can use Atlan AI to add descriptions to your columns from the following: - _Overview_ tab in the sidebar for column assets - _Column preview_ section in the asset profile for table and view assets - _Columns_ tab in the sidebar for table and view assets Supported asset types include: - Amazon QuickSight analysis visuals, dashboard visuals, and dataset fields - dbt Cloud and dbt Core columns - Looker fields for explores and views - Microsoft Power BI columns and measures - Salesforce fields - Sigma dataset columns and data element fields - SQL columns - Tableau data source fields and calculated fields - ThoughtSpot visualizations In this example, we'll use Atlan AI to add a description to a column from the _Column preview_ section in a table asset profile To add a description to a column using Atlan AI: 1 From the left menu on any screen, click **Assets**. 2. (Optional) Under the search bar on the _Assets_ page, click the **Table** tab. 3 Right-click an asset and then select **Open profile** to view its asset profile. 4 From the asset profile, navigate to the _Column Preview_ section and select a column to document. 5",
    "metadata": {
      "topic": "Use Atlan AI for documentation",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
      "keywords": [
        "use",
        "atlan",
        "documentation",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "use_atlan_ai_for_documentation_2",
    "text": "Right-click an asset and then select **Open profile** to view its asset profile. 4 From the asset profile, navigate to the _Column Preview_ section and select a column to document. 5 Under _Description_, click **use Atlan AI** to add an AI-generated description to the column. 6. (Optional) At the bottom of the _Atlan AI is writing..._ box, click **Enhance now** to briefly describe your organization and help Atlan AI make more relevant suggestions - this option is only visible to admin users. 7 Once Atlan AI has generated a description, you can: - Click anywhere in the text box to edit the text and then click **Apply**. - Click **Apply** to add the description to your asset. - Click **Discard** to discard the AI-generated description. - Click the retry button to generate a new description, compare the two to select the most relevant option, and then click **Apply**. 8. (Optional) From the sidebar tabs on the right, click the **Activity** tab to view the changelog - including the _Updated using Atlan AI_ stamp, user information, and timestamp for the update Your AI-suggested column descriptions are now live! üéâ Use Atlan AI to add READMEs Use Atlan AI to add READMEs !Begin by navigating to your glossaries!Select a term to generate a README using Atlan AI Illustrative example ## Add a description to glossary assets ‚Äã You can use Atlan AI to add descriptions to your terms and categories in Atlan and provide useful business context for your linked assets Use Atlan AI to document columns A Use Atlan AI to document columns !Let's add more context to a column in the form of a description!Simply use Atlan AI to add a description to the column Interactive walkthrough To add a description to a term using Atlan AI: 1 From the left menu on any screen, click **Glossary**. 2 Under _Glossary_ in the left menu, click the name of your glossary. 3 Under your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI. 4 You can either add a description from the term profile or sidebar Under _Description_, you can either: - For terms without a description, navigate to the text box and click **use Atlan AI** to add an Atlan AI-suggested description. - For terms with an existing description, click the text box and then click the **Improve using Atlan AI** button to replace the existing description with an Atlan AI-suggested one - this option is only available in the sidebar. 5. (Optional) At the bottom of the _Atlan AI is writing..._ box, click **Enhance now** to briefly describe your organization and help Atlan AI make more relevant suggestions - this option is only visible to admin users. 6",
    "metadata": {
      "topic": "Use Atlan AI for documentation",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
      "keywords": [
        "use",
        "atlan",
        "documentation",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "use_atlan_ai_for_documentation_3",
    "text": "You can either add a description from the term profile or sidebar Under _Description_, you can either: - For terms without a description, navigate to the text box and click **use Atlan AI** to add an Atlan AI-suggested description. - For terms with an existing description, click the text box and then click the **Improve using Atlan AI** button to replace the existing description with an Atlan AI-suggested one - this option is only available in the sidebar. 5. (Optional) At the bottom of the _Atlan AI is writing..._ box, click **Enhance now** to briefly describe your organization and help Atlan AI make more relevant suggestions - this option is only visible to admin users. 6 Once Atlan AI has generated a description, you can: - Click anywhere in the text box to edit the text and then click **Apply**. - Click **Apply** to add the description to your term. - Click **Discard** to discard the AI-generated description. - Click the retry button to generate a new description, compare the two to select the most relevant option, and then click **Apply**. 7. (Optional) From the sidebar tabs on the right, click the **Activity** tab to view the changelog - including the _Updated using Atlan AI_ stamp, user information, and timestamp for the update Your AI-suggested term description is now live! üéâ ## Add a README to terms ‚Äã You can use Atlan AI to generate comprehensive READMEs for your terms in Atlan This provides you with a solid foundation for documenting business context You can then simply edit the Atlan AI-generated README and customize the format accordingly Use Atlan AI to document terms A Use Atlan AI to document terms !Begin by navigating to your glossaries!Select a glossary In this example, we'll select the **Finance** glossary Interactive walkthrough To add a README to a term using Atlan AI: 1 From the left menu on any screen, click **Glossary**. 2 Under _Glossary_ in the left menu, click the name of your glossary. 3 Under your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI. 4 In the _Readme_ section of the asset profile, click **Use Atlan AI** to add an Atlan AI-suggested README to the term. 5 Click anywhere in the text box to edit or format the text and then click **Save**. **Did you know?** If there are automated suggestions for asset descriptions, the option to use Atlan AI to document such assets will be unavailable Automated suggestions are based on user-generated descriptions for similar assets and may be more accurate than Atlan AI-generated descriptions. - Add a description to a table or view - Add a description to a column - Add a description to glossary assets - Add a README to terms",
    "metadata": {
      "topic": "Use Atlan AI for documentation",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
      "keywords": [
        "use",
        "atlan",
        "documentation",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "use_atlan_ai_for_lineage_analy_0",
    "text": "On this page ‚ûï **Available to customers in Enterprise and Business-Critical platform editions** Who can do this Before using Atlan AI, your admin user must enable Atlan AI in your Atlan workspace Atlan AI can help you understand lineage transformations using natural language You can use Atlan AI to create a natural language explanation for assets with SQL attributes and help you better understand the transformation logic. ## Explain lineage transformations ‚Äã Use Atlan AI to explain lineage A Use Atlan AI to explain lineage !Begin by navigating to your assets!Select an asset and then click the lineage icon to open the lineage graph Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms To use Atlan AI to explain lineage transformations: 1 From the left menu of any screen, click **Assets**. 2 Select an asset, and from the top right of the asset card, click the **View lineage** icon to open the lineage graph. 3 On the lineage graph, click any circular process button to view more details in the sidebar. 4 From _Overview_ in the sidebar, under _Query_, click the **Atlan AI** icon to explain the SQL query You can now understand lineage transformations using Atlan AI! üéâ **Did you know?** The lineage graph in Atlan provides a granular view of the data flows and transformations for your assets, learn more here. - Explain lineage transformations",
    "metadata": {
      "topic": "Use Atlan AI for lineage analysis",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-lineage-analysis",
      "keywords": [
        "use",
        "atlan",
        "lineage",
        "analysis",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "data_models_0",
    "text": "On this page Data models provide a framework to describe how data is structured, organized, and related within a system It acts as a blueprint for organizations to design their business applications and processes Data models can be of different types: relational, hierarchical, entity relationship, and network Atlan enables you to ingest your entity‚Äìrelationship (ER) models and associate them with existing data assets in Atlan Cataloging your ER model metadata in Atlan can help you: - Foster collaboration - business and technical users work best when they share a common understanding of the data landscape without tool boundaries. - Handle change management through impact analysis - data models enable visualization of an asset's lifecycle within an organization, helping users assess business impact due to technical changes with accuracy and vice versa. - Implement data governance - define access control mechanisms, data retention policies, and data governance rules spanning different systems by understanding relationships between data assets When business-approved data models are coupled with technical objects, trust and accountability are established between key stakeholders. !Data models_Atlan.png ## Ingest ER models ‚Äã You can ingest your ER models in Atlan using the following methods: - Data model ingestion - Atlan recommends using this custom package to ingest your ER models via an Excel template. - Atlan SDK - Atlan REST API ## Entity‚Äìrelationship models ‚Äã Entity‚Äìrelationship (ER) models focus on entities (objects/concepts) and the attributes (characteristics) and relationships (associations) between those entities In the context of entity‚Äìrelationship modeling, a model encompasses the entities, attributes, and relationships that define how data is organized and interactions between different elements within a specific domain Data models can be used to represent information at different levels of abstraction: - Conceptual - overall structure of content without specific details This acts as a starting point for new data initiatives and is the most abstract form of the model. - Logical - implementation-agnostic breakdown of data into specific objects and interactions between these objects. - Physical - a refined adaptation of data concepts conforming to a particular software application or data storage system This level takes into account finer nuances like naming conventions, optimizations, partitioning, and more. ## Entity-relationship diagrams ‚Äã An entity-relationship diagram (ERD) is a visual representation of data that illustrates the entities (objects or concepts) within a system, relationships between those entities, and their attributes. - **Entity** - in an ERD, an entity is a fundamental component that represents a real-world object or concept within a database For example, entities are typically nouns, such as `Customer`, `Order`, or `Product` and data can be stored about them. - **Attribute** - an entity has attributes, which are the properties or characteristics of the entity For example, a `Customer` entity may have attributes like `CustomerID`, `Name`, `Email`, and `Phone Number`. - **Relationship** - a relationship determines how two entities interact with each other For example, a `Customer` places an `Order` A relationship encompasses several elements, like: - Cardinality - defines the quantitative aspect of a relationship",
    "metadata": {
      "topic": "Data Models",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/data-models/concepts/what-are-data-models",
      "keywords": [
        "data",
        "models",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "data_models_1",
    "text": "For example, a `Customer` places an `Order` A relationship encompasses several elements, like: - Cardinality - defines the quantitative aspect of a relationship For example, a `Quote` provides pricing for many related `Orders` (one-to-many). - Optionality - defines whether a relationship is mandatory in an entity For example, an `Order` must have an associated `Customer`. - Cardinality and optionality can be combined to define business rules For example, in a `Library` system, a `Member` can borrow 0-n book(s). - Types of relationships: - Association - refers to a peer-to-peer relationship between two entities. - Generalization - refers to a parent-child relationship between two entities For example, a `Loan` entity can be of type `Home Loan`, `Auto Loan`, `Business Loan`, and so on. - **Model** - in the context of ER modeling, a model encompasses the entities, attributes, and relationships that define how data is organized and how different elements interact within a specific domain. - Models can be of different types - conceptual, logical, and physical. - Mapping - entities within a model can be mapped to entities within another model of a different type For example, a logical entity `Order` can be mapped to your assets in Atlan, such as an `Order` table in Snowflake. - Ingest ER models - Entity‚Äìrelationship models - Entity-relationship diagrams",
    "metadata": {
      "topic": "Data Models",
      "category": "Features",
      "source_url": "https://docs.atlan.com/product/capabilities/data-models/concepts/what-are-data-models",
      "keywords": [
        "data",
        "models",
        "features",
        "functionality",
        "capability"
      ]
    }
  },
  {
    "id": "atlan_get_started_0",
    "text": "- üöÄ **Quick-start guide** Step-by-step onboarding - üîß **Secure agent** Enterprise-grade deployment options - üìù **Playbooks automation** Rule-based metadata updates at scale ## Core features - üîç **Find & understand data** Search, discover, and profile assets - üõ°Ô∏è **Govern & manage** Create data contracts & policies - üîå **Integrate** Automation, collaboration & other integrations ## Developer hub - ‚öôÔ∏è **Introductory walkthrough** Play with APIs in minutes - üíª **Client SDKs** Java, Python & more - üì¶ **Packages** Developer-built utilities and integrations **Atlan University** Get started with Atlan by building the right strategy and setting a strong foundation. **Atlan Security** A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures **Help and support** Find answers or contact our team for personalized assistance",
    "metadata": {
      "topic": "Atlan - Get started",
      "category": "General",
      "source_url": "https://docs.atlan.com",
      "keywords": [
        "atlan",
        "general",
        "get started"
      ]
    }
  },
  {
    "id": "atlan_core_features_1",
    "text": "- üîç **Find & understand data** Search, discover, and profile assets - üõ°Ô∏è **Govern & manage** Create data contracts & policies - üîå **Integrate** Automation, collaboration & other integrations ## Developer hub - ‚öôÔ∏è **Introductory walkthrough** Play with APIs in minutes - üíª **Client SDKs** Java, Python & more - üì¶ **Packages** Developer-built utilities and integrations **Atlan University** Get started with Atlan by building the right strategy and setting a strong foundation. **Atlan Security** A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures **Help and support** Find answers or contact our team for personalized assistance",
    "metadata": {
      "topic": "Atlan - Core features",
      "category": "General",
      "source_url": "https://docs.atlan.com",
      "keywords": [
        "atlan",
        "general",
        "core features"
      ]
    }
  },
  {
    "id": "atlan_developer_hub_2",
    "text": "- ‚öôÔ∏è **Introductory walkthrough** Play with APIs in minutes - üíª **Client SDKs** Java, Python & more - üì¶ **Packages** Developer-built utilities and integrations **Atlan University** Get started with Atlan by building the right strategy and setting a strong foundation. **Atlan Security** A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures **Help and support** Find answers or contact our team for personalized assistance",
    "metadata": {
      "topic": "Atlan - Developer hub",
      "category": "General",
      "source_url": "https://docs.atlan.com",
      "keywords": [
        "atlan",
        "general",
        "developer hub"
      ]
    }
  },
  {
    "id": "crawl_amazon_quicksight_0",
    "text": "On this page Once you have configured the Amazon QuickSight permissions, you can establish a connection between Atlan and Amazon QuickSight To crawl metadata from Amazon QuickSight, review the order of operations and then complete the following steps Crawl QuickSight A Crawl QuickSight !Begin by adding a new workflow!The QuickSight Assets package allows us to crawl metadata from Amazon QuickSight Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Select the source ‚Äã To select Amazon QuickSight as your source: 1 In the top right of any screen in Atlan, navigate to _+New_ and click **New Workflow**. 2 From the _Marketplace_ page, click **QuickSight Assets**. 3 In the right panel, click **Setup Workflow**. ## Provide your credentials ‚Äã Choose your extraction method: - In **Direct** extraction, Atlan connects to your database and crawls metadata directly. - In **Agent** extraction, Atlan‚Äôs secure agent executes metadata extraction within the organization's environment. ### Direct extraction method ‚Äã To enter your Amazon QuickSight credentials: 1 For _Authentication,_ _IAM User_ is the default authentication method. 2 For _AWS Access Key_, enter the AWS access key you downloaded. 3 For _AWS Secret Key_, enter the AWS secret key you downloaded. 4 At the bottom, enter the _Region_ and _AWS Account ID_ of your Amazon QuickSight instance. 5 Click the **Test Authentication** button to confirm connectivity to Amazon QuickSight. 6 Once authentication is successful, navigate to the bottom of the screen and click **Next**. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for fetching metadata from Amazon QuickSight To use a Secure Agent, follow these steps: 1 Select the **Agent** tab. 2 Configure the Amazon QuickSight data source by adding the secret keys for your secret store For details on the required fields, refer to the Direct extraction section. 3 Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. 4 Click **Next** after completing the configuration. ## Configure the connection ‚Äã To complete the Amazon QuickSight connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users who are able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, no one will be able to manage the connection - not even admins. 3 Navigate to the bottom of the screen and click **Next** to proceed. ## Configure the crawler ‚Äã Before running the Amazon QuickSight crawler, you can further configure it",
    "metadata": {
      "topic": "Crawl Amazon QuickSight",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/crawl-amazon-quicksight",
      "keywords": [
        "crawl",
        "amazon",
        "quicksight",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_amazon_quicksight_1",
    "text": "For example, you might use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users who are able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, no one will be able to manage the connection - not even admins. 3 Navigate to the bottom of the screen and click **Next** to proceed. ## Configure the crawler ‚Äã Before running the Amazon QuickSight crawler, you can further configure it On the _Metadata Filters_ page, you can override the defaults for any of these options: - For _Fetch all assets without folder_, click **Yes** to fetch assets not linked to any folders, including datasets, analyses, and dashboards, or click **No** to only fetch assets linked to folders. - To select the folders you want to include in crawling, click **Include Folders**. (This will default to all folders, if none are specified.) - To select the folders you want to exclude from crawling, click **Exclude Folders**. (This will default to no folders, if none are specified.) **Did you know?** If an asset appears in both the include and exclude filters, the exclude filter takes precedence. ## Run the crawler ‚Äã To run the Amazon QuickSight crawler, after completing the steps above: 1 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. 2 You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button Once the crawler has completed running, you will see the assets on Atlan's asset page! üéâ - Select the source - Provide your credentials - Configure the connection - Configure the crawler - Run the crawler",
    "metadata": {
      "topic": "Crawl Amazon QuickSight",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/crawl-amazon-quicksight",
      "keywords": [
        "crawl",
        "amazon",
        "quicksight",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_amazon_quicksight_0",
    "text": "On this page warning **ü§ì Who can do this?** You will probably need your Amazon QuickSight administrator to run these commands - you may not have access yourself Atlan currently only supports IAM user authentication for Amazon QuickSight. ## Create IAM policy ‚Äã To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide Create the policy using the following JSON: ```codeBlockLines_e6Vv { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"quicksight:ListAnalyses\", \"quicksight:ListDataSets\", \"quicksight:ListDashboards\", \"quicksight:ListFolders\", \"quicksight:ListDataSources\", \"quicksight:DescribeAnalysis\", \"quicksight:DescribeDashboard\", \"quicksight:DescribeDataSet\", \"quicksight:DescribeFolder\", \"quicksight:ListFolderMembers\" ], \"Resource\": [ \"arn:aws:quicksight:<region>:<account_id>:*\" ] }] } ``` - Replace `<region>` with the AWS region of your Amazon QuickSight instance. - Replace `<account_id>` with your AWS account ID. ## Configure user-based authentication ‚Äã Using the IAM policy created above, configure user-based authentication To configure user-based authentication: 1 Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide. 2 On the _Set permissions_ page, attach the policy created in the previous step to this user. 3 Once the user is created, view or download the user's _access key ID_ and _secret access key_. danger This will be your only opportunity to view or download the access keys You will not have access to them again after leaving the user creation screen. - Create IAM policy - Configure user-based authentication",
    "metadata": {
      "topic": "Set up Amazon QuickSight",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight",
      "keywords": [
        "set",
        "amazon",
        "quicksight",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_snowflake_tag__0",
    "text": "On this page Here are a few things to know about managing Snowflake tags in Atlan: #### If a Snowflake tag is not attached to an asset, will enabling reverse sync trigger any updates? ‚Äã When reverse tag sync is enabled, updates in Snowflake will only be triggered if an imported tag is attached to a Snowflake asset in Atlan or an existing Atlan tag for an asset is updated with an imported Snowflake tag. #### Will Snowflake tags show up as native assets in Atlan? ‚Äã No, Atlan currently does not support Snowflake tags as native assets. #### Do corresponding tags in Atlan and Snowflake need to be of the same case for the sync to work? ‚Äã Tag sync happens through case-insensitive name match Even if the Snowflake tag and the corresponding Atlan tag have the same name but are of a different case, the tags will be synced and remain consistent between Atlan and Snowflake. #### When reverse tag sync is enabled, will all Snowflake assets with tags in Atlan be updated in Snowflake? ‚Äã Enabling reverse tag sync will not trigger any updates unless tag updates are made for the Snowflake assets either in Atlan or Snowflake. #### If a tag is deleted in Snowflake, will the corresponding tag in Atlan also be deleted? ‚Äã Only the imported Snowflake tag associated with the corresponding Atlan tag and its association with linked assets will be removed The Atlan tag and the Snowflake assets to which the Atlan tag is attached will remain unaffected. #### How will tag propagation work for Snowflake tags? ‚Äã Atlan supports tag propagation based on asset hierarchy and lineage This existing functionality will remain unaffected by imported Snowflake tags To learn about tag propagation in Snowflake, see here. #### How can I find out if reverse tag sync was successful? ‚Äã If you're using the account usage method, expect latency of data for up to 3 hours You can also use a table function like `TAG_REFERENCES` to confirm reverse tag sync. #### What happens if a tag is deleted in Atlan but remains in Snowflake? ‚Äã The tag will be recreated during the next Snowflake crawler run and linked to assets where the relationship still exists in Snowflake. #### Does enabling reverse sync automatically push all updates to Snowflake? ‚Äã No However, you can set up playbooks in Atlan to update your Snowflake assets with imported tags at scale If reverse sync is enabled for the Snowflake tags, updates will be pushed to Snowflake as well. #### What is the default propagation setting for synced Snowflake tags and can it be changed? ‚Äã When imported Snowflake tags are attached to assets after a crawler run, tag propagation is turned off by default in Atlan. #### Can I restrict who can push asset-specific tag syncs to Snowflake? ‚Äã No, this functionality is currently unavailable",
    "metadata": {
      "topic": "Troubleshooting Snowflake tag management",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-tag-management",
      "keywords": [
        "troubleshooting",
        "snowflake",
        "tag",
        "management",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_snowflake_tag__1",
    "text": "However, you can set up playbooks in Atlan to update your Snowflake assets with imported tags at scale If reverse sync is enabled for the Snowflake tags, updates will be pushed to Snowflake as well. #### What is the default propagation setting for synced Snowflake tags and can it be changed? ‚Äã When imported Snowflake tags are attached to assets after a crawler run, tag propagation is turned off by default in Atlan. #### Can I restrict who can push asset-specific tag syncs to Snowflake? ‚Äã No, this functionality is currently unavailable However, stay tuned for more updates. #### Will reverse sync remove a tag from an asset in Snowflake if it is removed in Atlan? ‚Äã If a tag is removed from a Snowflake asset in Atlan, it will not delete the tag in Snowflake Instead, the asset will no longer be linked to the Snowflake tag in Atlan only. #### If tag propagation and reverse sync are enabled, will my assets in Snowflake display multiple values of the same tag? ‚Äã If both tag propagation and reverse sync are enabled, and multiple values of the same tag are being propagated via different paths to a Snowflake asset: - Atlan will preserve all values of the attached tag on the asset. - Snowflake will only preserve the last modified value of the attached tag on the asset. #### Is it possible to filter and select specific tags to import in Atlan? ‚Äã No, Atlan currently does not support filtering and selecting specific Snowflake tags to import Atlan will crawl all the tags associated with an asset in Snowflake. #### Can Atlan reverse sync tags to dropped tables in Snowflake? ‚Äã If tables are dropped in Snowflake, then the corresponding link between the table and tag is also removed If new tables are created in their place, you will need to recreate the link You can either: - Apply tags to the new tables in Snowflake - Crawl the new Snowflake tables in Atlan, run a playbook for tag attachment, and then push tag updates to Snowflake If you're also using dbt, you can use pre-hooks and post-hooks to preserve metadata like attached tags while building your dbt models Refer to dbt documentation to learn more about hooks",
    "metadata": {
      "topic": "Troubleshooting Snowflake tag management",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-tag-management",
      "keywords": [
        "troubleshooting",
        "snowflake",
        "tag",
        "management",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "snowflake_0",
    "text": "On this page > **Overview:** > Catalog Snowflake databases, schemas, tables, and views in Atlan Gain visibility into lineage, usage, and governance for your cloud data warehouse assets. ## Get started ‚Äã Follow these steps to connect and catalog Snowflake assets in Atlan: - Set up the connector - Crawl Snowflake assets ## Guides ‚Äã ### Authentication ‚Äã - Enable Snowflake OAuth: Set up OAuth authentication for Snowflake connections. ### Metadata & lineage ‚Äã - Mine Snowflake: Learn how to mine query history and construct lineage for Snowflake assets. ### Tag management ‚Äã - Manage Snowflake tags: Configure and manage tags and policy tags in Snowflake. ### Advanced features ‚Äã - Configure Snowflake data metric functions: Configure and use data metric functions in Snowflake. ### Private networking ‚Äã - Set up an AWS private network link to Snowflake: Establish a secure, private network connection to Snowflake on AWS. - Set up an Azure private network link to Snowflake: Establish a secure, private network connection to Snowflake on Azure. ## References ‚Äã - What does Atlan crawl from Snowflake: Learn about the Snowflake assets and metadata that Atlan discovers and catalogs. - Preflight checks for Snowflake: Verify prerequisites before setting up the Snowflake connector. ## Troubleshooting ‚Äã - Troubleshooting connectivity: Resolve common Snowflake connection issues and errors. - Get started - Guides - References - Troubleshooting",
    "metadata": {
      "topic": "Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/",
      "keywords": [
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_sno_0",
    "text": "On this page Atlan crawls and maps the following assets and properties from Snowflake Once you've crawled Snowflake, you can use connector-specific filters for quick asset discovery The following filters are currently supported for Snowflake assets: - Streams - Source type and Stale filters - Functions - Language, Function type, Is secure, and Is external filters - Snowflake tags and tag values ## Lineage ‚Äã Atlan supports lineage for the following asset types: ### External Named Stages ‚Äã - Table - Pipe ‚Üí Table - External Table - Iceberg Table ### Internal Named Stages ‚Äã - Table - Pipe ‚Üí Table (auto-ingest not recommended) - Not supported for External or Iceberg Tables ## Databases ‚Äã Atlan maps databases from Snowflake to its `Database` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `DATABASES.DATABASE_NAME` | `name` | asset profile and overview sidebar | | `DATABASE.COMMENT` | `description` | asset profile and overview sidebar | | `SCHEMATA` (count) | `schemaCount` | asset preview and profile | | `DATABASES.DATABASE_OWNER` | Created (in Snowflake) | properties sidebar | | `DATABASES.CREATED` | `sourceCreatedAt` | properties sidebar | | `DATABASES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Schemas ‚Äã Atlan maps schemas from Snowflake to its `Schema` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `SCHEMATA.SCHEMA_NAME` | `name` | asset profile and overview sidebar | | `SCHEMA.COMMENT` | `description` | asset profile and overview sidebar | | `TABLES` of type %TABLE% (count) | `tableCount` | asset preview and profile | | `TABLES` of type %VIEW% (count) | `viewsCount` | asset preview and profile | | `SCHEMATA.CATALOG_NAME` | `databaseName` | asset preview and profile | | `SCHEMATA.SCHEMA_OWNER` | Created (in Snowflake) | properties sidebar | | `SCHEMATA.CREATED` | `sourceCreatedAt` | properties sidebar | | `SCHEMATA.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Tables ‚Äã Atlan maps tables from Snowflake to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ### For Iceberg tables ‚Äã In addition to the table metadata above, Atlan supports additional metadata for Iceberg tables crawled from Snowflake",
    "metadata": {
      "topic": "What does Atlan crawl from Snowflake?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/what-does-atlan-crawl-from-snowflake",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_sno_1",
    "text": "Once you've crawled Snowflake, you can use connector-specific filters for quick asset discovery The following filters are currently supported for Snowflake assets: - Streams - Source type and Stale filters - Functions - Language, Function type, Is secure, and Is external filters - Snowflake tags and tag values ## Lineage ‚Äã Atlan supports lineage for the following asset types: ### External Named Stages ‚Äã - Table - Pipe ‚Üí Table - External Table - Iceberg Table ### Internal Named Stages ‚Äã - Table - Pipe ‚Üí Table (auto-ingest not recommended) - Not supported for External or Iceberg Tables ## Databases ‚Äã Atlan maps databases from Snowflake to its `Database` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `DATABASES.DATABASE_NAME` | `name` | asset profile and overview sidebar | | `DATABASE.COMMENT` | `description` | asset profile and overview sidebar | | `SCHEMATA` (count) | `schemaCount` | asset preview and profile | | `DATABASES.DATABASE_OWNER` | Created (in Snowflake) | properties sidebar | | `DATABASES.CREATED` | `sourceCreatedAt` | properties sidebar | | `DATABASES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Schemas ‚Äã Atlan maps schemas from Snowflake to its `Schema` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `SCHEMATA.SCHEMA_NAME` | `name` | asset profile and overview sidebar | | `SCHEMA.COMMENT` | `description` | asset profile and overview sidebar | | `TABLES` of type %TABLE% (count) | `tableCount` | asset preview and profile | | `TABLES` of type %VIEW% (count) | `viewsCount` | asset preview and profile | | `SCHEMATA.CATALOG_NAME` | `databaseName` | asset preview and profile | | `SCHEMATA.SCHEMA_OWNER` | Created (in Snowflake) | properties sidebar | | `SCHEMATA.CREATED` | `sourceCreatedAt` | properties sidebar | | `SCHEMATA.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Tables ‚Äã Atlan maps tables from Snowflake to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ### For Iceberg tables ‚Äã In addition to the table metadata above, Atlan supports additional metadata for Iceberg tables crawled from Snowflake Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.IS_ICEBERG` | `tableType` | asset preview, profile, and filter, and overview sidebar | | `ICEBERG_TABLES.CATALOG_NAME` | `icebergCatalogName` | overview sidebar | | `ICEBERG_TABLES.ICEBERG_TABLE_TYPE` | `icebergTableType` | overview sidebar | | `CATALOG_INTEGRATION.CATALOG_SOURCE` | `icebergCatalogSource` | overview sidebar | | `ICEBERG_TABLES.CATALOG_TABLE_NAME` | `icebergCatalogTableName` | overview sidebar | | `ICEBERG_TABLES.CATALOG_NAMESPACE` | `icebergCatalogTableNamespace` | overview sidebar | | `ICEBERG_TABLES.EXTERNAL_VOLUME_NAME` | `tableExternalVolumeName` | overview sidebar | | `ICEBERG_TABLES.BASE_LOCATION` | `icebergTableBaseLocation` | overview sidebar | | `TABLES.RETENTION_TIME` | `tableRetentionTime` | overview sidebar | ## Views ‚Äã Atlan maps views from Snowflake to its `View` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `VIEWS.VIEW_DEFINITION` | `definition` | asset profile and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Materialized views ‚Äã Atlan maps materialized views from Snowflake to its `MaterialisedView` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `VIEWS.VIEW_DEFINITION` | `definition` | asset profile and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## External tables ‚Äã Atlan maps external tables from Snowflake to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `EXTERNAL_TABLES.LOCATION` | `externalLocation` | overview sidebar | | `STAGES.STAGE_REGION` | `externalLocationRegion` | API only | | `EXTERNAL_TABLES.FILE_FORMAT_TYPE` | `externalLocationFormat` | overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Columns ‚Äã Atlan maps columns from Snowflake to its `Column` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `COLUMNS.COLUMN_NAME` | `name` | asset profile and overview sidebar | | `COLUMNS.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS.ORDINAL_POSITION` | `order` | API only | | `COLUMNS.DATA_TYPE` | `dataType` | asset filter, preview, and profile, overview sidebar | | `PRIMARY KEY` | `isPrimary` | asset preview and filter, overview sidebar | | `COLUMNS.IS_NULLABLE` | `isNullable` | overview sidebar | | `COLUMNS.CHARACTER_MAXIMUM_LENGTH` | `maxLength` | properties sidebar | | `COLUMNS.NUMERIC_PRECISION` | `precision` | properties sidebar | ## Stages ‚Äã Atlan maps stages from Snowflake to its `Stage` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `Stages.STAGE_NAME` | `name` | asset profile and overview sidebar | | `Stages.COMMENT` | `description` | asset profile and overview sidebar | | `Stages.STAGE_SCHEMA` | `schemaName` | asset profile and overview sidebar | | `Stages.STAGE_CATALOG` | `databaseName` | asset profile and overview sidebar | | `Stages.STAGE_URL` | `snowflakeStageExternalLocation` | asset profile and overview sidebar | | `Stages.STAGE_REGION` | `snowflakeStageExternalLocationRegion` | asset profile and overview sidebar | | `Stages.STAGE_TYPE` | `snowflakeStageType` | asset profile and overview sidebar | | `Stages.STAGE_OWNER` | `sourceOwners` | overview sidebar | | `Stages.CREATED` | `sourceCreatedAt` | properties sidebar | | `Stages.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | | `Stages.STORAGE_INTEGRATION` | `snowflakeStageStorageIntegration` | asset profile and overview sidebar | ## Streams ‚Äã Atlan maps streams from Snowflake to its `Stream` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `STREAMS.NAME` | `name` | asset profile and overview sidebar | | `STREAMS.COMMENT` | `description` | asset profile and overview sidebar | | `STREAMS.OWNER` | `sourceOwners` | asset preview and profile, properties sidebar | | `STREAMS.DATABASE_NAME` | `databaseName` | asset preview and profile | | `STREAMS.SCHEMA_NAME` | `schemaName` | asset preview and profile | | `STREAMS.SOURCE_TYPE` | `snowflakeStreamSourceType` | asset filter and overview sidebar | | `STREAMS.STALE` | `snowflakeStreamIsStale` | asset filter and overview sidebar | | `STREAMS.MODE` | `snowflakeStreamMode` | overview sidebar | | `STREAMS.STALE_AFTER` | `snowflakeStreamStaleAfter` | overview sidebar | ## Pipes ‚Äã Atlan maps pipes from Snowflake to its `Pipe` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `PIPES.PIPE_NAME` | `name` | asset profile and overview sidebar | | `PIPES.COMMENT` | `description` | asset profile and overview sidebar | | `PIPES.DEFINITION` | `definition` | asset profile and overview sidebar | | `PIPES.PIPE_OWNER` | `sourceOwners` | asset preview and profile, properties sidebar | | `PIPES.PIPE_CATALOG` | `databaseName` | asset preview and profile | | `PIPES.PIPE_SCHEMA` | `schemaName` | asset preview and profile | | `PIPES.IS_AUTOINGEST_ENABLED` | `snowflakePipeIsAutoIngestEnabled` | overview sidebar | | `PIPES.NOTIFICATION_CHANNEL_NAME` | `snowflakePipeNotificationChannelName` | overview sidebar | ## User-defined functions ‚Äã Atlan maps user-defined functions (UDFs) from Snowflake to its `Function` asset type",
    "metadata": {
      "topic": "What does Atlan crawl from Snowflake?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/what-does-atlan-crawl-from-snowflake",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_sno_2",
    "text": "The following filters are currently supported for Snowflake assets: - Streams - Source type and Stale filters - Functions - Language, Function type, Is secure, and Is external filters - Snowflake tags and tag values ## Lineage ‚Äã Atlan supports lineage for the following asset types: ### External Named Stages ‚Äã - Table - Pipe ‚Üí Table - External Table - Iceberg Table ### Internal Named Stages ‚Äã - Table - Pipe ‚Üí Table (auto-ingest not recommended) - Not supported for External or Iceberg Tables ## Databases ‚Äã Atlan maps databases from Snowflake to its `Database` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `DATABASES.DATABASE_NAME` | `name` | asset profile and overview sidebar | | `DATABASE.COMMENT` | `description` | asset profile and overview sidebar | | `SCHEMATA` (count) | `schemaCount` | asset preview and profile | | `DATABASES.DATABASE_OWNER` | Created (in Snowflake) | properties sidebar | | `DATABASES.CREATED` | `sourceCreatedAt` | properties sidebar | | `DATABASES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Schemas ‚Äã Atlan maps schemas from Snowflake to its `Schema` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `SCHEMATA.SCHEMA_NAME` | `name` | asset profile and overview sidebar | | `SCHEMA.COMMENT` | `description` | asset profile and overview sidebar | | `TABLES` of type %TABLE% (count) | `tableCount` | asset preview and profile | | `TABLES` of type %VIEW% (count) | `viewsCount` | asset preview and profile | | `SCHEMATA.CATALOG_NAME` | `databaseName` | asset preview and profile | | `SCHEMATA.SCHEMA_OWNER` | Created (in Snowflake) | properties sidebar | | `SCHEMATA.CREATED` | `sourceCreatedAt` | properties sidebar | | `SCHEMATA.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Tables ‚Äã Atlan maps tables from Snowflake to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ### For Iceberg tables ‚Äã In addition to the table metadata above, Atlan supports additional metadata for Iceberg tables crawled from Snowflake Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.IS_ICEBERG` | `tableType` | asset preview, profile, and filter, and overview sidebar | | `ICEBERG_TABLES.CATALOG_NAME` | `icebergCatalogName` | overview sidebar | | `ICEBERG_TABLES.ICEBERG_TABLE_TYPE` | `icebergTableType` | overview sidebar | | `CATALOG_INTEGRATION.CATALOG_SOURCE` | `icebergCatalogSource` | overview sidebar | | `ICEBERG_TABLES.CATALOG_TABLE_NAME` | `icebergCatalogTableName` | overview sidebar | | `ICEBERG_TABLES.CATALOG_NAMESPACE` | `icebergCatalogTableNamespace` | overview sidebar | | `ICEBERG_TABLES.EXTERNAL_VOLUME_NAME` | `tableExternalVolumeName` | overview sidebar | | `ICEBERG_TABLES.BASE_LOCATION` | `icebergTableBaseLocation` | overview sidebar | | `TABLES.RETENTION_TIME` | `tableRetentionTime` | overview sidebar | ## Views ‚Äã Atlan maps views from Snowflake to its `View` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `VIEWS.VIEW_DEFINITION` | `definition` | asset profile and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Materialized views ‚Äã Atlan maps materialized views from Snowflake to its `MaterialisedView` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `VIEWS.VIEW_DEFINITION` | `definition` | asset profile and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## External tables ‚Äã Atlan maps external tables from Snowflake to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `EXTERNAL_TABLES.LOCATION` | `externalLocation` | overview sidebar | | `STAGES.STAGE_REGION` | `externalLocationRegion` | API only | | `EXTERNAL_TABLES.FILE_FORMAT_TYPE` | `externalLocationFormat` | overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | ## Columns ‚Äã Atlan maps columns from Snowflake to its `Column` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `COLUMNS.COLUMN_NAME` | `name` | asset profile and overview sidebar | | `COLUMNS.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS.ORDINAL_POSITION` | `order` | API only | | `COLUMNS.DATA_TYPE` | `dataType` | asset filter, preview, and profile, overview sidebar | | `PRIMARY KEY` | `isPrimary` | asset preview and filter, overview sidebar | | `COLUMNS.IS_NULLABLE` | `isNullable` | overview sidebar | | `COLUMNS.CHARACTER_MAXIMUM_LENGTH` | `maxLength` | properties sidebar | | `COLUMNS.NUMERIC_PRECISION` | `precision` | properties sidebar | ## Stages ‚Äã Atlan maps stages from Snowflake to its `Stage` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `Stages.STAGE_NAME` | `name` | asset profile and overview sidebar | | `Stages.COMMENT` | `description` | asset profile and overview sidebar | | `Stages.STAGE_SCHEMA` | `schemaName` | asset profile and overview sidebar | | `Stages.STAGE_CATALOG` | `databaseName` | asset profile and overview sidebar | | `Stages.STAGE_URL` | `snowflakeStageExternalLocation` | asset profile and overview sidebar | | `Stages.STAGE_REGION` | `snowflakeStageExternalLocationRegion` | asset profile and overview sidebar | | `Stages.STAGE_TYPE` | `snowflakeStageType` | asset profile and overview sidebar | | `Stages.STAGE_OWNER` | `sourceOwners` | overview sidebar | | `Stages.CREATED` | `sourceCreatedAt` | properties sidebar | | `Stages.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | | `Stages.STORAGE_INTEGRATION` | `snowflakeStageStorageIntegration` | asset profile and overview sidebar | ## Streams ‚Äã Atlan maps streams from Snowflake to its `Stream` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `STREAMS.NAME` | `name` | asset profile and overview sidebar | | `STREAMS.COMMENT` | `description` | asset profile and overview sidebar | | `STREAMS.OWNER` | `sourceOwners` | asset preview and profile, properties sidebar | | `STREAMS.DATABASE_NAME` | `databaseName` | asset preview and profile | | `STREAMS.SCHEMA_NAME` | `schemaName` | asset preview and profile | | `STREAMS.SOURCE_TYPE` | `snowflakeStreamSourceType` | asset filter and overview sidebar | | `STREAMS.STALE` | `snowflakeStreamIsStale` | asset filter and overview sidebar | | `STREAMS.MODE` | `snowflakeStreamMode` | overview sidebar | | `STREAMS.STALE_AFTER` | `snowflakeStreamStaleAfter` | overview sidebar | ## Pipes ‚Äã Atlan maps pipes from Snowflake to its `Pipe` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `PIPES.PIPE_NAME` | `name` | asset profile and overview sidebar | | `PIPES.COMMENT` | `description` | asset profile and overview sidebar | | `PIPES.DEFINITION` | `definition` | asset profile and overview sidebar | | `PIPES.PIPE_OWNER` | `sourceOwners` | asset preview and profile, properties sidebar | | `PIPES.PIPE_CATALOG` | `databaseName` | asset preview and profile | | `PIPES.PIPE_SCHEMA` | `schemaName` | asset preview and profile | | `PIPES.IS_AUTOINGEST_ENABLED` | `snowflakePipeIsAutoIngestEnabled` | overview sidebar | | `PIPES.NOTIFICATION_CHANNEL_NAME` | `snowflakePipeNotificationChannelName` | overview sidebar | ## User-defined functions ‚Äã Atlan maps user-defined functions (UDFs) from Snowflake to its `Function` asset type Atlan currently does not support lineage for user-defined functions from Snowflake. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `NAME` | `name` | asset profile and overview sidebar | | `FUNCTION_DEFINITION` | `functionDefinition` | overview sidebar | | `COMMENT` | `description` | asset profile and overview sidebar | | `FUNCTION_CATALOG` | `databaseName` | asset preview and profile | | `FUNCTION_SCHEMA` | `schemaName` | asset preview and profile | | `FUNCTION_OWNER` | Created (in Snowflake) | properties sidebar | | `CREATED` | `sourceCreatedAt` | properties sidebar | | `LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | | `FUNCTION_LANGUAGE` | `functionLanguage` | overview sidebar | | `DATA_TYPE` | `functionReturnType` | API only | | `IS_SECURE` | `functionIsSecure` | asset filter and properties sidebar | | `IS_EXTERNAL` | `functionIsExternal` | asset filter and properties sidebar | | `IS_MEMOIZABLE` | `functionIsMemoizable` | API only | | `ARGUMENT_SIGNATURE` | `functionArguments` | API only | ## Dynamic tables ‚Äã Atlan maps dynamic tables from Snowflake to its `DynamicTable` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLES.TABLE_NAME` | `name` | asset profile and overview sidebar | | `TABLES.COMMENT` | `description` | asset profile and overview sidebar | | `COLUMNS` (count) | `columnCount` | asset preview and profile, overview sidebar | | `TABLES.DEFINITION` | `definition` | asset profile and overview sidebar | | `TABLES.ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `TABLES.BYTES` | `sizeBytes` | asset filter and overview sidebar | | `TABLES.TABLE_OWNER` | Created (in Snowflake) | properties sidebar | | `TABLES.CREATED` | `sourceCreatedAt` | properties sidebar | | `TABLES.LAST_ALTERED` | `sourceUpdatedAt` | properties sidebar | - Lineage - Databases - Schemas - Tables - Views - Materialized views - External tables - Columns - Stages - Streams - Pipes - User-defined functions - Dynamic tables",
    "metadata": {
      "topic": "What does Atlan crawl from Snowflake?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/what-does-atlan-crawl-from-snowflake",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_snowflake_conn_0",
    "text": "On this page #### How to debug test authentication and preflight check errors? ‚Äã **Missing warehouse grants** `The user doesn‚Äôt have USAGE and OPERATE grants on a warehouse.` - Grant warehouse access to the role: ```codeBlockLines_e6Vv GRANT OPERATE, USAGE ON WAREHOUSE \"<warehouse>\" TO ROLE atlan_user_role; ``` - Then, ensure that you grant the role to the new user: ```codeBlockLines_e6Vv GRANT ROLE atlan_user_role TO USER atlan_user; ``` **Missing authorized access to SNOWFLAKE.ACCOUNT_USAGE schema** `The user doesn‚Äôt have authorized access to the SNOWFLAKE.ACCOUNT_USAGE database` - Reach out to your account admin to grant imported privileges on the `Snowflake` database to the role: ```codeBlockLines_e6Vv USE ROLE ACCOUNTADMIN; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role; ``` - If using a copied database, you'll need to grant the following permissions: ```codeBlockLines_e6Vv GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role; ``` **Missing usage grants on databases and/or schemas** ``The user doesn't have usage grants to the databases ` $missingDatabases ` and schemas ` $missingSchemas`` - Grant missing permissions listed here for information schema extraction method. **Atlan IP not allowlisted** `Atlan's current location or network isn't recognized by Snowflake's security settings This can happen if Atlan's IP address isn't on the list of allowed addresses in Snowflake's network policies.` - If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist Contact Atlan support to obtain Atlan's IP addresses. **Incorrect credentials** `The username or the password provided to connect to the Snowflake account is incorrect.` - Sign into the Snowflake account for the specified host and verify that the username and password are correct. - You can also create a new user, if required, by following the steps here. **Missing or unauthorized role** `The role specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this role.` - If the role does not exist or is missing the required grants, create a role and then grant the role to the user. **User account locked** `The user account you're using to connect to Snowflake has been locked temporarily because of multiple incorrect login attempts.` - Wait for the user account to unlock or create a different user account to continue. **Missing or unauthorized warehouse** `The warehouse specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this warehouse.` - Ensure that the warehouse name is configured correctly. - Update the warehouse name in the configuration if your account is using a different warehouse",
    "metadata": {
      "topic": "Troubleshooting Snowflake connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity",
      "keywords": [
        "troubleshooting",
        "snowflake",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_snowflake_conn_1",
    "text": "This can happen if Atlan's IP address isn't on the list of allowed addresses in Snowflake's network policies.` - If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist Contact Atlan support to obtain Atlan's IP addresses. **Incorrect credentials** `The username or the password provided to connect to the Snowflake account is incorrect.` - Sign into the Snowflake account for the specified host and verify that the username and password are correct. - You can also create a new user, if required, by following the steps here. **Missing or unauthorized role** `The role specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this role.` - If the role does not exist or is missing the required grants, create a role and then grant the role to the user. **User account locked** `The user account you're using to connect to Snowflake has been locked temporarily because of multiple incorrect login attempts.` - Wait for the user account to unlock or create a different user account to continue. **Missing or unauthorized warehouse** `The warehouse specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this warehouse.` - Ensure that the warehouse name is configured correctly. - Update the warehouse name in the configuration if your account is using a different warehouse Create a role and then grant the role to the user for the updated warehouse. **Missing access to non-system databases or schemas** `The configured user doesn't have usage grants to any database or schema.` or `The configured user doesn't have usage grants to any non-system database or schema.` - This pertains to the information schema method of fetching metadata Ensure that the user has authorized access to the databases and schemas to be crawled. - Grant the requisite permissions as outlined here. #### Why are some assets from a database or schema missing? ‚Äã - Check the grants on the role attached to the user defined for the crawler",
    "metadata": {
      "topic": "Troubleshooting Snowflake connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity",
      "keywords": [
        "troubleshooting",
        "snowflake",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_snowflake_conn_2",
    "text": "Create a role and then grant the role to the user for the updated warehouse. **Missing access to non-system databases or schemas** `The configured user doesn't have usage grants to any database or schema.` or `The configured user doesn't have usage grants to any non-system database or schema.` - This pertains to the information schema method of fetching metadata Ensure that the user has authorized access to the databases and schemas to be crawled. - Grant the requisite permissions as outlined here. #### Why are some assets from a database or schema missing? ‚Äã - Check the grants on the role attached to the user defined for the crawler Ensure the missing database or schema is present in these grants. ```codeBlockLines_e6Vv SHOW GRANTS TO ROLE atlan_user_role; ``` #### Why are new tables or views missing? ‚Äã - When using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata. - Make sure the role attached to the user defined for the crawler has grants for future tables and views being created in the database: ```codeBlockLines_e6Vv GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role; ``` - Make sure you run the below commands as well so that new tables and views you've created in-between are also visible to the user: ```codeBlockLines_e6Vv GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO role atlan_user_role; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO role atlan_user_role; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO atlan_user_role; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO role atlan_user_role; ``` #### Why is some lineage missing? ‚Äã - The query miner only mines query history for up to the previous two weeks The miner will not mine any queries that ran before that time window If the queries that created your assets ran before that time window, lineage for those assets will not be present. - To mine more than the previous two weeks of query history, either use S3-based query mining or contact Atlan support Note that Snowflake itself only retains query history for so long as well, though Once Snowflake itself no longer contains the query history we will be unable to mine it for lineage. - Lineage is unsupported for parameterized queries Snowflake currently does not resolve values for parameterized queries before logging them in query history This limits Atlan from generating lineage in such cases. #### Missing attributes and lineage ‚Äã - When using the account usage extraction method, there are currently some limitations",
    "metadata": {
      "topic": "Troubleshooting Snowflake connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity",
      "keywords": [
        "troubleshooting",
        "snowflake",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_snowflake_conn_3",
    "text": "Snowflake currently does not resolve values for parameterized queries before logging them in query history This limits Atlan from generating lineage in such cases. #### Missing attributes and lineage ‚Äã - When using the account usage extraction method, there are currently some limitations We are working with Snowflake to find workarounds for crawling the following: - External table location data - Procedures - Primary key designation - Furthermore, only database-level filtering is currently possible. #### What views does Atlan require access to for the account usage method? ‚Äã When using the account usage method for fetching metadata, Atlan requires access to the following views in Snowflake: - For the crawler: `DATABASES`, `SCHEMATA`, `TABLES`, `VIEWS`, `COLUMNS`, and `PIPES` - For the miner and popularity metrics: `QUERY_HISTORY`, `ACCESS_HISTORY`, and `SESSIONS` #### Why am I getting a destination URL mismatch error when authenticating via Okta SSO? ‚Äã This error can occur when you're connecting to Snowflake through Okta SSO and enter the URL of your Snowflake instance in a format different from the one used in Okta Snowflake follows two URL formats: - Legacy format - `<AccountLocator>.<Region>.snowflakecomputing.com` or `<AccountLocator>.<Region>.<cloud>.snowflakecomputing.com` - New URL format - `<Orgname>-<AccountName>.snowflakecomputing.com` Ensure that you're using the same Snowflake URL format in Snowflake and Okta Refer to Snowflake documentation to learn more. #### Why am I getting a 'name or service not known' error when connecting via private link? ‚Äã If you're getting the following error messages - `java.net.UnknownHostException` and `Name or service not known` - this is a known error for users who have upgraded to the Snowflake JDBC driver version 3.13.25., have underscores in their account name, and connect to their Snowflake accounts over private link (for example, `https://my_account.us-west-2.privatelink.snowflakecomputing.com`) If your Snowflake account name has an underscore - for example, `my_account` - the updated JDBC driver will automatically convert underscores to dashes or hyphens `-` This does not affect normal URLs because Snowflake accepts URLs with both hyphens and underscores For private link users, however, the JDBC driver will return an error if there are underscores present in the account name and the connection will fail To troubleshoot further, refer to Snowflake documentation",
    "metadata": {
      "topic": "Troubleshooting Snowflake connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/troubleshooting/troubleshooting-snowflake-connectivity",
      "keywords": [
        "troubleshooting",
        "snowflake",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "mine_snowflake_0",
    "text": "On this page Mine Snowflake A Mine Snowflake !Begin by adding a new workflow!Switch to the miner packages to find the miners more easily Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms Once you have crawled assets from Snowflake, you can mine its query history to construct lineage To mine lineage from Snowflake, review the order of operations and then complete the following steps. ## Select the miner ‚Äã To select the Snowflake miner: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the filters along the top, click **Miner**. 3 From the list of packages, select **Snowflake Miner** and then click **Setup Workflow**. ## Configure the miner ‚Äã To configure the Snowflake miner: 1 For _Connection_, select the connection to mine. (To select a connection, the crawler must have already run.) 2 For _Miner Extraction Method_, select **Source**, **Agent**, or see the separate instructions for the S3 miner. 3 For _Snowflake Database_: - If the connection is configured with access to the snowflake database, choose **Default**. - If the connection can only access a separate cloned database, choose **Cloned Database**. 4 If you are using a cloned database, enter the name of the cloned database in _Database Name_ and the name of the cloned schema in _Schema Name_. 5 For _Start time_, choose the earliest date from which to mine query history. info üí™ **Did you know?** The miner restricts you to only querying the past two weeks of query history If you need to query more history, for example in an initial load, consider using the S3 miner first After the initial load, you can modify the miner's configuration to use query history extraction. 6 To check for any permissions or other configuration issues before running the miner, click **Preflight checks**. 7 At the bottom of the screen, click **Next** to proceed. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for mining query history from Snowflake To use a Secure Agent, follow these steps: 1 Select the **Agent** tab. 2 Configure the Snowflake data source by adding the secret keys for your secret store For details on the required fields, refer to the connection configuration used when crawling Snowflake. 3 Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. 4 Click **Next** after completing the configuration. danger If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history Mining two weeks of query history on the first miner run may cause delays For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session",
    "metadata": {
      "topic": "Mine Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/mine-snowflake",
      "keywords": [
        "mine",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "mine_snowflake_1",
    "text": "Mining two weeks of query history on the first miner run may cause delays For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session Learn more about the miner logic here. ## Configure the miner behavior ‚Äã To configure the Snowflake miner behavior: 1. (Optional) For _Calculate popularity_, keep **True** to retrieve usage and popularity metrics for your Snowflake assets from query history. - For _Excluded Users_, type the names of users to be excluded while calculating usage metrics for Snowflake assets Press `Enter` after each name to add more names. 2. (Optional) For _Advanced Config_, keep _Default_ for the default configuration or click **Custom** to configure the miner: - If Atlan support has provided you with a custom control configuration, enter the configuration into the _Custom Config_ box. - You can also enter `{‚Äúignore-all-case‚Äù: true}` to enable crawling assets with case-sensitive identifiers. - For _Popularity Window (days)_, 90 days is the maximum limit You can set a shorter popularity window of less than 90 days. ## Run the miner ‚Äã To run the Snowflake miner, after completing the steps above: - To run the miner once immediately, at the bottom of the screen, click the **Run** button. - To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule & Run** button Once the miner has completed running, you will see lineage for Snowflake assets that were created in Snowflake between the start time and when the miner ran! üéâ - Select the miner - Configure the miner - Configure the miner behavior - Run the miner",
    "metadata": {
      "topic": "Mine Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/mine-snowflake",
      "keywords": [
        "mine",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_an_azure_private_networ_0",
    "text": "On this page Azure Private Link creates a secure, private connection between services running in Azure This document describes the steps to set this up between Snowflake and Atlan Who can do this You will need Snowflake Support, and probably your Snowflake administrator involved - you may not have access or the tools to run these tasks. ## Prerequisites ‚Äã - Snowflake must be set up with Business Critical Edition (or higher). - Open a ticket with Snowflake Support to enable Azure Private Link for your Snowflake account. - Snowflake support will take 1-2 days to review and enable Azure Private Link. - If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist Please raise a support request to do so. (For all details, see the Snowflake documentation.) ## Fetch Private Link information ‚Äã Log in to snowCLI using the `ACCOUNTADMIN` account, and run the following commands: ```codeBlockLines_e6Vv use role accountadmin; select system$get_privatelink_config(); ``` This will produce an output like the following (formatted here for readability): ```codeBlockLines_e6Vv { \"regionless-snowsight-privatelink-url\": \"abc123.privatelink.snowflakecomputing.com\", \"privatelink-account-name\": \"abc123.west-europe.privatelink\", \"snowsight-privatelink-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-account-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-connection-ocsp-urls\": \"[]\", \"privatelink-pls-id\": \"abc123.westeurope.azure.privatelinkservice\", \"regionless-privatelink-account-url\": \"abc123.privatelink.snowflakecomputing.com\", \"privatelink_ocsp-url\": \"ocsp.abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-connection-urls\": \"[]\" } ``` ## Share details with Atlan support team ‚Äã Share the following values with the Atlan support team: - `regionless-snowsight-privatelink-url` - `privatelink-account-name` - `snowsight-privatelink-url` - `privatelink-account-url` - `privatelink-connection-ocsp-urls` - `privatelink-pls-id` - `regionless-privatelink-account-url` - `privatelink_ocsp-url` - `privatelink-connection-urls` Atlan support will finish the configuration on the Atlan side using these values Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request. ## Approve the endpoint connection request ‚Äã Log in to snowCLI using the `ACCOUNTADMIN` account, and run the following commands: ```codeBlockLines_e6Vv use role accountadmin; SELECT SYSTEM$AUTHORIZE_PRIVATELINK ( '/subscriptions/26d.../resourcegroups/sf-1/providers/microsoft.network/privateendpoints/test-self-service', 'eyJ...' ); ``` Snowflake will return an `Account is authorized for PrivateLink.` message to confirm successful authorization The status of the private endpoint in Atlan will then change to `Approved` When you use this endpoint in the configuration for crawling and mining Snowflake, Atlan will connect to Snowflake over the Private Link. ## (Optional) Configure private endpoint for internal stages ‚Äã Configure private endpoint for internal stages\") This is only required if you're using Snowflake internal stages To enable Atlan to securely access your Snowflake internal stages, Atlan will require a private endpoint to your Azure storage account Refer to Snowflake documentation to learn more To configure an Azure private endpoint to access Snowflake internal stages: 1 Open the Azure portal and navigate to your Azure Storage account. 2 On the _Storage accounts_ page, select the storage account to connect From the storage account menu, click **Overview** In the _Resource JSON_ form, for _Resource ID_, click the clipboard icon to copy the value and contact Atlan support to share the value. (Atlan support will finish the configuration on the Atlan side using the _Resource ID_ value and contact you to confirm endpoint creation.) 3",
    "metadata": {
      "topic": "Set up an Azure private network link to Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-azure-private-network-link-to-snowflake",
      "keywords": [
        "set",
        "azure",
        "private",
        "network",
        "link",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "enable_snowflake_oauth_0",
    "text": "On this page Atlan supports Snowflake OAuth-based authentication for Snowflake connections Once the integration has been completed, Atlan will generate a trusted secure token with Snowflake This will allow Atlan to authenticate users with Snowflake on their behalf to: - Query data with Snowflake OAuth credentials - View sample data with Snowflake OAuth credentials ## Configure Snowflake OAuth in Atlan ‚Äã Who can do this You will need to be a connection admin in Atlan to complete these steps You will also need inputs and approval from your Snowflake account administrator To configure Snowflake OAuth on a Snowflake connection, from Atlan: 1 From the left menu of any screen, click **Assets**. 2 From the _Assets_ page, click the **Connector** filter, and from the dropdown, click **Snowflake**. 3 From the pills below the search bar at the top of the screen, click **Connection**. 4 From the list of results, select a Snowflake connection to enable Snowflake OAuth-based authentication. 5 From the sidebar on the right, next to _Connection settings_, click **Edit**. 6 In the _Connection settings_ dialog: - Under _Allow query_, for _Authentication type_, click **Snowflake OAuth** to enforce Snowflake OAuth credentials for querying data: 1 For _Authentication Required_, click **Copy Code** to copy a security authorization code to execute it in Snowflake. - Under _Display sample data_, for _Source preview_, click **Snowflake OAuth** to enforce Snowflake OAuth credentials for viewing sample data: - If Snowflake OAuth-based authentication is enabled for querying data, the same connection details will be reused for viewing sample data. - If a different authentication method is enabled for querying data, click **Copy Code** to copy a security authorization code to execute it in Snowflake. 7. (Optional) Toggle on **Enable data policies created at source to apply for querying in Atlan** to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. 8 At the bottom right of the _Connection settings_ dialog, click **Update**. **Did you know?** The refresh token does not expire by default Configure Snowflake OAuth A Configure Snowflake OAuth !Begin by navigating to your assets!Select **Snowflake** as the connector type Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Create a security integration in Snowflake ‚Äã Who can do this You will need your Snowflake account administrator to run these commands You will also need to have an existing Snowflake connection in Atlan To create a security integration in Snowflake: 1 Log in to your Snowflake instance. 2 From the top right of your Snowflake instance, click the **+** button, and then from the dropdown, click **SQL Worksheet** to open a new worksheet. 3 In the query editor of your Snowflake SQL worksheet, paste the security authorization code you copied in Atlan",
    "metadata": {
      "topic": "Enable  Snowflake OAuth",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/enable-snowflake-oauth",
      "keywords": [
        "enable",
        "snowflake",
        "oauth",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_snowflake_0",
    "text": "On this page Once you have configured the Snowflake user permissions, you can establish a connection between Atlan and Snowflake. (If you are also using AWS PrivateLink or Azure Private Link for Snowflake, you will need to set that up first, too.) Crawl Snowflake A Crawl Snowflake !Begin by adding a new workflow!The Snowflake Assets package allows us to crawl metadata from Snowflake Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms To crawl metadata from Snowflake, review the order of operations and then complete the following steps. ## Select the source ‚Äã To select Snowflake as your source: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the list of packages, select **Snowflake Assets** and click on **Setup Workflow**. ## Provide credentials ‚Äã Choose your extraction method: - In **Direct** extraction, Atlan connects to your database and crawls metadata directly. - In **Offline** extraction, you will need to first extract metadata yourself and make it available in S3 This is currently only supported when using the information schema extraction method to fetch metadata with basic authentication. - In **Agent** extraction, Atlan's secure agent executes metadata extraction within the organization's environment. ### Direct extraction method ‚Äã To enter your Snowflake credentials: 1 For _Account Identifiers (Host)_, enter the hostname, AWS PrivateLink endpoint, or Azure Private Link endpoint for your Snowflake instance. 2 For _Authentication_, choose the method you configured when setting up the Snowflake user: - For **Basic** authentication, enter the _Username_ and _Password_ you configured in either Snowflake or the identity provider. info üí™ **Did you know?** Snowflake recommends transitioning away from basic authentication using username and password Change to key-pair authentication for enhanced security For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. - For **Keypair** authentication, enter the _Username_, _Encrypted Private Key_, and _Private Key_ _Password_ you configured Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure An empty passphrase will result in workflow failures To generate an encrypted private key, refer to Snowflake documentation. - For **Okta SSO** authentication, enter the _Username_, _Password_, and _Authenticator_ you configured The _Authenticator_ will be the Okta URL endpoint of your Okta account, typically in the form of `https://<okta_account_name>.okta.com`. 3 For _Role_, select the Snowflake role through which the crawler should run. 4 For _Warehouse_, select the Snowflake warehouse in which the crawler should run. 5 Click **Test Authentication** to confirm connectivity to Snowflake using these details. 6 Once successful, at the bottom of the screen, click **Next**. ### Offline extraction method ‚Äã Atlan supports the offline extraction method for fetching metadata from Snowflake This method uses Atlan's metadata-extractor tool to fetch metadata You will need to first extract the metadata yourself and then make it available in S3 To enter your S3 details: 1 For _Bucket name_, enter the name of your S3 bucket",
    "metadata": {
      "topic": "Crawl Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
      "keywords": [
        "crawl",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_snowflake_1",
    "text": "To enter your S3 details: 1 For _Bucket name_, enter the name of your S3 bucket If you are reusing Atlan's S3 bucket, you can leave this blank. 2 For _Bucket prefix_, enter the S3 prefix under which all the metadata files exist These include `databases.json`, `columns-<database>.json`, and so on. 3 For _Bucket region_, enter the name of the S3 region. 4 When complete, at the bottom of the screen, click **Next**. ## Configure the connection ‚Äã To complete the Snowflake connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. 3. (Optional) To prevent users from querying any Snowflake data, change _Allow SQL Query_ to **No**. 4. (Optional) To prevent users from previewing any Snowflake data, change _Allow Data Preview_ to **No**. 5 At the bottom of the screen, click **Next** to proceed. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for fetching metadata from Snowflake To use a Secure Agent, follow these steps: 1 Select the **Agent** tab. 2 Configure the Snowflake data source by adding the secret keys for your secret store For details on the required fields, refer to the Direct extraction section. 3 Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. 4 Click **Next** after completing the configuration. ## Configure the crawler ‚Äã danger When modifying an existing Snowflake connection, switching to a different extraction method will delete and recreate all assets in the existing connection If you'd like to change the extraction method, contact Atlan support for assistance Before running the Snowflake crawler, you can further configure it You must select the _Extraction method_ you configured when you set up Snowflake: - For **Information Schema** method, keep the default selection. - Change to **Account Usage** method and specify the following: - _Database Name_ of the copied Snowflake database - _Schema Name_ of the copied `ACCOUNT_USAGE` schema - **Incremental extraction** Public preview - Toggle incremental extraction for faster and more efficient metadata extraction",
    "metadata": {
      "topic": "Crawl Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
      "keywords": [
        "crawl",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_snowflake_2",
    "text": "Before running the Snowflake crawler, you can further configure it You must select the _Extraction method_ you configured when you set up Snowflake: - For **Information Schema** method, keep the default selection. - Change to **Account Usage** method and specify the following: - _Database Name_ of the copied Snowflake database - _Schema Name_ of the copied `ACCOUNT_USAGE` schema - **Incremental extraction** Public preview - Toggle incremental extraction for faster and more efficient metadata extraction You can override the defaults for any of the remaining options: - For _Asset selection_, select a filtering option: - To select the assets you want to include in crawling, click **Include by hierarchy** and filter for assets down to the database or schema level. (This will default to all assets, if none are specified.) - To have the crawler include _Databases_, _Schemas_, or _Tables & Views_ based on a naming convention, click **Include by regex** and specify a regular expression - for example, specifying `ATLAN_EXAMPLE_DB.*` for _Databases_ will include all the matching databases and their child assets. - To select the assets you want to exclude from crawling, click **Exclude by hierarchy** and filter for assets down to the database or schema level. (This will default to no assets, if none are specified.) - To have the crawler ignore _Databases_, _Schemas_, or _Tables & Views_ based on a naming convention, click **Exclude by regex** and specify a regular expression - for example, specifying `ATLAN_EXAMPLE_TABLES.*` for _Tables & Views_ will exclude all the matching tables and views. - Click **+** to add more filters If you add multiple filters, assets will be crawled based on matching _all_ the filtering conditions you have set. - To exclude lineage for views in Snowflake, change _View Definition Lineage_ to **No**. - To import tags from Snowflake to Atlan, change _Import Tags_ to **Yes** Note the following: - If using the _Account Usage_ extraction method, grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. - If using the _Information Schema_ extraction method, note that Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema You will need to grant permissions on the account usage schema instead to import tags from Snowflake. danger Object tagging in Snowflake currently requires Enterprise Edition or higher If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error - unable to retrieve tags. - For _Control Config_, keep _Default_ for the default configuration or click **Custom** to further configure the crawler: - If you have received a custom crawler configuration from Atlan support, for _Custom Config_, enter the value provided",
    "metadata": {
      "topic": "Crawl Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
      "keywords": [
        "crawl",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_snowflake_3",
    "text": "You will need to grant permissions on the account usage schema instead to import tags from Snowflake. danger Object tagging in Snowflake currently requires Enterprise Edition or higher If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error - unable to retrieve tags. - For _Control Config_, keep _Default_ for the default configuration or click **Custom** to further configure the crawler: - If you have received a custom crawler configuration from Atlan support, for _Custom Config_, enter the value provided You can also: - Enter `{\"ignore-all-case\": true}` to enable crawling assets with case-sensitive identifiers. - For _Enable Source Level Filtering_, click **True** to enable schema-level filtering at source or keep _False_ to disable it. - For _Use JDBC Internal Methods_, click **True** to enable JDBC internal methods for data extraction or click **False** to disable it. - For _Exclude tables with empty data_, change to **Yes** to exclude any tables and corresponding columns without any data. - For _Exclude views_, change to **Yes** to exclude all views from crawling. **Did you know?** If an asset appears in both the include and exclude filters, the exclude filter takes precedence. ## Run the crawler ‚Äã To run the Snowflake crawler, after completing the steps above: 1 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. 2 You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button Once the crawler has completed running, you will see the assets in Atlan's asset page! üéâ Note that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run. - Select the source - Provide credentials - Configure the connection - Configure the crawler - Run the crawler",
    "metadata": {
      "topic": "Crawl Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
      "keywords": [
        "crawl",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "preflight_checks_for_snowflake_0",
    "text": "On this page Before running the Snowflake crawler, you can run preflight checks to perform the necessary technical validations The following preflight checks will be completed: ## Database and schema check ‚Äã ### Information schema ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed for $missingObjectName` ### Account usage ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed for $missingObjectName` ## Warehouse access ‚Äã ‚úÖ `Check successful` ‚ùå `Operation cannot be performed` / `User is not authorized to perform this action` / `Cannot be resumed because resource monitor has exceeded its quota` ## Miner ‚Äã **Did you know?** Once you have crawled assets from Snowflake, you can mine query history. ### Query history view ‚Äã ‚úÖ `Check successful` ‚ùå `Cannot access query history table Please run the command in your Snowflake instance: GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role;` ### Access history view ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed Something went wrong with your request.` ### Sessions view ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed Something went wrong with your request.` ### S3 ‚Äã ‚úÖ `Check successful` if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. ‚ùå `Check failed with error code <AWS error code> - <AWS SDK ERR message>` For example: `Miner S3 credentials: failed with error code: NoSuchBucket` - Database and schema check - Warehouse access - Miner",
    "metadata": {
      "topic": "Preflight checks for Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/references/preflight-checks-for-snowflake",
      "keywords": [
        "preflight",
        "checks",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_0",
    "text": "On this page To use system data metric functions (DMFs) from Snowflake, you need to configure your Snowflake setup. ## Prerequisites ‚Äã Before proceeding with the configuration, make sure the following prerequisites are met: - Snowflake editions - Atlan data quality for Snowflake is only supported for Enterprise and Business Critical editions of Snowflake. - Administrative access - the user configuring Snowflake must have an `ACCOUNTADMIN` role or equivalent administrative privileges. - Dedicated warehouse - your organization must have a dedicated Snowflake warehouse for running data quality-related queries. ## Create roles in Snowflake ‚Äã You will need to create the following two roles in Snowflake for the Atlan data quality integration: - Data quality admin role ( `dq_admin`) - a high-privilege role responsible for managing DMF associations on tables and views This role is used to create and manage the database objects required for data quality operations. - Atlan data quality service role ( `atlan_dq_service_role`) - a service role that Atlan will use to interact with Snowflake objects related to data quality operations This role will be assigned to the Atlan service user. ### Create data quality admin role ‚Äã Run the following commands to create the `dq_admin` role and grant access to the Snowflake warehouse: ```codeBlockLines_e6Vv CREATE OR REPLACE ROLE dq_admin; GRANT OPERATE, USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE dq_admin; ``` - Replace `<warehouse-name>` with the name of your dedicated Snowflake warehouse for running data quality-related queries. ### Create Atlan data quality service role ‚Äã Run the following commands to create the `atlan_dq_service_role` and grant access to the Snowflake warehouse: ```codeBlockLines_e6Vv CREATE OR REPLACE ROLE atlan_dq_service_role; GRANT OPERATE, USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_dq_service_role; ``` - Replace `<warehouse-name>` with the name of your dedicated Snowflake warehouse for running data quality-related queries. ## Create a user in Snowflake ‚Äã A dedicated Snowflake user is required for Atlan to connect to your Snowflake instance You will need to create this integration user and assign the Atlan data quality service role to this user Refer to the Create a user documentation to create the new user",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_1",
    "text": "You will need to create this integration user and assign the Atlan data quality service role to this user Refer to the Create a user documentation to create the new user After creating the user, grant the Atlan data quality service role to the new user you created in Snowflake: ```codeBlockLines_e6Vv GRANT ROLE atlan_dq_service_role TO USER <atlan_dq_user>; ``` ## Grant privileges ‚Äã The following privileges must be granted to the roles created in Snowflake for the Atlan data quality integration: ### Privileges for data quality admin ‚Äã Grant the `dq_admin` role the ability to create databases and access system DMFs in Snowflake: ```codeBlockLines_e6Vv GRANT CREATE DATABASE ON ACCOUNT TO ROLE dq_admin; GRANT DATABASE ROLE SNOWFLAKE.DATA_METRIC_USER TO ROLE dq_admin; ``` ### Privileges for table owners ‚Äã For each role that owns tables in your Snowflake environment, grant the following privileges: ```codeBlockLines_e6Vv GRANT ROLE <table_owner> TO ROLE dq_admin; GRANT DATABASE ROLE SNOWFLAKE.DATA_METRIC_USER TO ROLE <table_owner>; GRANT EXECUTE DATA METRIC FUNCTION ON ACCOUNT TO ROLE <table_owner>; ``` - Replace `<table_owner>` with the role that owns Snowflake tables. ### Privileges for Atlan data quality service role ‚Äã Grant the following privileges to the `atlan_dq_service_role`: ```codeBlockLines_e6Vv GRANT APPLICATION ROLE SNOWFLAKE.DATA_QUALITY_MONITORING_VIEWER TO ROLE atlan_dq_service_role; GRANT DATABASE ROLE SNOWFLAKE.DATA_METRIC_USER TO ROLE atlan_dq_service_role; GRANT EXECUTE TASK ON ACCOUNT TO ROLE atlan_dq_service_role; GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE atlan_dq_service_role; ``` ## Set up required objects ‚Äã Once you have created roles and granted the required privileges, you will need to create the necessary objects such as a dedicated database, schema, and stored procedure to be used for managing DMF operations. 1 Change to the `dq_admin` role: ```codeBlockLines_e6Vv USE ROLE dq_admin; ``` 2 Create the database and schema in Snowflake for the Atlan data quality integration: ```codeBlockLines_e6Vv CREATE DATABASE ATLAN_DQ; CREATE SCHEMA ATLAN_DQ.SHARED; ``` - The `ATLAN_DQ` database serves as a container for all objects related to the Atlan data quality integration. - The `ATLAN_DQ.SHARED` schema provides a separate namespace for shared procedures and functions. 3 Create the store procedure in Snowflake to manage DMFs: ```codeBlockLines_e6Vv / Manages Data Metric Functions (DMF) attachment operations for Snowflake tabular entities",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_2",
    "text": "Create the database and schema in Snowflake for the Atlan data quality integration: ```codeBlockLines_e6Vv CREATE DATABASE ATLAN_DQ; CREATE SCHEMA ATLAN_DQ.SHARED; ``` - The `ATLAN_DQ` database serves as a container for all objects related to the Atlan data quality integration. - The `ATLAN_DQ.SHARED` schema provides a separate namespace for shared procedures and functions. 3 Create the store procedure in Snowflake to manage DMFs: ```codeBlockLines_e6Vv / Manages Data Metric Functions (DMF) attachment operations for Snowflake tabular entities Runs with the privileges of the procedure owner. * @param {string} ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE) * @param {string} ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param {string} ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param {string} [DMF_NAME=null] - Fully qualified name of the DMF (database.schema.name) * @param {string} [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param {string} [SCHEDULE_TYPE=null] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param {string} [SCHEDULE_VALUE=null] - Schedule value based on type * @returns {string} - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ.SHARED.MANAGE_DMF( ACTION STRING, ENTITY_TYPE STRING, ENTITY_NAME STRING, DMF_NAME STRING DEFAULT NULL, DMF_ARGUMENTS_JSON STRING DEFAULT '[]', SCHEDULE_TYPE STRING DEFAULT NULL, SCHEDULE_VALUE STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- / Executes a SQL query with parameters * @param {string} sqlText - SQL statement to execute * @param {Array} [binds=[]] - Array of bind parameters for the query * @param {boolean} [returnFirstRow=false] - Whether to return only the first row * @returns {Object} Object containing execution result or error information */ function executeQuery(sqlText, binds = [], returnFirstRow = false) { try { if (!sqlText) return { isErrored: true, message: \"SQL Text is required\", result: null, }; const statement = snowflake.createStatement({ sqlText, binds }); const result = statement.execute(); const response = { isErrored: false, message: \"\", result: null, }; if (returnFirstRow) { response.result = result.next() ? result : null; return response; } response.result = result; return response; } catch (err) { return { isErrored: true, message: `${err.code} - ${err.message} - ${sqlText} with binds: ${binds.join(\", \")}`, result: null, }; } } / Safely parses a JSON string * @param {string} jsonString - JSON string to parse * @returns {Object} Parsed JSON object or null if invalid */ function safelyParseJSON(jsonString) { try { return JSON.parse(jsonString); } catch (err) { return null; } } / Validates a number within a range * @param {string} value - Number to validate * @param {number} min - Minimum value * @param {number} max - Maximum value * @returns {boolean} True if number is valid * @returns {boolean} False if number is invalid */ function isNumberValid(value, min, max) { const num = parseInt(value, 10); return !isNaN(num) && num = min && num &le; max;=\"max;\" }=\"}\" **=\"**\" *=\"*\" escapes=\"Escapes\" and=\"and\" quotes=\"quotes\" a=\"a\" snowflake=\"Snowflake\" identifier=\"identifier\" @param=\"@param\" {string}=\"{string}\" -=\"-\" raw=\"Raw\" to=\"to\" normalize=\"normalize\" @returns=\"@returns\" properly=\"Properly\" quoted=\"quoted\" safe=\"safe\" for=\"for\" sql=\"SQL\" function=\"FUNCTION\" normalizeidentifier(identifier)=\"normalizeIdentifier(identifier)\" {=\"{\" return=\"return\" `=\"`\" ${identifier.replace(=\"${identifier.replace(\" g,=\"g,\" )}=\")}\" `;=\"`;\" retrieves=\"Retrieves\" all=\"all\" columns=\"columns\" given=\"given\" entity.=\"entity.\" validates=\"Validates\" that=\"that\" the=\"the\" entityexists=\"entityexists\" procedure=\"procedure\" owner=\"owner\" has=\"has\" access=\"access\" it.=\"it.\" entityname=\"entityName\" fully=\"Fully\" qualified=\"qualified\" entity=\"entity\" name=\"name\" {array}=\"{Array}\" array=\"Array\" of=\"of\" column=\"column\" objects=\"objects\" with=\"with\" datatype=\"dataType\" properties=\"properties\" @throws=\"@throws\" {error}=\"{Error}\" if=\"if\" doesn=\"doesn\" t=\"t\" exist=\"exist\" or=\"or\" is=\"is\" inaccessible=\"inaccessible\" getallcolumnsforentity(entityname)=\"getAllColumnsForEntity(entityName)\" const=\"const\" sqltext=\"SHOW COLUMNS IN IDENTIFIER(?)\" ;=\";\" binds=\"[entityName];\" result,=\"result,\" iserrored,=\"isErrored,\" message=\"message\" binds);=\"binds);\" (iserrored)=\"(isErrored)\" exists=\"exists\" it=\"it\" throw=\"throw\" new=\"new\" error(message);=\"Error(message);\" while=\"while\" (result.next())=\"(result.next())\" name:=\"name:\" result.getcolumnvalue(=\"result.getColumnValue(\" column_name=\"column_name\" ),=\"),\" datatype:=\"dataType:\" json.parse(result.getcolumnvalue(=\"JSON.parse(result.getColumnValue(\" data_type=\"data_type\" )).type,=\")).type,\" };=\"};\" (column.datatype=\"==\" fixed=\"FIXED\" )=\")\" column.datatype=\"NUMBER\" columns.push(column);=\"columns.push(column);\" columns;=\"columns;\" dmf=\"DMF\" valid=\"valid\" dmfname=\"dmfName\" dmfarguments=\"dmfArguments\" arguments=\"arguments\" {boolean}=\"{boolean}\" whether=\"Whether\" invalid=\"invalid\" isdmfvalid(dmfname,=\"isDMFValid(dmfName,\" dmfarguments)=\"dmfArguments)\" identifier(?)(${dmfarguments})`,=\"IDENTIFIER(?)(${dmfArguments})`,\" [dmfname],=\"[dmfName],\" true);=\"true);\" true;=\"true;\" checks=\"Checks\" timezone=\"Timezone\" validate=\"validate\" true=\"True\" false=\"False\" istimezonevalid(timezone)=\"isTimezoneValid(timezone)\" result=\"executeQuery(`SELECT\" convert_timezone(?,=\"CONVERT_TIMEZONE(?,\" current_timestamp())`,=\"CURRENT_TIMESTAMP())`,\" [timezone],=\"[timezone],\" !result.iserrored;=\"!result.isErrored;\" generates=\"Generates\" type=\"type\" signature=\"signature\" based=\"based\" on=\"on\" {object}=\"{Object}\" entitycolumnsmap=\"entityColumnsMap\" map=\"Map\" names=\"names\" in=\"in\" format=\"format\">: [ { name: , dataType: } ] } * @param {string} baseEntityName - Name of the base entity * @returns {string} DMF type signature * @throws {Error} If entity not found in the cache */ function generateDMFTypeSignature(dmfArguments, entityColumnsMap, baseEntityName) { if(!dmfArguments || !dmfArguments.length) return \"\"; const baseEntityColumns = entityColumnsMap[baseEntityName]; if (!baseEntityColumns) { throw new Error(`Entity ${baseEntityName} not found in the cache`); } const baseEntityColumnArguments = dmfArguments .filter(param = param.type === \"COLUMN\") .map(param = { const column = baseEntityColumns.find(col = col.name === param.name); return column ? column.dataType : null; }) .join(\", \"); const baseEntityArguments = `TABLE(${baseEntityColumnArguments})`; const referencedEntityArguments = dmfArguments .filter(param = param.type === \"TABLE\") .map(entityParam = { const entityName = entityParam.name; const entityColumns = entityColumnsMap[entityName]; if (!entityColumns) { throw new Error(`Entity ${entityName} not found in the cache`); } const columnTypes = entityParam.nested .map(nestedParam = { const column = entityColumns.find(col = col.name === nestedParam.name); return column ? column.dataType : null; }) .filter(Boolean) .join(\", \"); return `TABLE(${columnTypes})`; }); const arguments = [baseEntityArguments, ...referencedEntityArguments].join(\", \"); return arguments; } / Generates DMF arguments for SQL statements * @param {string} dmfArguments - Array of DMF arguments * @returns {string} Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments(dmfArguments) { return dmfArguments .map(param = { if (param.type === \"COLUMN\") { return normalizeIdentifier(param.name); } // Handle TABLE type with nested columns return `${normalizeIdentifier(param.name)}(${ param.nested .map(nested = normalizeIdentifier(nested.name)) .join(\", \") })`; }) .join(\", \"); } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- / Validates that mandatory arguments are provided and valid * @throws {Error} If any mandatory argument is missing or invalid */ function validateMandatoryArguments() { const VALID_ACTIONS = new Set([\"ATTACH_DMF\", \"DETACH_DMF\", \"SUSPEND_DMF\", \"RESUME_DMF\", \"UPDATE_SCHEDULE\"]); const VALID_ENTITY_TYPES = new Set([\"TABLE\", \"VIEW\", \"MATERIALIZED VIEW\", \"EXTERNAL TABLE\", \"ICEBERG TABLE\"]); const DMF_OPS = new Set([\"ATTACH_DMF\", \"DETACH_DMF\", \"SUSPEND_DMF\", \"RESUME_DMF\"]); const VALID_SCHEDULE_TYPES = new Set([\"MINUTES\", \"CRON\", \"ON_DATA_CHANGE\", \"NOT_SCHEDULED\"]); const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set([\"MINUTES\", \"CRON\"]); if (!VALID_ACTIONS.has(ACTION)) throw new Error( `Invalid ACTION: \"${ACTION}\"",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_3",
    "text": "Create the store procedure in Snowflake to manage DMFs: ```codeBlockLines_e6Vv / Manages Data Metric Functions (DMF) attachment operations for Snowflake tabular entities Runs with the privileges of the procedure owner. * @param {string} ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE) * @param {string} ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param {string} ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param {string} [DMF_NAME=null] - Fully qualified name of the DMF (database.schema.name) * @param {string} [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param {string} [SCHEDULE_TYPE=null] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param {string} [SCHEDULE_VALUE=null] - Schedule value based on type * @returns {string} - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ.SHARED.MANAGE_DMF( ACTION STRING, ENTITY_TYPE STRING, ENTITY_NAME STRING, DMF_NAME STRING DEFAULT NULL, DMF_ARGUMENTS_JSON STRING DEFAULT '[]', SCHEDULE_TYPE STRING DEFAULT NULL, SCHEDULE_VALUE STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- / Executes a SQL query with parameters * @param {string} sqlText - SQL statement to execute * @param {Array} [binds=[]] - Array of bind parameters for the query * @param {boolean} [returnFirstRow=false] - Whether to return only the first row * @returns {Object} Object containing execution result or error information */ function executeQuery(sqlText, binds = [], returnFirstRow = false) { try { if (!sqlText) return { isErrored: true, message: \"SQL Text is required\", result: null, }; const statement = snowflake.createStatement({ sqlText, binds }); const result = statement.execute(); const response = { isErrored: false, message: \"\", result: null, }; if (returnFirstRow) { response.result = result.next() ? result : null; return response; } response.result = result; return response; } catch (err) { return { isErrored: true, message: `${err.code} - ${err.message} - ${sqlText} with binds: ${binds.join(\", \")}`, result: null, }; } } / Safely parses a JSON string * @param {string} jsonString - JSON string to parse * @returns {Object} Parsed JSON object or null if invalid */ function safelyParseJSON(jsonString) { try { return JSON.parse(jsonString); } catch (err) { return null; } } / Validates a number within a range * @param {string} value - Number to validate * @param {number} min - Minimum value * @param {number} max - Maximum value * @returns {boolean} True if number is valid * @returns {boolean} False if number is invalid */ function isNumberValid(value, min, max) { const num = parseInt(value, 10); return !isNaN(num) && num = min && num &le; max;=\"max;\" }=\"}\" **=\"**\" *=\"*\" escapes=\"Escapes\" and=\"and\" quotes=\"quotes\" a=\"a\" snowflake=\"Snowflake\" identifier=\"identifier\" @param=\"@param\" {string}=\"{string}\" -=\"-\" raw=\"Raw\" to=\"to\" normalize=\"normalize\" @returns=\"@returns\" properly=\"Properly\" quoted=\"quoted\" safe=\"safe\" for=\"for\" sql=\"SQL\" function=\"FUNCTION\" normalizeidentifier(identifier)=\"normalizeIdentifier(identifier)\" {=\"{\" return=\"return\" `=\"`\" ${identifier.replace(=\"${identifier.replace(\" g,=\"g,\" )}=\")}\" `;=\"`;\" retrieves=\"Retrieves\" all=\"all\" columns=\"columns\" given=\"given\" entity.=\"entity.\" validates=\"Validates\" that=\"that\" the=\"the\" entityexists=\"entityexists\" procedure=\"procedure\" owner=\"owner\" has=\"has\" access=\"access\" it.=\"it.\" entityname=\"entityName\" fully=\"Fully\" qualified=\"qualified\" entity=\"entity\" name=\"name\" {array}=\"{Array}\" array=\"Array\" of=\"of\" column=\"column\" objects=\"objects\" with=\"with\" datatype=\"dataType\" properties=\"properties\" @throws=\"@throws\" {error}=\"{Error}\" if=\"if\" doesn=\"doesn\" t=\"t\" exist=\"exist\" or=\"or\" is=\"is\" inaccessible=\"inaccessible\" getallcolumnsforentity(entityname)=\"getAllColumnsForEntity(entityName)\" const=\"const\" sqltext=\"SHOW COLUMNS IN IDENTIFIER(?)\" ;=\";\" binds=\"[entityName];\" result,=\"result,\" iserrored,=\"isErrored,\" message=\"message\" binds);=\"binds);\" (iserrored)=\"(isErrored)\" exists=\"exists\" it=\"it\" throw=\"throw\" new=\"new\" error(message);=\"Error(message);\" while=\"while\" (result.next())=\"(result.next())\" name:=\"name:\" result.getcolumnvalue(=\"result.getColumnValue(\" column_name=\"column_name\" ),=\"),\" datatype:=\"dataType:\" json.parse(result.getcolumnvalue(=\"JSON.parse(result.getColumnValue(\" data_type=\"data_type\" )).type,=\")).type,\" };=\"};\" (column.datatype=\"==\" fixed=\"FIXED\" )=\")\" column.datatype=\"NUMBER\" columns.push(column);=\"columns.push(column);\" columns;=\"columns;\" dmf=\"DMF\" valid=\"valid\" dmfname=\"dmfName\" dmfarguments=\"dmfArguments\" arguments=\"arguments\" {boolean}=\"{boolean}\" whether=\"Whether\" invalid=\"invalid\" isdmfvalid(dmfname,=\"isDMFValid(dmfName,\" dmfarguments)=\"dmfArguments)\" identifier(?)(${dmfarguments})`,=\"IDENTIFIER(?)(${dmfArguments})`,\" [dmfname],=\"[dmfName],\" true);=\"true);\" true;=\"true;\" checks=\"Checks\" timezone=\"Timezone\" validate=\"validate\" true=\"True\" false=\"False\" istimezonevalid(timezone)=\"isTimezoneValid(timezone)\" result=\"executeQuery(`SELECT\" convert_timezone(?,=\"CONVERT_TIMEZONE(?,\" current_timestamp())`,=\"CURRENT_TIMESTAMP())`,\" [timezone],=\"[timezone],\" !result.iserrored;=\"!result.isErrored;\" generates=\"Generates\" type=\"type\" signature=\"signature\" based=\"based\" on=\"on\" {object}=\"{Object}\" entitycolumnsmap=\"entityColumnsMap\" map=\"Map\" names=\"names\" in=\"in\" format=\"format\">: [ { name: , dataType: } ] } * @param {string} baseEntityName - Name of the base entity * @returns {string} DMF type signature * @throws {Error} If entity not found in the cache */ function generateDMFTypeSignature(dmfArguments, entityColumnsMap, baseEntityName) { if(!dmfArguments || !dmfArguments.length) return \"\"; const baseEntityColumns = entityColumnsMap[baseEntityName]; if (!baseEntityColumns) { throw new Error(`Entity ${baseEntityName} not found in the cache`); } const baseEntityColumnArguments = dmfArguments .filter(param = param.type === \"COLUMN\") .map(param = { const column = baseEntityColumns.find(col = col.name === param.name); return column ? column.dataType : null; }) .join(\", \"); const baseEntityArguments = `TABLE(${baseEntityColumnArguments})`; const referencedEntityArguments = dmfArguments .filter(param = param.type === \"TABLE\") .map(entityParam = { const entityName = entityParam.name; const entityColumns = entityColumnsMap[entityName]; if (!entityColumns) { throw new Error(`Entity ${entityName} not found in the cache`); } const columnTypes = entityParam.nested .map(nestedParam = { const column = entityColumns.find(col = col.name === nestedParam.name); return column ? column.dataType : null; }) .filter(Boolean) .join(\", \"); return `TABLE(${columnTypes})`; }); const arguments = [baseEntityArguments, ...referencedEntityArguments].join(\", \"); return arguments; } / Generates DMF arguments for SQL statements * @param {string} dmfArguments - Array of DMF arguments * @returns {string} Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments(dmfArguments) { return dmfArguments .map(param = { if (param.type === \"COLUMN\") { return normalizeIdentifier(param.name); } // Handle TABLE type with nested columns return `${normalizeIdentifier(param.name)}(${ param.nested .map(nested = normalizeIdentifier(nested.name)) .join(\", \") })`; }) .join(\", \"); } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- / Validates that mandatory arguments are provided and valid * @throws {Error} If any mandatory argument is missing or invalid */ function validateMandatoryArguments() { const VALID_ACTIONS = new Set([\"ATTACH_DMF\", \"DETACH_DMF\", \"SUSPEND_DMF\", \"RESUME_DMF\", \"UPDATE_SCHEDULE\"]); const VALID_ENTITY_TYPES = new Set([\"TABLE\", \"VIEW\", \"MATERIALIZED VIEW\", \"EXTERNAL TABLE\", \"ICEBERG TABLE\"]); const DMF_OPS = new Set([\"ATTACH_DMF\", \"DETACH_DMF\", \"SUSPEND_DMF\", \"RESUME_DMF\"]); const VALID_SCHEDULE_TYPES = new Set([\"MINUTES\", \"CRON\", \"ON_DATA_CHANGE\", \"NOT_SCHEDULED\"]); const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set([\"MINUTES\", \"CRON\"]); if (!VALID_ACTIONS.has(ACTION)) throw new Error( `Invalid ACTION: \"${ACTION}\" Valid options are ${Array.from(VALID_ACTIONS).join(\", \")}` ); if (!VALID_ENTITY_TYPES.has(ENTITY_TYPE)) throw new Error( `Invalid ENTITY_TYPE: \"${ENTITY_TYPE}\"",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_4",
    "text": "Runs with the privileges of the procedure owner. * @param {string} ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE) * @param {string} ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param {string} ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param {string} [DMF_NAME=null] - Fully qualified name of the DMF (database.schema.name) * @param {string} [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param {string} [SCHEDULE_TYPE=null] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param {string} [SCHEDULE_VALUE=null] - Schedule value based on type * @returns {string} - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ.SHARED.MANAGE_DMF( ACTION STRING, ENTITY_TYPE STRING, ENTITY_NAME STRING, DMF_NAME STRING DEFAULT NULL, DMF_ARGUMENTS_JSON STRING DEFAULT '[]', SCHEDULE_TYPE STRING DEFAULT NULL, SCHEDULE_VALUE STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- / Executes a SQL query with parameters * @param {string} sqlText - SQL statement to execute * @param {Array} [binds=[]] - Array of bind parameters for the query * @param {boolean} [returnFirstRow=false] - Whether to return only the first row * @returns {Object} Object containing execution result or error information */ function executeQuery(sqlText, binds = [], returnFirstRow = false) { try { if (!sqlText) return { isErrored: true, message: \"SQL Text is required\", result: null, }; const statement = snowflake.createStatement({ sqlText, binds }); const result = statement.execute(); const response = { isErrored: false, message: \"\", result: null, }; if (returnFirstRow) { response.result = result.next() ? result : null; return response; } response.result = result; return response; } catch (err) { return { isErrored: true, message: `${err.code} - ${err.message} - ${sqlText} with binds: ${binds.join(\", \")}`, result: null, }; } } / Safely parses a JSON string * @param {string} jsonString - JSON string to parse * @returns {Object} Parsed JSON object or null if invalid */ function safelyParseJSON(jsonString) { try { return JSON.parse(jsonString); } catch (err) { return null; } } / Validates a number within a range * @param {string} value - Number to validate * @param {number} min - Minimum value * @param {number} max - Maximum value * @returns {boolean} True if number is valid * @returns {boolean} False if number is invalid */ function isNumberValid(value, min, max) { const num = parseInt(value, 10); return !isNaN(num) && num = min && num &le; max;=\"max;\" }=\"}\" **=\"**\" *=\"*\" escapes=\"Escapes\" and=\"and\" quotes=\"quotes\" a=\"a\" snowflake=\"Snowflake\" identifier=\"identifier\" @param=\"@param\" {string}=\"{string}\" -=\"-\" raw=\"Raw\" to=\"to\" normalize=\"normalize\" @returns=\"@returns\" properly=\"Properly\" quoted=\"quoted\" safe=\"safe\" for=\"for\" sql=\"SQL\" function=\"FUNCTION\" normalizeidentifier(identifier)=\"normalizeIdentifier(identifier)\" {=\"{\" return=\"return\" `=\"`\" ${identifier.replace(=\"${identifier.replace(\" g,=\"g,\" )}=\")}\" `;=\"`;\" retrieves=\"Retrieves\" all=\"all\" columns=\"columns\" given=\"given\" entity.=\"entity.\" validates=\"Validates\" that=\"that\" the=\"the\" entityexists=\"entityexists\" procedure=\"procedure\" owner=\"owner\" has=\"has\" access=\"access\" it.=\"it.\" entityname=\"entityName\" fully=\"Fully\" qualified=\"qualified\" entity=\"entity\" name=\"name\" {array}=\"{Array}\" array=\"Array\" of=\"of\" column=\"column\" objects=\"objects\" with=\"with\" datatype=\"dataType\" properties=\"properties\" @throws=\"@throws\" {error}=\"{Error}\" if=\"if\" doesn=\"doesn\" t=\"t\" exist=\"exist\" or=\"or\" is=\"is\" inaccessible=\"inaccessible\" getallcolumnsforentity(entityname)=\"getAllColumnsForEntity(entityName)\" const=\"const\" sqltext=\"SHOW COLUMNS IN IDENTIFIER(?)\" ;=\";\" binds=\"[entityName];\" result,=\"result,\" iserrored,=\"isErrored,\" message=\"message\" binds);=\"binds);\" (iserrored)=\"(isErrored)\" exists=\"exists\" it=\"it\" throw=\"throw\" new=\"new\" error(message);=\"Error(message);\" while=\"while\" (result.next())=\"(result.next())\" name:=\"name:\" result.getcolumnvalue(=\"result.getColumnValue(\" column_name=\"column_name\" ),=\"),\" datatype:=\"dataType:\" json.parse(result.getcolumnvalue(=\"JSON.parse(result.getColumnValue(\" data_type=\"data_type\" )).type,=\")).type,\" };=\"};\" (column.datatype=\"==\" fixed=\"FIXED\" )=\")\" column.datatype=\"NUMBER\" columns.push(column);=\"columns.push(column);\" columns;=\"columns;\" dmf=\"DMF\" valid=\"valid\" dmfname=\"dmfName\" dmfarguments=\"dmfArguments\" arguments=\"arguments\" {boolean}=\"{boolean}\" whether=\"Whether\" invalid=\"invalid\" isdmfvalid(dmfname,=\"isDMFValid(dmfName,\" dmfarguments)=\"dmfArguments)\" identifier(?)(${dmfarguments})`,=\"IDENTIFIER(?)(${dmfArguments})`,\" [dmfname],=\"[dmfName],\" true);=\"true);\" true;=\"true;\" checks=\"Checks\" timezone=\"Timezone\" validate=\"validate\" true=\"True\" false=\"False\" istimezonevalid(timezone)=\"isTimezoneValid(timezone)\" result=\"executeQuery(`SELECT\" convert_timezone(?,=\"CONVERT_TIMEZONE(?,\" current_timestamp())`,=\"CURRENT_TIMESTAMP())`,\" [timezone],=\"[timezone],\" !result.iserrored;=\"!result.isErrored;\" generates=\"Generates\" type=\"type\" signature=\"signature\" based=\"based\" on=\"on\" {object}=\"{Object}\" entitycolumnsmap=\"entityColumnsMap\" map=\"Map\" names=\"names\" in=\"in\" format=\"format\">: [ { name: , dataType: } ] } * @param {string} baseEntityName - Name of the base entity * @returns {string} DMF type signature * @throws {Error} If entity not found in the cache */ function generateDMFTypeSignature(dmfArguments, entityColumnsMap, baseEntityName) { if(!dmfArguments || !dmfArguments.length) return \"\"; const baseEntityColumns = entityColumnsMap[baseEntityName]; if (!baseEntityColumns) { throw new Error(`Entity ${baseEntityName} not found in the cache`); } const baseEntityColumnArguments = dmfArguments .filter(param = param.type === \"COLUMN\") .map(param = { const column = baseEntityColumns.find(col = col.name === param.name); return column ? column.dataType : null; }) .join(\", \"); const baseEntityArguments = `TABLE(${baseEntityColumnArguments})`; const referencedEntityArguments = dmfArguments .filter(param = param.type === \"TABLE\") .map(entityParam = { const entityName = entityParam.name; const entityColumns = entityColumnsMap[entityName]; if (!entityColumns) { throw new Error(`Entity ${entityName} not found in the cache`); } const columnTypes = entityParam.nested .map(nestedParam = { const column = entityColumns.find(col = col.name === nestedParam.name); return column ? column.dataType : null; }) .filter(Boolean) .join(\", \"); return `TABLE(${columnTypes})`; }); const arguments = [baseEntityArguments, ...referencedEntityArguments].join(\", \"); return arguments; } / Generates DMF arguments for SQL statements * @param {string} dmfArguments - Array of DMF arguments * @returns {string} Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments(dmfArguments) { return dmfArguments .map(param = { if (param.type === \"COLUMN\") { return normalizeIdentifier(param.name); } // Handle TABLE type with nested columns return `${normalizeIdentifier(param.name)}(${ param.nested .map(nested = normalizeIdentifier(nested.name)) .join(\", \") })`; }) .join(\", \"); } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- / Validates that mandatory arguments are provided and valid * @throws {Error} If any mandatory argument is missing or invalid */ function validateMandatoryArguments() { const VALID_ACTIONS = new Set([\"ATTACH_DMF\", \"DETACH_DMF\", \"SUSPEND_DMF\", \"RESUME_DMF\", \"UPDATE_SCHEDULE\"]); const VALID_ENTITY_TYPES = new Set([\"TABLE\", \"VIEW\", \"MATERIALIZED VIEW\", \"EXTERNAL TABLE\", \"ICEBERG TABLE\"]); const DMF_OPS = new Set([\"ATTACH_DMF\", \"DETACH_DMF\", \"SUSPEND_DMF\", \"RESUME_DMF\"]); const VALID_SCHEDULE_TYPES = new Set([\"MINUTES\", \"CRON\", \"ON_DATA_CHANGE\", \"NOT_SCHEDULED\"]); const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set([\"MINUTES\", \"CRON\"]); if (!VALID_ACTIONS.has(ACTION)) throw new Error( `Invalid ACTION: \"${ACTION}\" Valid options are ${Array.from(VALID_ACTIONS).join(\", \")}` ); if (!VALID_ENTITY_TYPES.has(ENTITY_TYPE)) throw new Error( `Invalid ENTITY_TYPE: \"${ENTITY_TYPE}\" Valid options are ${Array.from(VALID_ENTITY_TYPES).join(\", \")}` ); if (DMF_OPS.has(ACTION) && !DMF_NAME) throw new Error(\"DMF_NAME is required for DMF related actions\"); if (ACTION === \"UPDATE_SCHEDULE\") { if (!SCHEDULE_TYPE) throw new Error(\"SCHEDULE_TYPE is required for SCHEDULE action\"); if (!VALID_SCHEDULE_TYPES.has(SCHEDULE_TYPE)) throw new Error( `Invalid schedule type: \"${SCHEDULE_TYPE}\"",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_5",
    "text": "Valid options are ${Array.from(VALID_SCHEDULE_TYPES).join(\", \")}` ); if (SCHEDULE_TYPES_THAT_REQUIRE_VALUE.has(SCHEDULE_TYPE) && !SCHEDULE_VALUE) throw new Error(\"SCHEDULE_VALUE is required for SCHEDULE action\"); } } / Parses a fully qualified name into its components * @param {string} fullyQualifiedName - Fully qualified name to parse * @returns {Object} Object with database, schema, and name properties * @throws {Error} If invalid fully qualified name */ function validateFullyQualifiedName(fullyQualifiedName) { const parts = fullyQualifiedName.split(\".\").map(part = part.trim()).filter(Boolean); if(parts.length !== 3) throw new Error(`Invalid fully qualified name: ${fullyQualifiedName} Expected format: database.schema.name`); } / Validates the structure of DMF arguments JSON * @param {string} rawDMFArguments - Raw JSON string of DMF arguments * @throws {Error} If DMF arguments structure is invalid */ function validateDMFArgumentsStructure(rawDMFArguments) { const parsedStructure = safelyParseJSON(rawDMFArguments); if(!parsedStructure) throw new Error( \"Invalid DMF_ARGUMENTS_JSON Expected a valid JSON string\" ); if (!Array.isArray(parsedStructure)) throw new Error(\"DMF_ARGUMENTS_JSON must be an array\"); const referencedEntities = parsedStructure.filter((param) = param.type === \"TABLE\"); if (referencedEntities.length 1) throw new Error(\"Only one referenced entity is allowed\"); const validationFunctions = { arrayItem: (param) = [\"COLUMN\", \"TABLE\"].includes(param.type) && param.name, nestedItem: (param) = [\"COLUMN\"].includes(param.type) && param.name, }; if (!parsedStructure.every(validationFunctions.arrayItem)) throw new Error(\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"); if (referencedEntities.length 0) { for(const referencedEntity of referencedEntities) { if (!Array.isArray(referencedEntity.nested) || !referencedEntity.nested.every(validationFunctions.nestedItem)) throw new Error(\"Invalid nested parameters\"); } } } / Validates that all specified columns exist in an entity * @param {Array} columnsToCheck - Array of column names to validate * @param {Array} entityColumns - Array of column metadata from the entity * @param {string} entityName - Name of the entity for error message * @throws {Error} If any column doesn't exist in the entity */ function validateColumnsExistInEntity(entityName, allColumnsInEntity, columnsToCheck) { for(const column of columnsToCheck) { if (!allColumnsInEntity.some(col = col.name === column)) throw new Error(`Column ${column} not found in entity ${entityName}`); } } / Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param {string} entityName - Fully qualified name of the entity * @param {string} dmfName - Fully qualified name of the DMF * @param {Array} dmfArguments - Array of DMF arguments * @throws {Error} If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers(entityName, dmfName = \"\", dmfArguments = []) { validateFullyQualifiedName(entityName); // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName; const baseEntityAllColumns = getAllColumnsForEntity(entityName); const entityColumnsMap = { [baseEntityName]: baseEntityAllColumns }; const allReferencedEntities = dmfArguments .filter(param = param.type === \"TABLE\"); for(const referencedEntity of allReferencedEntities) { const columns = getAllColumnsForEntity(referencedEntity.name); entityColumnsMap[referencedEntity.name] = columns; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments .filter(param = param.type === \"COLUMN\") .map(param = param.name); validateColumnsExistInEntity(baseEntityName, baseEntityAllColumns, allBaseEntityColumnsInArguments); for (const referencedEntity of allReferencedEntities) { const columnsInArguments = referencedEntity.nested.map(nestedParam = nestedParam.name); validateColumnsExistInEntity(referencedEntity.name, entityColumnsMap[referencedEntity.name], columnsInArguments); } if(dmfName) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature(dmfArguments, entityColumnsMap, baseEntityName); isDMFValid(dmfName, generatedTypeSignature); } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } / Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param {string} cronExpression - CRON expression to validate * @throws {Error} If CRON expression is invalid */ function validateCronExpression(cronExpression) { if (cronExpression.length 100) throw new Error(\"Cron expression is too long\"); const cronFields = cronExpression.trim().split(/s+/); if (cronFields.length !== 6) throw new Error(\"Invalid cron expression",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_6",
    "text": "Expected format: database.schema.name`); } / Validates the structure of DMF arguments JSON * @param {string} rawDMFArguments - Raw JSON string of DMF arguments * @throws {Error} If DMF arguments structure is invalid */ function validateDMFArgumentsStructure(rawDMFArguments) { const parsedStructure = safelyParseJSON(rawDMFArguments); if(!parsedStructure) throw new Error( \"Invalid DMF_ARGUMENTS_JSON Expected a valid JSON string\" ); if (!Array.isArray(parsedStructure)) throw new Error(\"DMF_ARGUMENTS_JSON must be an array\"); const referencedEntities = parsedStructure.filter((param) = param.type === \"TABLE\"); if (referencedEntities.length 1) throw new Error(\"Only one referenced entity is allowed\"); const validationFunctions = { arrayItem: (param) = [\"COLUMN\", \"TABLE\"].includes(param.type) && param.name, nestedItem: (param) = [\"COLUMN\"].includes(param.type) && param.name, }; if (!parsedStructure.every(validationFunctions.arrayItem)) throw new Error(\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"); if (referencedEntities.length 0) { for(const referencedEntity of referencedEntities) { if (!Array.isArray(referencedEntity.nested) || !referencedEntity.nested.every(validationFunctions.nestedItem)) throw new Error(\"Invalid nested parameters\"); } } } / Validates that all specified columns exist in an entity * @param {Array} columnsToCheck - Array of column names to validate * @param {Array} entityColumns - Array of column metadata from the entity * @param {string} entityName - Name of the entity for error message * @throws {Error} If any column doesn't exist in the entity */ function validateColumnsExistInEntity(entityName, allColumnsInEntity, columnsToCheck) { for(const column of columnsToCheck) { if (!allColumnsInEntity.some(col = col.name === column)) throw new Error(`Column ${column} not found in entity ${entityName}`); } } / Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param {string} entityName - Fully qualified name of the entity * @param {string} dmfName - Fully qualified name of the DMF * @param {Array} dmfArguments - Array of DMF arguments * @throws {Error} If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers(entityName, dmfName = \"\", dmfArguments = []) { validateFullyQualifiedName(entityName); // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName; const baseEntityAllColumns = getAllColumnsForEntity(entityName); const entityColumnsMap = { [baseEntityName]: baseEntityAllColumns }; const allReferencedEntities = dmfArguments .filter(param = param.type === \"TABLE\"); for(const referencedEntity of allReferencedEntities) { const columns = getAllColumnsForEntity(referencedEntity.name); entityColumnsMap[referencedEntity.name] = columns; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments .filter(param = param.type === \"COLUMN\") .map(param = param.name); validateColumnsExistInEntity(baseEntityName, baseEntityAllColumns, allBaseEntityColumnsInArguments); for (const referencedEntity of allReferencedEntities) { const columnsInArguments = referencedEntity.nested.map(nestedParam = nestedParam.name); validateColumnsExistInEntity(referencedEntity.name, entityColumnsMap[referencedEntity.name], columnsInArguments); } if(dmfName) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature(dmfArguments, entityColumnsMap, baseEntityName); isDMFValid(dmfName, generatedTypeSignature); } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } / Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param {string} cronExpression - CRON expression to validate * @throws {Error} If CRON expression is invalid */ function validateCronExpression(cronExpression) { if (cronExpression.length 100) throw new Error(\"Cron expression is too long\"); const cronFields = cronExpression.trim().split(/s+/); if (cronFields.length !== 6) throw new Error(\"Invalid cron expression Expected 6 fields\"); const [minute, hour, dayOfMonth, month, dayOfWeek, timezone] = cronFields; const isTimezoneValidResult = isTimezoneValid(timezone); if (!isTimezoneValidResult) throw new Error(\"Invalid timezone provided in the cron expression\"); const regexPatterns = { minute: /^(*|d+|*/d+|d+-d+|d+(,d+)*)$/, hour: /^(*|d+|*/d+|d+-d+|d+(,d+)*)$/, dayOfMonth: /^(*|L|d+|*/d+|d+-d+|d+(,d+)*)$/, month: /^(*|d+|JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC|*/d+|d+-d+|[A-Z]{3}-[A-Z]{3}|d+(,d+)*|([A-Z]{3}(,[A-Z]{3})*))$/i, dayOfWeek: /^(*|d+|SUN|MON|TUE|WED|THU|FRI|SAT|d+L|[A-Z]{3}L|*/d+|d+-d+|[A-Z]{3}-[A-Z]{3}|d+(,d+)*|([A-Z]{3}(,[A-Z]{3})*))$/i, }; if (minute.match(/^d+$/)) if (!isNumberValid(minute, 0, 59)) throw new Error(\"Invalid minute value\"); if (hour.match(/^d+$/)) if (!isNumberValid(hour, 0, 23)) throw new Error(\"Invalid hour value\"); if (dayOfMonth.match(/^d+$/)) if (!isNumberValid(dayOfMonth, 1, 31)) throw new Error(\"Invalid day of month value\"); if (month.match(/^d+$/)) if (!isNumberValid(month, 1, 12)) throw new Error(\"Invalid month value\"); if (dayOfWeek.match(/^d+$/)) if (!isNumberValid(dayOfWeek, 0, 6)) throw new Error(\"Invalid day of week value\"); if ( !regexPatterns.minute.test(minute) || !regexPatterns.hour.test(hour) || !regexPatterns.dayOfMonth.test(dayOfMonth) || !regexPatterns.month.test(month) || !regexPatterns.dayOfWeek.test(dayOfWeek) ) throw new Error(\"Invalid cron expression\"); } / Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws {Error} If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule() { const VALID_MINUTES = new Set([\"5\", \"15\", \"30\", \"60\", \"720\", \"1440\"]); if (SCHEDULE_TYPE === \"MINUTES\" && !VALID_MINUTES.has(SCHEDULE_VALUE)) throw new Error(`Invalid SCHEDULE_VALUE for MINUTES",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "configure_snowflake_data_metri_7",
    "text": "Expected a valid JSON string\" ); if (!Array.isArray(parsedStructure)) throw new Error(\"DMF_ARGUMENTS_JSON must be an array\"); const referencedEntities = parsedStructure.filter((param) = param.type === \"TABLE\"); if (referencedEntities.length 1) throw new Error(\"Only one referenced entity is allowed\"); const validationFunctions = { arrayItem: (param) = [\"COLUMN\", \"TABLE\"].includes(param.type) && param.name, nestedItem: (param) = [\"COLUMN\"].includes(param.type) && param.name, }; if (!parsedStructure.every(validationFunctions.arrayItem)) throw new Error(\"Each parameter must have a valid type(TABLE/COLUMN) and name field\"); if (referencedEntities.length 0) { for(const referencedEntity of referencedEntities) { if (!Array.isArray(referencedEntity.nested) || !referencedEntity.nested.every(validationFunctions.nestedItem)) throw new Error(\"Invalid nested parameters\"); } } } / Validates that all specified columns exist in an entity * @param {Array} columnsToCheck - Array of column names to validate * @param {Array} entityColumns - Array of column metadata from the entity * @param {string} entityName - Name of the entity for error message * @throws {Error} If any column doesn't exist in the entity */ function validateColumnsExistInEntity(entityName, allColumnsInEntity, columnsToCheck) { for(const column of columnsToCheck) { if (!allColumnsInEntity.some(col = col.name === column)) throw new Error(`Column ${column} not found in entity ${entityName}`); } } / Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param {string} entityName - Fully qualified name of the entity * @param {string} dmfName - Fully qualified name of the DMF * @param {Array} dmfArguments - Array of DMF arguments * @throws {Error} If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers(entityName, dmfName = \"\", dmfArguments = []) { validateFullyQualifiedName(entityName); // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName; const baseEntityAllColumns = getAllColumnsForEntity(entityName); const entityColumnsMap = { [baseEntityName]: baseEntityAllColumns }; const allReferencedEntities = dmfArguments .filter(param = param.type === \"TABLE\"); for(const referencedEntity of allReferencedEntities) { const columns = getAllColumnsForEntity(referencedEntity.name); entityColumnsMap[referencedEntity.name] = columns; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments .filter(param = param.type === \"COLUMN\") .map(param = param.name); validateColumnsExistInEntity(baseEntityName, baseEntityAllColumns, allBaseEntityColumnsInArguments); for (const referencedEntity of allReferencedEntities) { const columnsInArguments = referencedEntity.nested.map(nestedParam = nestedParam.name); validateColumnsExistInEntity(referencedEntity.name, entityColumnsMap[referencedEntity.name], columnsInArguments); } if(dmfName) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature(dmfArguments, entityColumnsMap, baseEntityName); isDMFValid(dmfName, generatedTypeSignature); } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } / Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param {string} cronExpression - CRON expression to validate * @throws {Error} If CRON expression is invalid */ function validateCronExpression(cronExpression) { if (cronExpression.length 100) throw new Error(\"Cron expression is too long\"); const cronFields = cronExpression.trim().split(/s+/); if (cronFields.length !== 6) throw new Error(\"Invalid cron expression Expected 6 fields\"); const [minute, hour, dayOfMonth, month, dayOfWeek, timezone] = cronFields; const isTimezoneValidResult = isTimezoneValid(timezone); if (!isTimezoneValidResult) throw new Error(\"Invalid timezone provided in the cron expression\"); const regexPatterns = { minute: /^(*|d+|*/d+|d+-d+|d+(,d+)*)$/, hour: /^(*|d+|*/d+|d+-d+|d+(,d+)*)$/, dayOfMonth: /^(*|L|d+|*/d+|d+-d+|d+(,d+)*)$/, month: /^(*|d+|JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC|*/d+|d+-d+|[A-Z]{3}-[A-Z]{3}|d+(,d+)*|([A-Z]{3}(,[A-Z]{3})*))$/i, dayOfWeek: /^(*|d+|SUN|MON|TUE|WED|THU|FRI|SAT|d+L|[A-Z]{3}L|*/d+|d+-d+|[A-Z]{3}-[A-Z]{3}|d+(,d+)*|([A-Z]{3}(,[A-Z]{3})*))$/i, }; if (minute.match(/^d+$/)) if (!isNumberValid(minute, 0, 59)) throw new Error(\"Invalid minute value\"); if (hour.match(/^d+$/)) if (!isNumberValid(hour, 0, 23)) throw new Error(\"Invalid hour value\"); if (dayOfMonth.match(/^d+$/)) if (!isNumberValid(dayOfMonth, 1, 31)) throw new Error(\"Invalid day of month value\"); if (month.match(/^d+$/)) if (!isNumberValid(month, 1, 12)) throw new Error(\"Invalid month value\"); if (dayOfWeek.match(/^d+$/)) if (!isNumberValid(dayOfWeek, 0, 6)) throw new Error(\"Invalid day of week value\"); if ( !regexPatterns.minute.test(minute) || !regexPatterns.hour.test(hour) || !regexPatterns.dayOfMonth.test(dayOfMonth) || !regexPatterns.month.test(month) || !regexPatterns.dayOfWeek.test(dayOfWeek) ) throw new Error(\"Invalid cron expression\"); } / Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws {Error} If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule() { const VALID_MINUTES = new Set([\"5\", \"15\", \"30\", \"60\", \"720\", \"1440\"]); if (SCHEDULE_TYPE === \"MINUTES\" && !VALID_MINUTES.has(SCHEDULE_VALUE)) throw new Error(`Invalid SCHEDULE_VALUE for MINUTES Valid options are ${Array.from(VALID_MINUTES).join(\", \")}`); if (SCHEDULE_TYPE === \"CRON\") validateCronExpression(SCHEDULE_VALUE); // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } / Validates all provided arguments * Performs comprehensive validation on input parameters * @throws {Error} If any validation fails */ function validateAllArguments() { validateMandatoryArguments(); // Validates all mandatory arguments are provided in the correct format if(ACTION === \"UPDATE_SCHEDULE\") validateProvidedArgumentsForSchedule(); // Validates the provided schedule type and value else { validateDMFArgumentsStructure(DMF_ARGUMENTS_JSON); } validateProvidedIdentifiers(ENTITY_NAME, DMF_NAME, safelyParseJSON(DMF_ARGUMENTS_JSON)); // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- / Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns {string} JSON string with operation status and result message */ function main(){ validateAllArguments(); // Validate all provided arguments // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments(safelyParseJSON(DMF_ARGUMENTS_JSON)); const SQL_TEMPLATES = { ATTACH_DMF: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} ADD DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments})`, DETACH_DMF: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} DROP DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments})`, SUSPEND_DMF: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) SUSPEND`, RESUME_DMF: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) RESUME`, UPDATE_SCHEDULE: { MINUTES: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = '${SCHEDULE_VALUE} MINUTE'`, CRON: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'USING CRON ${SCHEDULE_VALUE}'`, ON_DATA_CHANGE: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES'`, NOT_SCHEDULED: `ALTER ${ENTITY_TYPE} ${ENTITY_NAME} UNSET DATA_METRIC_SCHEDULE`, }, }; let sqlText = \"\"; let returnMessage = \"\"; if (ACTION === \"UPDATE_SCHEDULE\") { sqlText = SQL_TEMPLATES[ACTION][SCHEDULE_TYPE]; returnMessage = `Successfully updated schedule for ${ENTITY_NAME} to ${SCHEDULE_TYPE} ${SCHEDULE_VALUE}`; } else { sqlText = SQL_TEMPLATES[ACTION]; returnMessage = `ACTION: ${ACTION} performed successfully on ${ENTITY_NAME} with DMF: ${DMF_NAME} and DMF Arguments: ${dmfArguments}`; } const result = executeQuery(sqlText); return JSON.stringify({ isSuccessful: !result.isErrored, message: result.isErrored ? result.message : returnMessage, }); } // Execute the main function and return the result try { return main(); } catch (err) { return JSON.stringify({ isSuccessful: false, message: err.message, }); } $$; ``` ## Grant access to Atlan data quality service role ‚Äã Finally, grant permissions to the Atlan data quality service role to access the database, schema, and stored procedure you created in Snowflake: ```codeBlockLines_e6Vv USE ROLE dq_admin; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role; GRANT USAGE ON SCHEMA ATLAN_DQ.SHARED TO ROLE atlan_dq_service_role; GRANT USAGE ON PROCEDURE ATLAN_DQ.SHARED.MANAGE_DMF( STRING, STRING, STRING, STRING, STRING, STRING, STRING ) TO ROLE atlan_dq_service_role; GRANT CREATE SCHEMA ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role; ``` - Prerequisites - Create roles in Snowflake - Create a user in Snowflake - Grant privileges - Set up required objects - Grant access to Atlan data quality service role",
    "metadata": {
      "topic": "Configure Snowflake data metric functions",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
      "keywords": [
        "configure",
        "snowflake",
        "data",
        "metric",
        "functions",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_an_aws_private_network__0",
    "text": "On this page AWS PrivateLink creates a secure, private connection between services running in AWS This document describes the steps to set this up between Snowflake and Atlan, when you use our Single Tenant SaaS deployment Who can do this You will need Snowflake Support, and probably your Snowflake administrator involved - you may not have access or the tools to run these tasks. ## Prerequisites ‚Äã - Snowflake must be setup with Business Critical Edition (or higher). - Open a ticket with Snowflake Support to enable PrivateLink for your Snowflake account. - Snowflake support will take 1-2 days to review and enable PrivateLink. - If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist Please raise a support request to do so. (For all details, see the Snowflake documentation.) ## Fetch PrivateLink information ‚Äã Log in to snowCLI using the `ACCOUNTADMIN` account, and run the following commands: ```codeBlockLines_e6Vv use role accountadmin; select system$get_privatelink_config(); ``` This will produce output like the following (formatted here for readability): ```codeBlockLines_e6Vv { \"privatelink-account-name\":\"abc123.ap-south-1.privatelink\", \"privatelink-vpce-id\":\"com.amazonaws.vpce.ap-south-1.vpce-svc-257a4d536bd8e3594\", \"privatelink-account-url\":\"abc123.ap-south-1.privatelink.snowflakecomputing.com\", \"regionless-privatelink-account-url\":\"xyz789-abc123.privatelink.snowflakecomputing.com\", \"privatelink_ocsp-url\":\"ocsp.abc123.ap-south-1.privatelink.snowflakecomputing.com\", \"privatelink-connection-urls\":\"[]\" } ``` ## Share details with Atlan support team ‚Äã Share the following values with the Atlan support team: - `privatelink-account-name` - `privatelink-vpce-id` - `privatelink-account-url` - `privatelink_ocsp-url` Atlan support will finish the configuration on the Atlan side using these values Support will then provide the Snowflake PrivateLink endpoint back to you When you use this endpoint in the configuration for crawling and mining, Atlan will connect to Snowflake over the PrivateLink. - Prerequisites - Fetch PrivateLink information - Share details with Atlan support team",
    "metadata": {
      "topic": "Set up an AWS private network link to Snowflake",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-aws-private-network-link-to-snowflake",
      "keywords": [
        "set",
        "aws",
        "private",
        "network",
        "link",
        "snowflake",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "manage_snowflake_tags_0",
    "text": "On this page Note that object tagging in Snowflake currently requires Enterprise Edition or higher Atlan enables you to import your Snowflake tags, update your Snowflake assets with the imported tags, and push the tag updates back to Snowflake: - Import tags - crawl Snowflake tags from Snowflake to Atlan - Reverse sync - sync Snowflake tag updates from Atlan to Snowflake !Snowflake tag management in Atlan.png Once you've imported your Snowflake tags to Atlan: - Your Snowflake assets in Atlan will be automatically enriched with their Snowflake tags. - Imported Snowflake tags will be mapped to corresponding Atlan tags through case-insensitive name match - multiple Snowflake tags can be matched to a single tag in Atlan. - You can also attach Snowflake tags, including tag values, to your Snowflake assets in Atlan - allowing you to categorize your assets at a more granular level Atlan supports: - Allowed values: attach an allowed value from a predefined list of values imported from Snowflake. - Tag values: enter any value in Atlan while attaching or editing imported Snowflake tags on an asset. - You can enable reverse sync to push any tag updates for your Snowflake assets back to Snowflake - including allowed and tag values added to assets in Atlan. - You can filter your assets by Snowflake tags and tag and allowed values. **Did you know?** Enabling reverse sync will only update existing tags in Snowflake It will neither create nor delete any tags in Snowflake. ## Prerequisites ‚Äã **Did you know?** Additional privileges are only required when using the information schema method for fetching metadata This is because Snowflake stores all tag objects in the `ACCOUNT_USAGE` schema If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner, any permissions required will already be set. ### Account usage method ‚Äã Before you can import tags from Snowflake, you will need to do the following: - Create tags or have existing tags in Snowflake. - Grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. ### Information schema method ‚Äã Before you can import tags from Snowflake, you will need to do the following: - Create tags or have existing tags in Snowflake. - Grant additional permissions to import tags from Snowflake. - Grant additional permissions to push updated tags to Snowflake View Snowflake tags A View Snowflake tags !Begin by navigating to the governance center!Manage tags from here Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Import Snowflake tags to Atlan ‚Äã Who can do this You will need to be an admin user in Atlan to import Snowflake tags to Atlan You will also need to work with your Snowflake administrator to grant additional permissions to import tags from Snowflake - you may not have access yourself You can import your Snowflake tags to Atlan through one-way tag sync",
    "metadata": {
      "topic": "Manage Snowflake tags",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags",
      "keywords": [
        "manage",
        "snowflake",
        "tags",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "manage_snowflake_tags_1",
    "text": "You will also need to work with your Snowflake administrator to grant additional permissions to import tags from Snowflake - you may not have access yourself You can import your Snowflake tags to Atlan through one-way tag sync The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake To import Snowflake tags to Atlan, you can either: - Create a new Snowflake workflow and configure the crawler to import tags. - Modify the crawler's configuration for an existing Snowflake workflow to change _Import Tags_ to **Yes** If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags Once the crawler has completed running, tags imported from Snowflake will be available to use for tagging assets! üéâ Enable reverse sync for Snowflake tags A Enable reverse sync for Snowflake tags !Begin by navigating to the governance center!Manage tags from here Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## View Snowflake tags in Atlan ‚Äã Once you've imported your Snowflake tags, you will be able to view and manage your Snowflake tags in Atlan To view Snowflake tags: 1 From the left menu of any screen, click **Governance**. 2 Under the _Governance_ heading of the _Governance cente_r, click **Tags**. 3. (Optional) Under _Tags_, click the funnel icon to filter tags by source type Click **Snowflake** to filter for tags imported from Snowflake. 4 From the left menu under _Tags_, select a synced tag - synced tags will display the Snowflake ‚ùÑÔ∏è icon next to the tag name. 5 In the _Overview_ section, you can view a total count of synced Snowflake tags To the right of _Overview_, click **Synced tags** to view additional details - including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. 6. (Optional) Click the **Linked assets** tab to view linked assets for your Snowflake tag. 7. (Optional) In the top right, click the pencil icon to add a description and change the tag icon You cannot rename tags synced from Snowflake. ## Push tag updates to Snowflake ‚Äã Who can do this Any admin or member user in Atlan can configure reverse sync for tag updates to Snowflake You will also need to work with your Snowflake administrator to grant additional permissions to push updates - you may not have access yourself. **Did you know?** Reverse sync is currently only available for imported Snowflake tags in Atlan The imported tags will display a Snowflake ‚ùÑÔ∏è icon next to the tag name If using the account usage method, expect a data latency of up to 3 hours for reverse tag sync to be successful You can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source",
    "metadata": {
      "topic": "Manage Snowflake tags",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags",
      "keywords": [
        "manage",
        "snowflake",
        "tags",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "manage_snowflake_tags_2",
    "text": "If using the account usage method, expect a data latency of up to 3 hours for reverse tag sync to be successful You can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source Once you have enabled reverse sync, any Snowflake assets with tags updated in Atlan will also be updated in Snowflake To enable reverse sync for imported Snowflake tags: 1 From the left menu of any screen, click **Governance**. 2 Under the _Governance_ heading of the _Governance cente_r, click **Tags**. 3. (Optional) Under _Tags_, click the funnel icon to filter tags by source type Click **Snowflake** to filter for tags imported from Snowflake. 4 In the left menu under _Tags_, select a synced Snowflake tag - synced tags will display the Snowflake ‚ùÑÔ∏è icon next to the tag name. 5 On your selected tag page, to the right of _Overview_, click **Synced tags**. 6 Under _Synced tags_, in the upper right, turn on **Enable reverse sync** to synchronize tag updates from Atlan to Snowflake. 7 In the corresponding confirmation dialog, click **Yes, enable it** to enable reverse tag sync or click **Cancel** Now when you attach Snowflake tags to your Snowflake assets in Atlan, these tag updates will also be pushed to Snowflake! üéâ **Did you know?** Enabling reverse sync will **not** trigger any updates in Snowflake until synced tags are attached to Snowflake assets in Atlan For any questions about managing Snowflake tags, head over here. - Prerequisites - Import Snowflake tags to Atlan - View Snowflake tags in Atlan - Push tag updates to Snowflake",
    "metadata": {
      "topic": "Manage Snowflake tags",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags",
      "keywords": [
        "manage",
        "snowflake",
        "tags",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_google_bigquery_0",
    "text": "For views and materialized views, Atlan will send you a cost nudge before you run the preview or query the data - learn more here You must create a service account to enable Atlan to extract metadata from Google BigQuery To create a service account, you can either use: - Google Cloud console - Google Cloud CLI ## Permissions ‚Äã Atlan requires the following permissions to extract metadata from Google BigQuery. ### (Required) For metadata crawling ‚Äã For metadata crawling\") To configure permissions for crawling metadata, add the following permissions to the custom role: - `bigquery.datasets.get` enables Atlan to retrieve metadata about a dataset. - `bigquery.datasets.getIamPolicy` enables Atlan to read a dataset's IAM permissions. - `bigquery.jobs.create` enables Atlan to run jobs (including queries) within the project. danger Without this, Atlan can't query the source. - `bigquery.routines.get` enables Atlan to retrieve routine definitions and metadata. - `bigquery.routines.list` enables Atlan to list routines and metadata on routines. - `bigquery.tables.get` enables Atlan to retrieve table metadata. - `bigquery.tables.getIamPolicy` enables Atlan to read a table's IAM policy. - `bigquery.tables.list` enables Atlan to list tables and metadata on tables. - `bigquery.readsessions.create` enables Atlan to create a session to stream large results. - `bigquery.readsessions.getData` enables Atlan to retrieve data from the session. - `bigquery.readsessions.update` enables Atlan to cancel the session. - `resourcemanager.projects.get` enables Atlan to retrieve project names and metadata. ### (Optional) To add data preview and querying ‚Äã To add data preview and querying\") To configure permissions for previewing and querying data, add the following permissions to the custom role: - `bigquery.tables.getData` enables Atlan to retrieve table data. danger This permission is also required for retrieving metadata such as the row count and update time of a table. - `bigquery.jobs.get` enables Atlan to retrieve data and metadata on any job, including queries. - `bigquery.jobs.listAll` enables Atlan to list all jobs and retrieve metadata on any job submitted by any user. - `bigquery.jobs.update` enables Atlan to cancel any job, including a running query. ### (Optional) To add query history mining ‚Äã To add query history mining\") To configure permissions for mining query history, add the following permissions to the custom role: - `bigquery.jobs.listAll` enables Atlan to fetch all queries for a project. - `bigquery.jobs.get` enables Atlan to access query text for queries. ### (Optional) To crawl tags ‚Äã To crawl tags\") To configure permissions for crawling Google BigQuery tags and policy tags, add the following permissions to the custom role: - `resourcemanager.tagKeys.list` enables Atlan to fetch all tag keys. - `resourcemanager.tagValues.list` enables Atlan to fetch all tag values for tag keys. - `datacatalog.taxonomies.list` enables Atlan to fetch all policy tag taxonomies. - `datacatalog.taxonomies.get` enables Atlan to fetch all policy tag taxonomies. ## Google Cloud console ‚Äã ### Create a custom role ‚Äã You will need to create a custom role in the Google Cloud console for integration with Atlan To create a custom role: 1 Open the Google Cloud console. 2 From the left menu under _IAM and admin_, click **Roles**. 3",
    "metadata": {
      "topic": "Set up Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery",
      "keywords": [
        "set",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_google_bigquery_1",
    "text": "Open the Google Cloud console. 2 From the left menu under _IAM and admin_, click **Roles**. 3 Using the dropdown list at the top of the page, select the project in which you want to create a role. 4 From the upper left of the _Roles_ page, click **Create Role**. 5 In the _Create role_ page, enter the following details: 1 For _Title_, enter a meaningful name for the custom role - for example, `Atlan User Role`. 2. (Optional) For _Description_, enter a description for the custom role. 3 For _ID_, the Google Cloud console generates a custom role ID based on the custom role name Edit the ID if necessary - the ID cannot be changed later. 4. (Optional) For _Role launch stage_, assign a stage for the custom role - for example, _Alpha_, _Beta_, or _General Availability_. 5 Click **Add permissions** to select the permissions you want to include in the custom role In the _Add permissions_ dialog, click the **Enter property name or value** filter and add the required and any optional permissions. 6 Click **Create** to finish custom role setup Once you have created a custom role, you will need to create a service account and add your custom role to it To create a service account: 1 Open the Google Cloud console. 2 From the left menu under _IAM and admin_, click **Service accounts**. 3 Select a Google Cloud project. 4 From the upper left of the _Service accounts_ page, click **Create Service Account**. 5 For _Service account details_, enter the following details: 1 For _Service account name_, enter a service account name to display in the Google Cloud console. 2 For _Service account ID_, the Google Cloud console generates a service account ID based on this name Edit the ID if necessary - the ID cannot be changed later. 3. (Optional) For _Service account description_, enter a description for the service account. 4 Click **Create and continue** to proceed to the next step. 6 For _Grant this service account access to the project_, enter the following details: 1 Click the **Select a role** dropdown and then select the custom role you created in the previous step - for example, `Atlan User Role`. 2 Click **Continue** to proceed to the next step. 7 Click **Done** to finish the service account setup. ### Create service account key ‚Äã Once you have created a service account, you will need to create a service account key for crawling Google BigQuery To create a service account key: 1 Open the Google Cloud console. 2 From the left menu under _IAM and admin_, click **Service accounts**. 3 Select the Google Cloud project for which you created the service account. 4 On the _Service accounts_ page, click the email address of the service account that you want to create a key for. 5 From the upper left of your service account page, click the **Keys** tab. 6",
    "metadata": {
      "topic": "Set up Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery",
      "keywords": [
        "set",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_google_bigquery_2",
    "text": "On the _Service accounts_ page, click the email address of the service account that you want to create a key for. 5 From the upper left of your service account page, click the **Keys** tab. 6 On the _Keys_ page, click the **Add Key** dropdown and then click **Create new key**. 7 In the _Create private key_ dialog, for _Key type_, click **JSON** and then click **Create** This will create a service account key file Download the key file and store it in a secure location - you will not be able to download it again. ## Google Cloud CLI ‚Äã ### Prerequisites ‚Äã You will need to set up the Google Cloud CLI in any one of the following development environments: - **Cloud Shell** - to use an online terminal with the gcloud CLI already set up, activate Cloud Shell: - To launch a Cloud Shell session from the Google Cloud console, open the Google Cloud console, and from the top right, click the **Activate Cloud Shell** icon. - A Cloud Shell session will start and display a command-line prompt It can take a few seconds for the session to initialize. - **Local shell** - to use a local development environment, install and initialize the gcloud CLI. ### Create a custom role ‚Äã To create a custom role with the requisite and any optional permissions, run the following command: ```codeBlockLines_e6Vv gcloud iam roles create atlanUserRole --project=<project_id> --title=\"Atlan User Role\" --description=\"Atlan User Role to extract metadata\" --permissions=\"bigquery.datasets.get,bigquery.datasets.getIamPolicy,bigquery.jobs.create,bigquery.readsessions.create,bigquery.readsessions.getData,bigquery.readsessions.update,bigquery.routines.get,bigquery.routines.list,bigquery.tables.get,bigquery.tables.getIamPolicy,bigquery.tables.list,resourcemanager.projects.get\" --stage=ALPHA ``` - Replace `<project_id>` with the project ID of your Google Cloud project. ### Create a service account ‚Äã To create a service account, run the following command: ```codeBlockLines_e6Vv gcloud iam service-accounts create atlanUser --description=\"Atlan Service Account to extract metadata\" --display-name=\"Atlan User\" ``` To add your custom role to your service account, run the following command: ```codeBlockLines_e6Vv gcloud projects add-iam-policy-binding <project_id> --member=\"serviceAccount:atlanUser@<project_id>.iam.gserviceaccount.com\" --role=\"atlanUserRole\" ``` - Replace `<project_id>` with the project ID of your Google Cloud project. ### Create a service account key ‚Äã To create a service account key, run the following command: ```codeBlockLines_e6Vv gcloud iam service-accounts keys create <key_file_path> --iam-account=atlanUser@<project_id>.iam.gserviceaccount.com\" ``` - Replace `<key_file_path>` with path to a new output file for the private key - for example, `~/atlanUser-private-key.json`. - Replace `<project_id>` with the project ID of your Google Cloud project. danger Due to limitations at source, Atlan currently does not support generating lineage using the `bq cp` commands - for example, `bq cp <source-table> <destination-table>`. - Permissions - Google Cloud console - Google Cloud CLI",
    "metadata": {
      "topic": "Set up Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery",
      "keywords": [
        "set",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_google_bigquer_0",
    "text": "On this page #### Does Atlan support nested columns beyond level 1? ‚Äã Atlan gets the raw structure for nested columns beyond level 1 from JSON files, which are then parsed and rendered on the table asset sidebar However, nested columns beyond level 1 can only be viewed and not enriched with metadata For example, if you attach a tag to a level 1 column, the tag will not be propagated to the nested columns. #### What are the known limitations of generating lineage for nested columns? ‚Äã The following examples illustrate the known limitations of generating lineage for Google BigQuery nested columns in Atlan: - Lineage will not be generated if you use the nested column of a `RECORD` in the source table or view to create another nested column for a `RECORD REPEATED` column in the target table or view For example: ```codeBlockLines_e6Vv CREATE VIEW orders_view AS SELECT ARRAY(SELECT AS STRUCT customer.name AS customer_name, order_id AS order_id ) AS orders FROM orders_raw; ``` - Lineage will be generated for: `orders_raw.order_id` ‚Üí `orders_view.orders.order_id` - Lineage will not be generated for: `orders_raw.customer.name` ‚Üí `orders_view.orders.customer_name` - Lineage will not be generated for a nested column if you use the `RECORD` column as well as a level 1 nested column in the source table or view to create a target column in another table or view For example: ```codeBlockLines_e6Vv CREATE VIEW orders_view AS SELECT customer AS customer; customer.name AS customer_name, order_id AS order_id FROM orders_raw; ``` - Lineage will be generated for: - `orders_raw.customer` ‚Üí `orders_view.customer` - `orders_raw.customer.address.city` ‚Üí `orders_view.city` - Lineage will not be generated for: `orders_raw.customer.name` ‚Üí `orders_view.customer_name` Instead, lineage will be generated between `orders_raw.customer` ‚Üí `orders_view.customer_name`. - Lineage will not be generated for a nested column if you use a table alias to refer to nested columns in a source table or view to create a target column in another table or view For example: ```codeBlockLines_e6Vv CREATE VIEW orders_view AS SELECT o.order_id AS order_id o.customer.name as customer_name o.customer.address.city as city FROM orders_raw o; ``` - Lineage will be generated for: `orders_raw.order_id` ‚Üí `orders_view.order_id` - Lineage will not be generated for: - `orders_raw.customer.name` ‚Üí `orders_view.customer_name` - `orders_raw.customer.address.city` ‚Üí `orders_view.city` - Lineage will not be generated for a nested column if you use a source table or view `CROSS JOIN` with `UNNEST` to create a target table or view",
    "metadata": {
      "topic": "Troubleshooting Google BigQuery connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/troubleshooting/troubleshooting-google-bigquery-connectivity",
      "keywords": [
        "troubleshooting",
        "google",
        "bigquery",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_google_bigquer_1",
    "text": "Instead, lineage will be generated between `orders_raw.customer` ‚Üí `orders_view.customer_name`. - Lineage will not be generated for a nested column if you use a table alias to refer to nested columns in a source table or view to create a target column in another table or view For example: ```codeBlockLines_e6Vv CREATE VIEW orders_view AS SELECT o.order_id AS order_id o.customer.name as customer_name o.customer.address.city as city FROM orders_raw o; ``` - Lineage will be generated for: `orders_raw.order_id` ‚Üí `orders_view.order_id` - Lineage will not be generated for: - `orders_raw.customer.name` ‚Üí `orders_view.customer_name` - `orders_raw.customer.address.city` ‚Üí `orders_view.city` - Lineage will not be generated for a nested column if you use a source table or view `CROSS JOIN` with `UNNEST` to create a target table or view For example: ```codeBlockLines_e6Vv CREATE VIEW orders_view AS SELECT order_id, customer.name as customer_name, customer.address.city as city, i.item_name AS item_name FROM orders_raw CROSS JOIN UNNEST(items) AS i; ``` - Lineage will be generated for: `orders_raw.order_id` ‚Üí `orders_view.order_id` - Lineage will not be generated for: - `orders_raw.customer.name` ‚Üí `orders_view.customer_name` - `orders_raw.customer.address.city` ‚Üí `orders_view.city` - `orders_raw.items.item_name` ‚Üí `orders_view.item_name` #### How to debug test authentication and preflight check errors? ‚Äã **Invalid project ID** `Provided GCP project ID is invalid, please check and try again.` - Ensure that the project ID is non-empty and matches the expected project in your Google Cloud console. **Invalid service account JSON** Following are the possible error messages for this issue: `Provided Service account JSON is invalid, please check and try again.` `Private key in the service account JSON is invalid, please check and try again.` `Failed to sign service account access token request with the provided private key Check the service account JSON and try again.` `Malformed JSON, please check and try again.` - These indicate issues with the service account JSON key, possibly invalid or malformed data, or incorrect private key or formatting. - Verify that the service account JSON is correctly formatted, and the private key is correctly specified. - Regenerate the service account key if needed, and ensure that all required fields are included. - Ensure that the file is not corrupted and follows a proper JSON structure. **Insufficient permissions** `Service account doesn't have permission to create jobs, please ensure that the service account has the 'bigquery.jobs.create' permission.` - Ensure that you have assigned the `bigquery.jobs.create` permission to the service account. - Review the roles and permissions assigned to the service account in your Google Cloud IAM settings. **Cloud Resource Manager API disabled** `Cloud Resource Manager API has not been used in the configured project before or it is disabled",
    "metadata": {
      "topic": "Troubleshooting Google BigQuery connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/troubleshooting/troubleshooting-google-bigquery-connectivity",
      "keywords": [
        "troubleshooting",
        "google",
        "bigquery",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_google_bigquer_2",
    "text": "For example: ```codeBlockLines_e6Vv CREATE VIEW orders_view AS SELECT order_id, customer.name as customer_name, customer.address.city as city, i.item_name AS item_name FROM orders_raw CROSS JOIN UNNEST(items) AS i; ``` - Lineage will be generated for: `orders_raw.order_id` ‚Üí `orders_view.order_id` - Lineage will not be generated for: - `orders_raw.customer.name` ‚Üí `orders_view.customer_name` - `orders_raw.customer.address.city` ‚Üí `orders_view.city` - `orders_raw.items.item_name` ‚Üí `orders_view.item_name` #### How to debug test authentication and preflight check errors? ‚Äã **Invalid project ID** `Provided GCP project ID is invalid, please check and try again.` - Ensure that the project ID is non-empty and matches the expected project in your Google Cloud console. **Invalid service account JSON** Following are the possible error messages for this issue: `Provided Service account JSON is invalid, please check and try again.` `Private key in the service account JSON is invalid, please check and try again.` `Failed to sign service account access token request with the provided private key Check the service account JSON and try again.` `Malformed JSON, please check and try again.` - These indicate issues with the service account JSON key, possibly invalid or malformed data, or incorrect private key or formatting. - Verify that the service account JSON is correctly formatted, and the private key is correctly specified. - Regenerate the service account key if needed, and ensure that all required fields are included. - Ensure that the file is not corrupted and follows a proper JSON structure. **Insufficient permissions** `Service account doesn't have permission to create jobs, please ensure that the service account has the 'bigquery.jobs.create' permission.` - Ensure that you have assigned the `bigquery.jobs.create` permission to the service account. - Review the roles and permissions assigned to the service account in your Google Cloud IAM settings. **Cloud Resource Manager API disabled** `Cloud Resource Manager API has not been used in the configured project before or it is disabled Please enable it and try again after some time.` - Open your Google Cloud console and enable the Cloud Resource Manager API for the project. - Wait for the API to be fully activated before retrying the operation. **Invalid grant, service account not found** `Unable to get access token for the provided service account, ensure the service account is active and try again.` - The service account is either inactive or does not exist. - Ensure that the service account you created still exists in your Google Cloud console and has neither been disabled nor deleted. **General connection failure** Following are the possible error messages for this issue: `Unable to connect to the configured BigQuery instance, please check your credentials and configs and then try again.` `Cannot create poolable connection factory.` - These indicate a general connection failure to your Google BigQuery instance, possibly due to misconfigured credentials or network issues. - Verify that your credentials are correctly configured. - Ensure that there are no network issues blocking the connection. - If the problem still persists after verifying all of the above, contact Atlan support",
    "metadata": {
      "topic": "Troubleshooting Google BigQuery connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/troubleshooting/troubleshooting-google-bigquery-connectivity",
      "keywords": [
        "troubleshooting",
        "google",
        "bigquery",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "mine_google_bigquery_0",
    "text": "On this page Mine BigQuery A Mine BigQuery !Begin by adding a new workflow!Switch to the miner packages to find the miners more easily Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms Once you have crawled assets from Google BigQuery, you can mine its query history to construct lineage To mine lineage from Google BigQuery, review the order of operations and then complete the following steps. ## Select the miner ‚Äã To select the Google BigQuery miner: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the filters along the top, click **Miner**. 3 From the list of packages, select **BigQuery Miner** and then click **Setup Workflow**. ## Configure the miner ‚Äã To configure the Google BigQuery miner: 1 For _Connection_, select the connection to mine. (To select a connection, the crawler must have already run.) 2 For _Miner Extraction Method_, select **Query History**. 3 For _Start time_, choose the earliest date from which to mine query history. info üí™ **Did you know?** The miner restricts you to only querying the past two weeks of query history If you need to query more history, for example in an initial load, consider using the S3 miner first After the initial load, you can modify the miner's configuration to use query history extraction. 4. (Optional) By default, the miner fetches data from the US region To fetch data from another region, for _Region_, select **Custom** and then enter the region where your `INFORMATION_SCHEMA` is hosted under _Custom BigQuery Region_ Enter the region in the following format `region-<REGION>`, replacing `<REGION>` with your specific region - for example, `europe-north1`. 5 To check for any permissions or other configuration issues before running the miner, click **Preflight checks**. 6 At the bottom of the screen, click **Next** to proceed. danger If running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history Mining two weeks of query history on the first miner run may cause delays Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session Learn more about the miner logic here. ## Configure the miner behavior ‚Äã To configure the Google BigQuery miner behavior: 1. (Optional) For _Calculate popularity_, change to **True** to retrieve usage and popularity metrics for your Google BigQuery assets from query history: 1 To select a pricing model for running queries, for _Pricing Model_, click **On Demand** to be charged for the number of bytes processed or **Flat Rate** for the number of slots purchased. 2 For _Popularity Window (days)_, 30 days is the maximum limit You can set a shorter popularity window of less than 30 days. 3 For _Excluded Users_, type the names of users to be excluded while calculating usage metrics for Google BigQuery assets",
    "metadata": {
      "topic": "Mine Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/mine-google-bigquery",
      "keywords": [
        "mine",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "mine_google_bigquery_1",
    "text": "You can set a shorter popularity window of less than 30 days. 3 For _Excluded Users_, type the names of users to be excluded while calculating usage metrics for Google BigQuery assets Press `enter` after each name to add more names. 2. (Optional) For _Control Config_, click **Custom** to configure the following: 1 For _Fetch excluded project's QUERY_HISTORY_, click **Yes** to mine query history from databases or projects excluded while crawling metadata from Google BigQuery. 2 If Atlan support has provided you with a custom control configuration, enter the configuration into the _Custom Config_ box You can also: - (Optional) Enter `{‚Äúignore-all-case‚Äù: true}` to enable crawling assets with case-sensitive identifiers. ## Run the miner ‚Äã To run the Google BigQuery miner, after completing the steps above: - To run the miner once immediately, at the bottom of the screen, click the **Run** button. - To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule & Run** button Once the miner has completed running, you will see lineage for Google BigQuery assets that were created in Google BigQuery between the start time and when the miner ran! üéâ - Select the miner - Configure the miner - Configure the miner behavior - Run the miner",
    "metadata": {
      "topic": "Mine Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/mine-google-bigquery",
      "keywords": [
        "mine",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_goo_0",
    "text": "On this page Once you have crawled Google BigQuery, you can use connector-specific filters for quick asset discovery The following filters are currently supported for these assets: - Tables - BigQuery labels and Is sharded filters Atlan doesn't run any table scans Atlan leverages the table preview options from Google BigQuery that enable you to view data for free and without affecting any quotas using the `tabledata.list` API Hence, table asset previews in Atlan are already cost-optimized However, this doesn't apply to views and materialized views For Google BigQuery views and materialized views, Atlan sends you a cost nudge before viewing a sample data preview This informs you about the precise bytes that are spent during the execution of the query, helping you decide if you still want to run the preview. **Did you know?** You also receive a cost nudge before querying your Google BigQuery assets View BigQuery asset preview cost A View BigQuery asset preview cost !View sample data preview for your Google BigQuery view!Cancel your request or proceed with it In this example, we'll run the asset preview Illustrative example reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms Atlan crawls and maps the following assets and properties from Google BigQuery. ## Databases ‚Äã Atlan maps projects from Google BigQuery to its `Database` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `Project ID` | `name` | asset preview, profile, and filter, overview sidebar | ## Schemas ‚Äã Atlan maps datasets from Google BigQuery to its `Schema` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_SCHEMA` | `name` | asset preview and profile, overview sidebar | | `TABLE_COUNT` | `tableCount` | asset preview and profile | | `VIEW_COUNT` | `viewsCount` | asset preview and profile, overview sidebar | | `TABLE_CATALOG` | `databaseName` | asset preview | | `REMARKS` | `description` | asset preview and profile, overview sidebar | | `CREATED` | `sourceCreatedAt` | asset profile and properties sidebar | | `MODIFIED` | `sourceUpdatedAt` | asset profile and properties sidebar | ## Tables ‚Äã **Did you know?** Table asset previews are already cost-optimized Google BigQuery enables you to use the table preview options to view data for free and without affecting any quotas Note that this isn't currently supported for Google BigQuery views and materialized views in Atlan",
    "metadata": {
      "topic": "What does Atlan crawl from Google BigQuery?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_goo_1",
    "text": "Google BigQuery enables you to use the table preview options to view data for free and without affecting any quotas Note that this isn't currently supported for Google BigQuery views and materialized views in Atlan Atlan maps tables from Google BigQuery to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_NAME` | `name` | asset preview and profile, overview sidebar | | `REMARKS` | `description` | asset preview and profile, overview sidebar | | `COLUMN_COUNT` | `columnCount` | asset preview and profile, overview sidebar | | `ROW_COUNT` | `rowCount` | asset preview, profile, and filter, overview sidebar | | `SIZE_BYTES` | `sizeBytes` | asset filter and overview sidebar | | `TABLE_TYPE` | `subType` | asset preview and profile | | `LABELS` | `assetTags` | overview sidebar | | `CREATED` | `sourceCreatedAt` | asset profile and properties sidebar | | `MODIFIED` | `sourceUpdatedAt` | asset profile and properties sidebar | | `OPTION_NAMES (require_partition_filter)` | `isPartitioned` | API only | ## Views ‚Äã Atlan maps views from Google BigQuery to its `View` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_NAME` | `name` | asset preview and profile, overview sidebar | | `REMARKS` | `description` | asset preview and profile, overview sidebar | | `COLUMN_COUNT` | `columnCount` | asset preview and profile, overview sidebar | | `TABLE_TYPE` | `subType` | asset preview and profile | | `CREATED` | `sourceCreatedAt` | asset profile and properties sidebar | | `MODIFIED` | `sourceUpdatedAt` | asset profile and properties sidebar | | `OPTION_NAMES (require_partition_filter)` | `isPartitioned` | API only | | `DDL` | `definition` | asset profile and overview sidebar | ## Materialized views ‚Äã Atlan maps materialized views from Google BigQuery to its `MaterialisedView` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_NAME` | `name` | asset preview and profile, overview sidebar | | `REMARKS` | `description` | asset preview and profile, overview sidebar | | `COLUMN_COUNT` | `columnCount` | asset preview and profile, overview sidebar | | `ROW_COUNT` | `rowCount` | asset preview, profile, and filter, overview sidebar | | `SIZE_BYTES` | `sizeBytes` | asset filter and overview sidebar | | `TABLE_TYPE` | `subType` | asset preview and profile | | `CREATED` | `sourceCreatedAt` | asset profile and properties sidebar | | `MODIFIED` | `sourceUpdatedAt` | asset profile and properties sidebar | | `OPTION_NAMES (require_partition_filter)` | `isPartitioned` | API only | | `DDL` | `definition` | asset profile and overview sidebar | ## Columns ‚Äã Atlan supports nested columns up to level 1 for Google BigQuery to help you enrich your semi-structured data types You can view nested columns in the asset sidebar for your table assets Atlan maps columns from Google BigQuery to its `Column` asset type",
    "metadata": {
      "topic": "What does Atlan crawl from Google BigQuery?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_goo_2",
    "text": "You can view nested columns in the asset sidebar for your table assets Atlan maps columns from Google BigQuery to its `Column` asset type Important Atlan doesn't crawl primary key (PK) and foreign key (FK) information from Google BigQuery. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `COLUMN_NAME` | `name` | asset preview and profile, overview sidebar | | `REMARKS, DESCRIPTION` | `description` | asset preview and profile, overview sidebar | | `ORDINAL_POSITION` | `order` | asset profile | | `TYPE_NAME` | `dataType` | asset preview, profile, and filter, overview sidebar | | `IS_NULLABLE` | `isNullable` | API only | | `IS_PARTITIONING_COLUMN` | `isPartition` | asset preview, profile, and filter | | `CLUSTERING_COLUMN_LIST` | `isClustered` | asset preview, profile, and filter | ## Stored procedures ‚Äã Atlan maps stored procedures from Google BigQuery to its `Procedure` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `PROCEDURE_NAME` | `name` | API only | | `REMARKS` | `description` | API only | | `PROCEDURE_TYPE` | `subType` | API only | | `ROUTINE_DEFINITION` | `definition` | API only | | `CREATED` | `sourceCreatedAt` | API only | | `MODIFIED` | `sourceUpdatedAt` | API only | - Databases - Schemas - Tables - Views - Materialized views - Columns - Stored procedures",
    "metadata": {
      "topic": "What does Atlan crawl from Google BigQuery?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source"
      ]
    }
  },
  {
    "id": "preflight_checks_for_google_bi_0",
    "text": "On this page Before running the Google BigQuery crawler, you can run preflight checks to perform the necessary technical validations. **Did you know?** All Google BigQuery resources expose the `testIamPermissions()` method, which is used for permission testing in Atlan through REST API The following preflight checks will be completed: ## Authorization ‚Äã Each request requires an OAuth 2.0 access token generated via the service account key. ## Assets ‚Äã ### Metadata crawling permission ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed Not all permission granted Missing permissions:` ### Query and review permission ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed Not all permission granted Missing permissions:` ## Miner ‚Äã **Did you know?** Once you have crawled assets from Google BigQuery, you can mine query history. ### Miner policy ‚Äã #### Query history ‚Äã ‚úÖ `Check successful` ‚ùå `Check failed Not all permission granted Missing permissions:` #### S3 ‚Äã ‚úÖ `Check successful` if the bucket, region, and prefix combination is valid and the S3 credential passed is accessible. ‚ùå `Check failed with error code <AWS error code> - <AWS SDK ERR message>` For example: `Miner S3 credentials: failed with error code: NoSuchBucket` ### Crawler workflow ‚Äã This checks if the selected connection exists in Atlan. ‚úÖ `Check successful` ‚ùå `Check failed Connection does not exist.` / `Check failed Workflow artifacts are missing Please run the crawler workflow again.` - Authorization - Assets - Miner",
    "metadata": {
      "topic": "Preflight checks for Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/references/preflight-checks-for-google-bigquery",
      "keywords": [
        "preflight",
        "checks",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_google_bigquery_0",
    "text": "On this page Once you have configured the Google BigQuery user permissions, you can establish a connection between Atlan and Google BigQuery To crawl metadata from Google BigQuery, review the Crawl Google BigQuery A Crawl Google BigQuery !Begin by adding a new workflow!The BigQuery Assets package allows us to crawl metadata from Google BigQuery Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms order of operations and then complete the following steps. ## Select the source ‚Äã To select Google BigQuery as your source: 1 In the top right corner of any screen, click **New** and then click **New Workflow**. 2 From the list of packages, select **BigQuery Assets** and click **Setup Workflow**. ## Provide credentials ‚Äã To enter your Google BigQuery credentials: 1 For _Authentication_, _Service Account_ is the default selection. 2 For _Connectivity_, choose how you want Atlan to connect to Google BigQuery: - To connect using a public endpoint from Google, click **Public Network**. - To connect through a private endpoint, click **Private Network Link** Next, contact Atlan support to request the DNS name of the Private Service Connect endpoint that Atlan created for the integration: 1 For _Host_, enter the DNS name of the Private Service Connect endpoint received from Atlan in the following format - `https://bigquery-<privateserver>.p.googleapis.com` Replace `<privateserver>` with the DNS name. 2 For _Port_, _443_ is the default selection. 3 For _Project Id_, enter the value of `project_id` from the JSON for the service account you created This project ID is only used to authenticate the connection You can configure the crawler to extract more than just the specified project. 4 For _Service Account Json_, paste in the entire JSON for the service account you created. 5 For _Service Account Email_, enter the value of `client_email` from the JSON for the service account you created. 6 At the bottom of the form, click the **Test Authentication** button to confirm connectivity to Google BigQuery using these details. 7 When successful, at the bottom of the screen click the **Next** button. ## Configure the connection ‚Äã To complete the Google BigQuery connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might want to use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. 3. (Optional) To prevent users from querying any Google BigQuery data, change _Allow SQL Query_ to **No**. 4. (Optional) To prevent users from previewing any Google BigQuery data, change _Allow Data Preview_ to **No**. 5 At the bottom of the screen, click the **Next** button to proceed. ## Configure the crawler ‚Äã Before running the Google BigQuery crawler, you can further configure it",
    "metadata": {
      "topic": "Crawl Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery",
      "keywords": [
        "crawl",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_google_bigquery_1",
    "text": "For example, you might want to use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. 3. (Optional) To prevent users from querying any Google BigQuery data, change _Allow SQL Query_ to **No**. 4. (Optional) To prevent users from previewing any Google BigQuery data, change _Allow Data Preview_ to **No**. 5 At the bottom of the screen, click the **Next** button to proceed. ## Configure the crawler ‚Äã Before running the Google BigQuery crawler, you can further configure it You can override the defaults for any of these options: - For _Filter Sharded Tables_, keep _No_ for the default configuration or click **Yes** to enable Atlan to catalog and display sharded tables with the same naming prefix as a single table in asset discovery and the lineage graph. - Select assets you want to include in crawling in the _Include Metadata_ field. (This will default to all assets, if none are specified.) - Select assets you want to exclude from crawling in the _Exclude Metadata_ field. (This will default to no assets, if none are specified.) - To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the _Exclude regex for tables & views_ field. - To import existing tags from Google BigQuery to Atlan, for _Import Tags_, click **Yes**. - For _Advanced Config_, keep _Default_ for the default configuration or click **Custom** if Atlan support has provided you with a custom control configuration. - Enter the configuration into the _Custom Config_ box You can also enter `{‚Äúignore-all-case‚Äù: true}` to enable crawling assets with case-sensitive identifiers. - For _Hidden Assets_, keep _No_ for the default configuration or click **Yes** to crawl metadata from your hidden datasets in Google BigQuery. **Did you know?** If a folder or project appears in both the include and exclude filters, the exclude filter takes precedence. ## Run the crawler ‚Äã To run the Google BigQuery crawler, after completing the steps above: 1 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. 2 You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button Once the crawler has completed running, you will see the assets in Atlan's asset page! üéâ - Select the source - Provide credentials - Configure the connection - Configure the crawler - Run the crawler",
    "metadata": {
      "topic": "Crawl Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery",
      "keywords": [
        "crawl",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "enable_sso_for_google_bigquery_0",
    "text": "On this page Atlan supports SSO authentication for Google BigQuery connections Once you've configured SSO authentication for Google BigQuery, your users can: - Query data with SSO credentials - View sample data with SSO credentials **Did you know?** When using OAuth 2.0 for authorization, Google displays a consent screen to the user that includes a summary of your project, policies, and scopes If you have not configured the consent screen, complete the steps in configure OAuth consent screen Otherwise, skip to create access credentials. ## (Optional) Configure OAuth consent screen in Google BigQuery ‚Äã Configure OAuth consent screen in Google BigQuery\") Who can do this You will need your Google BigQuery administrator to complete these steps - you may not have access yourself To configure the OAuth consent screen, from Google BigQuery: 1 Open the Google Cloud console. 2 In the left menu of the _Google Cloud_ console, under _APIs & Services_, click **OAuth consent screen**. 3 On the _OAuth consent screen_ page, under _User Type_, select a preferred user type and then click **Create**. 4 In the corresponding _Edit app registration_ page, enter the following details: 1 For _App name_, enter a meaningful name - for example, `Atlan_SSO`. 2 For _User support email_, enter a support email for your users to troubleshoot. 3 For _Developer contact information_, enter an email address where Google can notify you about any changes to your project. 4 Click **Save and continue** to proceed to the next step. 5 On the _Scopes_ page, complete the following steps: 1 Click **Add or remove scopes** to add a new scope. 2 In the _Update selected scopes_ dialog, click **BigQuery API** to add the `/auth/bigquery` scope and then click **Update**. 3 Click **Save and continue** to finish setup. 6 Once the OAuth consent screen configuration is successful, click **Go back to dashboard** Configure SSO for BigQuery A Configure SSO for BigQuery !Begin by navigating to your assets!Select **BigQuery** as the connector type Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Create access credentials in Google BigQuery ‚Äã Who can do this You will need your Google BigQuery administrator to complete these steps - you may not have access yourself Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan To create access credentials, from Google BigQuery: 1 Open the Google Cloud console. 2 In the left menu of the _Google Cloud_ console, under _APIs & Services_, click **Credentials**. 3 From the upper right of the _Credentials_ page, click **Create credentials**, and from the dropdown, click **OAuth client ID**. 4 In the _OAuth client ID_ screen, enter the following details: 1 For _Application type_, click **Web application**. 2 For _Name_, enter a meaningful name - for example, `Atlan_client`. 3 Under _Authorized JavaScript origins_, click **Add URI** and enter your Atlan instance - for example, `https://<company-name>.atlan.com`. 4 Under _Authorized redirect URIs_, click **Add URI** and enter your Atlan endpoint URI - for example, `https://<company-name>.atlan.com/api/service/oauth`. 5",
    "metadata": {
      "topic": "Enable  SSO for Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/enable-sso-for-google-bigquery",
      "keywords": [
        "enable",
        "sso",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "enable_sso_for_google_bigquery_1",
    "text": "Under _Authorized JavaScript origins_, click **Add URI** and enter your Atlan instance - for example, `https://<company-name>.atlan.com`. 4 Under _Authorized redirect URIs_, click **Add URI** and enter your Atlan endpoint URI - for example, `https://<company-name>.atlan.com/api/service/oauth`. 5 Click **Create** to finish setup. 5 From the corresponding _OAuth client created_ dialog, copy the _Client ID_ and _Client secret_ and store it in a secure location. ## Configure SSO authentication in Atlan ‚Äã Who can do this You will need to be a connection admin in Atlan to complete these steps You will also need inputs and approval from your Google BigQuery administrator Once you have configured access credentials in Google BigQuery, you can enable SSO authentication for your users to query data and view sample data in Atlan To configure SSO on a Google BigQuery connection, from Atlan: 1 From the left menu of any screen, click **Assets**. 2 From the _Assets_ page, click the **Connector** filter, and from the dropdown, select **BigQuery**. 3 From the pills below the search bar at the top of the screen, click **Connection**. 4 From the list of results, select a Google BigQuery connection to enable SSO authentication. 5 From the sidebar on the right, next to _Connection settings_, click **Edit**. 6 In the _Connection settings_ dialog: - Under _Allow query_, for _Authentication type_, click **SSO authentication** to enforce SSO credentials for querying data: - For _SSO authentication_, enter the following details: 1 For _Client ID_, enter the client ID you copied from Google BigQuery. 2 For _Client secret_, enter the client secret you copied from Google BigQuery. - Under _Display sample data_, for _Source preview_, click **SSO authentication** to enforce SSO credentials for viewing sample data: - If SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data. - If a different authentication method is enabled for querying data, enter the client ID and client secret you copied from Google BigQuery. 7. (Optional) Toggle on **Enable data policies created at source to apply for querying in Atlan** to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. 8 At the bottom right of the _Connection settings_ dialog, click **Update** Your users will now be able to run queries and view sample data using their SSO credentials! üéâ - (Optional) Configure OAuth consent screen in Google BigQuery - Create access credentials in Google BigQuery - Configure SSO authentication in Atlan",
    "metadata": {
      "topic": "Enable  SSO for Google BigQuery",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/enable-sso-for-google-bigquery",
      "keywords": [
        "enable",
        "sso",
        "google",
        "bigquery",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "manage_google_bigquery_tags_0",
    "text": "On this page Atlan imports your Google BigQuery tags and allows you to update your Google BigQuery assets with the imported tags Note that object tagging in Google BigQuery currently requires Enterprise edition or higher Once you've crawled Google BigQuery: - Your Google BigQuery assets in Atlan will be automatically enriched with their Google BigQuery tags. - Imported Google BigQuery tags will be mapped to corresponding Atlan tags through case-insensitive name match - multiple Google BigQuery tags can be matched to a single tag in Atlan. - You can also attach Google BigQuery tags, including tag values and hierarchies, to your Google BigQuery assets in Atlan - allowing you to categorize your assets at a more granular level Atlan supports: - Tags - enrich your Google BigQuery tables, views, and materialized views with tags and tag values. - Policy tags - enrich your Google BigQuery columns with policy tags and tag hierarchies. - You can filter your assets by Google BigQuery tags Atlan currently does not support crawling Dataplex tag templates. ## Prerequisites ‚Äã Before you can import tags from Google BigQuery, you will need to do the following: - Create tags or have existing tags in Google BigQuery. - Grant permissions to import tags from Google BigQuery View Google BigQuery tags A View Google BigQuery tags !Begin by navigating to the governance center!Manage tags from here Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Import Google BigQuery tags to Atlan ‚Äã Who can do this You will need to be an admin user in Atlan to import Google BigQuery tags You will also need to work with your Google BigQuery administrator for additional inputs and approval Atlan imports existing Google BigQuery tags through one-way tag sync The imported Google BigQuery tags are matched to corresponding tags in Atlan through case-insensitive name match and your Google BigQuery assets enriched with the tags synced from Google BigQuery To import Google BigQuery tags to Atlan, you can either: - Create a new Google BigQuery workflow and configure the crawler to import tags. - Modify the crawler's configuration for an existing Google BigQuery workflow to change _Import Tags_ to **Yes** If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags Once the crawler has completed running, tags synced from Google BigQuery will be available to use for tagging assets! üéâ ## View Google BigQuery tags in Atlan ‚Äã Once you've crawled Google BigQuery, you will be able to view and manage your Google BigQuery tags in Atlan To view synced Google BigQuery tags: 1 From the left menu of any screen, click **Governance**. 2 Under the _Governance_ heading of the _Governance cente_r, click **Tags**. 3. (Optional) Under _Tags_, click the funnel icon to filter tags by source type Click **BigQuery** to filter for tags imported from Google BigQuery. 4 In the _Overview_ section, you can view a total count of synced Google BigQuery tags",
    "metadata": {
      "topic": "Manage Google BigQuery tags",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/manage-google-bigquery-tags",
      "keywords": [
        "manage",
        "google",
        "bigquery",
        "tags",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__0",
    "text": "On this page AWS PrivateLink creates a secure, private connection between services running in AWS This document describes the steps to set this up between Tableau server and Atlan, when you use our Single Tenant SaaS deployment Who can do this You will need your AWS administrator involved - you may not have access to run these tasks yourself. ## Prerequisites ‚Äã You should already have the following: - Tableau instance running in AWS (private EC2 instance). - Atlan hosted in the same region as the Tableau instance. **Did you know?** You will also need Atlan's AWS account ID later in this process If you do not already have this, request it now from support. ## Setup network to EC2 instance ‚Äã To setup the private network of your Tableau EC2 instance, from within AWS: ### Copy network settings ‚Äã To copy the network settings of your EC2 instance: 1 Navigate to **Services**, then **Compute**, then **EC2**. 2 On the left, under _Instances_, click **Instances**. 3 In the _Instances_ table, click on your Tableau EC2 instance. 4 Under the instance's _Details_ tab: 1 Under _VPC ID_ copy the VPC identifier. 2 Under _Subnet ID_ click the subnet for the instance. 3 In the _Subnets_ table, copy the value under the _IPv4 CIDR_ column. ### Create inbound rule ‚Äã To create an inbound rule allowing your private subnet access to your EC2 instance: 1 Navigate to **Services**, then **Compute**, then **EC2**. 2 On the left, under _Instances_, click **Instances**. 3 In the _Instances_ table, click on your Tableau EC2 instance. 4 Under the instance's details, change to the **Security** tab. 5 Under _Security groups_ click the security group for the instance. 6 Under the _Inbound rules_ tab, click the **Edit inbound rules** button. 7 At the bottom left of the _Inbound rules_ table, click the **Add rule** button. 1 For _Type_, select **Custom TCP**. 2 For _Port range_, enter the port on which Tableau is accessible (for example, default port **80** and TLS port **443**). 3 For _Source_, choose **Custom** and enter the CIDR range for your Tableau instance (see Copy network settings). 8 Below the bottom right of the _Inbound rules_ table, click the **Save rules** button. ## Create internal Network Load Balancer ‚Äã ### Start creating NLB ‚Äã To create an NLB, from within AWS: 1 Navigate to **Services**, then **Compute**, then **EC2**. 2 On the left, under _Load Balancing_, click on **Load Balancers**. 3 At the top of the screen, click the **Create Load Balancer** button. 4 Under the _Network Load Balancer_ option, click the **Create** button. 5 Enter the following _Basic configuration_ settings for the load balancer: 1 For _Load balancer name_ enter a unique name. 2 For _Scheme_ select **Internal**. 3 For _IP address type_ select **IPv4**. 6 Enter the following _Network mapping_ settings for the load balancer: 1 For _VPC_ select the VPC where the Tableau instance is located (see Copy network settings). 2 For _Mappings_ select the availability zones with private subnets. 7",
    "metadata": {
      "topic": "Set up a private network link to Tableau server",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-a-private-network-link-to-tableau-server",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "tableau",
        "server",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__1",
    "text": "For _VPC_ select the VPC where the Tableau instance is located (see Copy network settings). 2 For _Mappings_ select the availability zones with private subnets. 7 Enter the following _Listeners and routing_ settings for the load balancer: 1 For _Port_ enter **80** (or the non-default port value used in Created inbound rule). 2 For _Default action_ click the **Create target group** link This will open the target group creation in a new browser tab. ### Create target group ‚Äã To create a target group for the NLB: 1 Enter the following _Basic configuration_ settings for the target group: 1 For _Choose target type_ select **Instances**. 2 For _Target group name_ enter a name. 3 For _Port_ enter **80** (or the non-default port value used in Create inbound rule). 4 For _VPC_ select the VPC where the Tableau instance is located (see Copy network settings). 5 At the bottom of the form, click the **Next** button. 2 From the _Available instances_ table: 1 Click the checkbox next to your Tableau instance. 2 Enter the port for the instance (80 or non-default value used in steps above). 3 Click the **Include as pending below** button. 3 At the bottom right of the form, click the **Create target group** button. ### Finish creating NLB ‚Äã Return to the browser tab where you started the NLB creation, and continue: 1 Under _Listeners and routing_, click the refresh arrow to the far right of the _Default action_ drop-down box. 2 Select the target group you created above in the _Default action_ drop-down. 3 At the bottom right of the form click the **Create load balancer** button. 4 In the resulting screen, click the **View load balancer** button. ### Verify target group is healthy ‚Äã danger As a prerequisite for TLS configuration on Tableau Server only, ensure that the health check _Protocol_ of the target group is set to **HTTPS** or modify the health check settings as required To verify the target group is healthy: 1 From the EC2 menu on the left, under _Load Balancing_, click **Target Groups**. 2 From the _Target groups_ table, click the row for the target group you created above. 3 At the bottom of the screen, under the _Details_ tab, check that there is a 1 under both _Total targets_ and _Healthy_. ## Create endpoint service ‚Äã To create an endpoint service, from within AWS: 1 Navigate to **Services**, then **Networking & Content Delivery**, then **VPC**. 2 From the menu on the left, under _Virtual private cloud_ click **Endpoint services**. 3 At the top of the page, click the **Create endpoint service** button. 4 Enter the following _Endpoint service settings_: 1 For _Name_ enter a meaningful name. 2 For _Load balancer type_ choose **Network**. 5 For _Available load balancers_ select the load balancer you created above in Create internal Network Load Balancer. 6 Enter the following _Additional settings_: 1 For _Require acceptance for endpoint_ enable **Acceptance required**. 2 For _Supported IP address types_ enable **IPv4**. 7",
    "metadata": {
      "topic": "Set up a private network link to Tableau server",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-a-private-network-link-to-tableau-server",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "tableau",
        "server",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__2",
    "text": "For _Require acceptance for endpoint_ enable **Acceptance required**. 2 For _Supported IP address types_ enable **IPv4**. 7 At the bottom right of the form, click the **Create** button. 8 Under the _Details_ of the endpoint service, copy the hostname under _Service name_. ## Allow Atlan account access ‚Äã To allow Atlan's account access to the service, from within the endpoint service screen: 1 At the bottom of the screen, change to the **Allow principals** tab. 2 At the top of the _Allow principals_ table, click the **Allow principals** button. 3 Under _Principals to add_ and _ARN_ enter the Atlan account ID. 4 At the bottom right of the form, click the **Allow principals** button. ## Notify Atlan support ‚Äã Once all the above steps are complete, provide Atlan support with the following information: - The hostname for the endpoint service created above. - The port number for the Tableau instance. - For SSL certificates only, the private DNS name for which you have issued an SSL certificate on your Tableau Server instance There are additional steps Atlan then needs to complete: - Creating a security group. - Creating an endpoint. - For SSL certificates only, creating a DNS CNAME record pointing the private DNS name shared above to the VPC endpoint URL This will allow Atlan to use your private DNS name with the SSL certificate Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. ## Accept the consumer connection request ‚Äã To accept the consumer connection request, from within AWS: 1 Navigate to **Services**, then **Networking & Content Delivery**, then **VPC**. 2 From the menu on the left, under _Virtual private cloud_ click **Endpoint services**. 3 From the _Endpoint services_ table, select the endpoint service you created in Create endpoint service. 4 At the bottom of the screen, change to the **Endpoint connections** tab. 1 You should see a row in the _Endpoint connections_ table with a _State_ of _Pending_. 2 Select this row, and click the **Actions** button and then **Accept endpoint connection request**. 3 If prompted to confirm, type **accept** into the field and click the **Accept** button. 5 Wait for this to complete, it could take about 30 seconds. üòÖ The connection is now established You can now use the service endpoint provided by Atlan support (or the private DNS name for SSL certificates) as the hostname to crawl Tableau in Atlan! üéâ - Prerequisites - Setup network to EC2 instance - Create internal Network Load Balancer - Create endpoint service - Allow Atlan account access - Notify Atlan support - Accept the consumer connection request",
    "metadata": {
      "topic": "Set up a private network link to Tableau server",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-a-private-network-link-to-tableau-server",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "tableau",
        "server",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_tableau_0",
    "text": "On this page Once you have configured Tableau, you can establish a connection between Atlan and Tableau. (If you are also using a private network for Tableau, you will need to set that up first, too.) Crawl Tableau A Crawl Tableau !Begin by adding a new workflow!The Tableau Assets package allows us to crawl metadata from Tableau Interactive walkthrough reCAPTCHA To crawl metadata from Tableau, review the order of operations and then complete the following steps. ## Select the source ‚Äã To select Tableau as your source: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the list of packages, select **Tableau Assets** and click on **Setup Workflow**. ## Provide credentials ‚Äã - In **Direct** extraction, Atlan connects to Tableau and crawls metadata directly. - In **Offline** extraction, you need to first extract metadata yourself and make it available in S3. - In **Agent** extraction, Atlan‚Äôs secure agent executes metadata extraction within the organization's environment. ### Direct extraction method ‚Äã To enter your Tableau credentials: 1 For _Host Name_, enter the host name of your Tableau Online or Tableau Server instance (or the private DNS name if your Tableau Server instance uses an SSL certificate). 2 For _Port_, enter the port number of your Tableau instance. 3 For _Authentication_, choose how you would like to connect to Tableau: - For **Basic** authentication, enter the _Username_ and _Password_ you use to log in to Tableau. - For **Personal Access Token** authentication, enter the _Personal Access Token Name_ and _Personal Access Token Value_ you generated_Create_a_personal_access_token). - For **JWT Bearer** authentication, enter your Tableau Server username or Tableau Online email address for _Username_, and the _Client ID_, _Secret ID_, and _Secret Value_ you copied from the connected app in Tableau. 4. (Optional) For _SSL_, keep the default _Enabled_ to use HTTPS or click **Disabled** to use HTTP. 5 For _Site_, enter the name of the site you want to crawl. (If left blank, the default site will be used.) danger If you are using Tableau Online, the site is required for Atlan to authenticate properly. 6. (Optional) For _SSL certificate_, this is only required if your Tableau Server instance uses a self-signed or an internal CA SSL certificate, paste a supported SSL certificate in the recommended format. 7 At the bottom of the form, click the **Test Authentication** button to confirm connectivity to Tableau using these details. 8 When successful, at the bottom of the screen click the **Next** button. ### Offline extraction method ‚Äã Atlan also supports the offline extraction method for fetching metadata from Tableau This method uses Atlan's tableau-extractor tool to fetch metadata You will need to first extract the metadata yourself and then make it available in S3 To enter your S3 details: 1 For _Bucket name_, enter the name of your S3 bucket. 2 For _Bucket prefix_, enter the S3 prefix under which all the metadata files exist",
    "metadata": {
      "topic": "Crawl Tableau",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau",
      "keywords": [
        "crawl",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_tableau_1",
    "text": "For _Bucket name_, enter the name of your S3 bucket. 2 For _Bucket prefix_, enter the S3 prefix under which all the metadata files exist These include `dashboards/result-0.json`, `workbooks/result-0.json`, and so on. 3. (Optional) For _Bucket region_, enter the name of the S3 region. 4 When complete, at the bottom of the screen, click **Next**. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for fetching metadata from Tableau To use a Secure Agent, follow these steps: 1 Select the **Agent** tab. 2 Configure the Tableau data source by adding the secret keys for your secret store For details on the required fields, refer to the Direct extraction section. 3 Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. 4 Click **Next** after completing the configuration. ## Configure the connection ‚Äã To complete the Tableau connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might want to use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. ## Configure the crawler ‚Äã Before running the Tableau crawler, you can further configure it. 1 On the _Metadata_ page, you can override the defaults for any of these options: - To select the Tableau projects you want to include in crawling, click **Include Projects**. (This will default to all assets, if none are specified.) - To select the Tableau projects you want to exclude from crawling, click **Exclude Projects**. (This will default to no assets, if none are specified.) - To have the crawler ignore Tableau projects based on a naming convention, specify a regular expression in the _Exclude Projects Regex_ field. 2 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. **Did you know?** If a project appears in both the include and exclude filters, the exclude filter takes precedence. (The _Exclude Projects Regex_ also takes precedence.) ## Configure advanced controls ‚Äã Before running the Tableau crawler, you can also configure advanced controls for the crawler",
    "metadata": {
      "topic": "Crawl Tableau",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau",
      "keywords": [
        "crawl",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_tableau_2",
    "text": "On the _Metadata_ page, you can override the defaults for any of these options: - To select the Tableau projects you want to include in crawling, click **Include Projects**. (This will default to all assets, if none are specified.) - To select the Tableau projects you want to exclude from crawling, click **Exclude Projects**. (This will default to no assets, if none are specified.) - To have the crawler ignore Tableau projects based on a naming convention, specify a regular expression in the _Exclude Projects Regex_ field. 2 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. **Did you know?** If a project appears in both the include and exclude filters, the exclude filter takes precedence. (The _Exclude Projects Regex_ also takes precedence.) ## Configure advanced controls ‚Äã Before running the Tableau crawler, you can also configure advanced controls for the crawler On the _Advanced_ page, you can override the defaults for any of these options: - For _Alternate Host URL_, enter the protocol and host name to be used for viewing assets directly in Tableau. - For _Crawl Unpublished Worksheets and Dashboards_, click **Yes** to enable crawling hidden worksheets and dashboards or **No** to skip crawling them. - For _Hidden Datasource Fields_, click **Yes** to enable crawling hidden datasource fields or **No** to skip crawling them. - Crawl embedded dashboards: Embedded dashboard here means linking or displaying a dashboard inside another dashboard by providing a link to the dashboard in a Web Page item of the embedding dashboard. - Click **Yes** to enable relationships between different embedded dashboards. - Click **No** to skip creating relationships between embedded dashboards. ## Run the crawler ‚Äã To run the Tableau crawler, after completing the steps above: - You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button Once the crawler has completed running, you will see the assets in Atlan's asset page! üéâ - Select the source - Provide credentials - Configure the connection - Configure the crawler - Configure advanced controls - Run the crawler",
    "metadata": {
      "topic": "Crawl Tableau",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau",
      "keywords": [
        "crawl",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_tab_0",
    "text": "On this page Atlan crawls and maps the following assets and properties from Tableau Once you've crawled Tableau, you can use connector-specific filters for quick asset discovery The following filters are currently supported for these assets: - Projects - filter Tableau assets by projects, including nested projects - Data sources - Is published filter !Screen_Shot_2022-01-26_at_2.32.21_PM.png For your Tableau worksheets and dashboards, Atlan also provides asset previews to help with quick discovery and give you the context you need. warning You may need to disable clickjack protection for Tableau asset previews to load View Tableau asset preview A View Tableau asset preview !View a preview of your Tableau dashboard (also available for worksheets)!Step 3 Illustrative example reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms ## Lineage ‚Äã info **Did you know?** Lineage to dashboards may appear incomplete or missing if worksheets are not crawled Additionally, Tableau assets that haven't been refreshed since May 27, 2025, won't display the new column-level lineage (CLL) or updated lineage paths Atlan supports lineage for the following: - **Asset Lineage** - Datasource to Dashboard, Datasource to Worksheet, Datasource to Workbook - **Column Level Lineage** - Supported for Datasource to Worksheet and Worksheet to Dashboard ## Sites ‚Äã Atlan maps sites from Tableau to its `TableauSite` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | ## Projects ‚Äã Atlan maps projects from Tableau to its `TableauProject` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `owner` | `sourceOwner` | asset profile and overview sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `hierarchy` | `projectHierarchy` | asset preview and profile, overview sidebar | | `topLevelProject` | `isTopLevelProject` | API only | ## Flows ‚Äã warning Due to limitations at source, Atlan won't be able to crawl Tableau flows if you use the JWT bearer authentication method Atlan maps flows from Tableau to its `TableauFlow` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `owner` | `sourceOwner` | asset profile and overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Metrics ‚Äã warning Tableau has retired metrics methods in API 3.22 for Tableau Cloud and Tableau Server version 2024.2 If you're using Tableau API version 3.22 or higher, metadata for metrics is unavailable in Atlan",
    "metadata": {
      "topic": "What does Atlan crawl from Tableau?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-from-tableau",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_tab_1",
    "text": "Atlan maps flows from Tableau to its `TableauFlow` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `owner` | `sourceOwner` | asset profile and overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Metrics ‚Äã warning Tableau has retired metrics methods in API 3.22 for Tableau Cloud and Tableau Server version 2024.2 If you're using Tableau API version 3.22 or higher, metadata for metrics is unavailable in Atlan Atlan maps metrics from Tableau to its `TableauMetric` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Workbooks ‚Äã Atlan maps workbooks from Tableau to its `TableauWorkbook` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `webpageUrl` | `sourceURL` | overview sidebar | | `owner` | `sourceOwner` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Worksheets ‚Äã Atlan maps worksheets from Tableau to its `TableauWorksheet` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `source_url` | `sourceURL` | overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Dashboards ‚Äã Atlan maps dashboards from Tableau to its `TableauDashboard` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `source_url` | `sourceURL` | overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Data sources ‚Äã Atlan maps data sources (embedded and published) from Tableau to its `TableauDatasource` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `owner` | sourceOwner (for published data sources only) | asset profile and overview sidebar | | `isPublished` | `isPublished` | asset preview and overview sidebar | | `hasExtracts` | `hasExtracts` | API only | | `upstreamTables` | `upstreamTables` | API only | | `upstreamDatabases` | `upstreamDatabases` | API only | | `isCertified` | certificateStatus ( `VERIFIED`) | asset preview and filter, overview sidebar | | `certifier` | `certifier` | API only | | `certificationNote` | `certificationStatusMessage` | API only | | `certifierDisplayName` | `certificateUpdatedBy` | asset preview and overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Data source fields ‚Äã Atlan maps data source fields and column fields from Tableau to its `TableauDatasourceField` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `upstreamTables` | `upstreamTables` | API only | | `upstreamColumns` | `upstreamColumns` | API only | | `dataCategory` | `tableauDatasourceFieldDataCategory` | API only | | `role` | `tableauDatasourceFieldRole` | API only | | `dataType` | `tableauDatasourceFieldDataType` | API only | | `formula` | `tableauDatasourceFieldFormula` | API only | | `binSize` | `tableauDatasourceFieldBinSize` | API only | | `__typename` | `datasourceFieldType` | API only | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Custom SQL ‚Äã Atlan parses custom SQL queries used in Tableau data sources to extract lineage information",
    "metadata": {
      "topic": "What does Atlan crawl from Tableau?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-from-tableau",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_tab_2",
    "text": "If you're using Tableau API version 3.22 or higher, metadata for metrics is unavailable in Atlan Atlan maps metrics from Tableau to its `TableauMetric` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Workbooks ‚Äã Atlan maps workbooks from Tableau to its `TableauWorkbook` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `webpageUrl` | `sourceURL` | overview sidebar | | `owner` | `sourceOwner` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Worksheets ‚Äã Atlan maps worksheets from Tableau to its `TableauWorksheet` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `source_url` | `sourceURL` | overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Dashboards ‚Äã Atlan maps dashboards from Tableau to its `TableauDashboard` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `createdAt` | `sourceCreatedAt` | asset profile and properties sidebar | | `updatedAt` | `sourceUpdatedAt` | asset profile and properties sidebar | | `source_url` | `sourceURL` | overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Data sources ‚Äã Atlan maps data sources (embedded and published) from Tableau to its `TableauDatasource` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `owner` | sourceOwner (for published data sources only) | asset profile and overview sidebar | | `isPublished` | `isPublished` | asset preview and overview sidebar | | `hasExtracts` | `hasExtracts` | API only | | `upstreamTables` | `upstreamTables` | API only | | `upstreamDatabases` | `upstreamDatabases` | API only | | `isCertified` | certificateStatus ( `VERIFIED`) | asset preview and filter, overview sidebar | | `certifier` | `certifier` | API only | | `certificationNote` | `certificationStatusMessage` | API only | | `certifierDisplayName` | `certificateUpdatedBy` | asset preview and overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Data source fields ‚Äã Atlan maps data source fields and column fields from Tableau to its `TableauDatasourceField` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `upstreamTables` | `upstreamTables` | API only | | `upstreamColumns` | `upstreamColumns` | API only | | `dataCategory` | `tableauDatasourceFieldDataCategory` | API only | | `role` | `tableauDatasourceFieldRole` | API only | | `dataType` | `tableauDatasourceFieldDataType` | API only | | `formula` | `tableauDatasourceFieldFormula` | API only | | `binSize` | `tableauDatasourceFieldBinSize` | API only | | `__typename` | `datasourceFieldType` | API only | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Custom SQL ‚Äã Atlan parses custom SQL queries used in Tableau data sources to extract lineage information This process identifies the relationships between data assets based on the SQL logic defined within Tableau. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `downstreamDatasources` | `TableauDatasource` | Used to define downstream impact of lineage | | `query` | `CustomSQLQuery` | Used to form lineage from source | ## Calculated fields ‚Äã Atlan maps calculated fields from Tableau to its `TableauCalculatedField` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `dataCategory` | `dataCategory` | API only | | `role` | `role` | API only | | `dataType` | `tableauDataType` | asset preview, filter, and profile, overview sidebar | | `formula` | `formula` | overview sidebar | | `project_extra (hierarchy)` | `projectHierarchy` | asset preview and profile, overview sidebar | ## Lineage ‚Äã Atlan calculates lineage for Tableau as follows: | Source object | Tableau object | Tableau object (downstream) | | --- | --- | --- | | Table | Published data source | Published data source | | Table | Published data source | Embedded data source | | Table | Embedded data source | | | Column | Data source field | Calculated field | | Column | Data source field | Data source field | info Lineage is currently not supported for Tableau flows and metrics. - Lineage - Sites - Projects - Flows - Metrics - Workbooks - Worksheets - Dashboards - Data sources - Data source fields - Custom SQL - Calculated fields - Lineage",
    "metadata": {
      "topic": "What does Atlan crawl from Tableau?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-from-tableau",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_tableau_0",
    "text": "On this page Who can do this You will probably need your Tableau administrator to run these commands - you may not have access yourself. ## Enable the Tableau Metadata API ‚Äã To enable the Tableau Metadata API, follow the steps in Tableau documentation. danger Atlan needs the Tableau Metadata API to crawl metadata Please ensure you are running the latest version of Tableau Server or Tableau Online (2022.x with REST API version 3.14+) Learn more about the permissions used to access metadata through the Tableau Metadata API. ## Publish the worksheets you want to crawl ‚Äã Ensure you publish the worksheets in Tableau that you want to crawl in Atlan To publish Tableau worksheets, follow the steps in Tableau documentation. ## Choose authentication mechanism ‚Äã Atlan supports the following authentication methods for fetching metadata from Tableau: - Basic - this method uses a username and password. - Personal access token - this method uses a personal access token. - JWT bearer - this method uses a username and JWT client ID, secret ID, and secret value. ### Basic authentication ‚Äã **Did you know?** To crawl assets and extract asset lineage from Tableau, the user must have the _Site Administrator Explorer_ role Atlan requires the _Site Administrator Explorer_ role in Tableau to extract data source fields and calculated fields and create field-level assets and lineage It is not possible to fetch either with the _Viewer_ role in the current version of the Tableau Metadata API. #### Add a user ‚Äã Ensure you add a user with the role _Site Administrator Explorer_ to the site you want to crawl To add such a user, follow the steps in Tableau documentation. #### Grant user permissions ‚Äã Ensure you grant the _View_ capability for all the assets you want to crawl To grant the permission, follow the steps in Tableau documentation. ### Personal access token authentication ‚Äã If you want to access Tableau using an access token, you can generate a personal access token To generate a personal access token, follow the steps in Tableau documentation. ### JWT bearer authentication ‚Äã danger To access the Tableau Metadata API using JWT bearer authentication, you must have Tableau Cloud October 2023 or Tableau Server 2023.3 version In addition, JWT authorization currently does not support all REST API capabilities Due to these limitations at source, Atlan will not be able to crawl Tableau flows if you use the JWT bearer authentication method. #### Configure a connected app ‚Äã If you want to access Tableau using a JSON web token (JWT), you can configure a Tableau connected app There are two types of connected apps that you can configure - direct trust or OAuth 2.0 trust",
    "metadata": {
      "topic": "Set up Tableau",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-tableau",
      "keywords": [
        "set",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_tableau_1",
    "text": "Due to these limitations at source, Atlan will not be able to crawl Tableau flows if you use the JWT bearer authentication method. #### Configure a connected app ‚Äã If you want to access Tableau using a JSON web token (JWT), you can configure a Tableau connected app There are two types of connected apps that you can configure - direct trust or OAuth 2.0 trust To authenticate the Tableau connection in Atlan using this method, you will need the following: - Username - your Tableau Server username or Tableau Online email address, the user must have a _Site Administrator Explorer_ role - Connected app ID - client ID generated for the connected app - Secret ID - secret ID linked to the client ID of the connected app - Secret value - secret value used to sign the token To configure a connected app, follow the steps in Tableau documentation: - Direct trust - OAuth 2.0 trust #### Access scopes for connected apps ‚Äã For JWT authorization, scopes define access permissions granted to the token holder Scopes control the specific actions that an application or user can perform in Tableau while accessing content through a connected app The Tableau connector in Atlan uses two `read` scopes to extract metadata from Tableau Note that the Tableau connector is preconfigured to use these scopes, no action required Atlan uses the following scopes for JWT authentication: - `tableau:content:read` - allows read access to your assets in Tableau, including: - Workbooks - can list, access, and retrieve metadata for workbooks. - Views - can fetch specific views or dashboards within workbooks. - Data sources - can access published data sources and associated metadata. - Projects - can retrieve project metadata. - Metrics - can read metrics associated with workbooks or dashboards. - Tables and databases - can access metadata for tables and databases connected to Tableau. - `tableau:users:read` - allows read access to user details This enables Atlan to display the source owner property for supported Tableau assets, including in the impact analysis report. - Optional) `tableau:workbooks:download` ‚Äì allows downloading a workbook ( `.twb` or `.twbx`), enabling Atlan to display relationships for embedded Tableau dashboards. - Enable the Tableau Metadata API - Publish the worksheets you want to crawl - Choose authentication mechanism",
    "metadata": {
      "topic": "Set up Tableau",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-tableau",
      "keywords": [
        "set",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_onpremises_tableau_0",
    "text": "On this page Once you have set up the tableau-extractor tool, you can extract metadata from your on-premises Tableau instances by completing the following steps. ## Run tableau-extractor ‚Äã ### Crawl all Tableau connections ‚Äã To crawl all Tableau connections using the tableau-extractor tool: 1 Log into the server with Docker Compose installed. 2 Change to the directory containing the compose file. 3 Run Docker Compose: `sudo docker-compose up` ### Crawl a specific connection ‚Äã To crawl a specific Tableau connection using the tableau-extractor tool: 1 Log into the server with Docker Compose installed. 2 Change to the directory containing the compose file. 3 Run Docker Compose: `sudo docker-compose up <connection-name>` (Replace `<connection-name>` with the name of the connection from the `services` section of the compose file.) ## (Optional) Review generated files ‚Äã Review generated files\") The tableau-extractor tool will generate many folders with JSON files for each `service` For example: - `calculated_fields` - `dashboards` - `datasources` - `workbooks` - and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. ## Upload generated files to S3 ‚Äã To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. **Did you know?** We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues Reach out to your Data Success Manager to get the details of your Atlan bucket To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: 1 Ensure that all files for a particular connection have the same prefix For example, `output/tableau-example/dashboards/result-0.json`, `output/tableau-example/workbooks/result-0.json`, and so on. 2 Upload the files to the S3 bucket using your preferred method For example, to upload all files using the AWS CLI: ```codeBlockLines_e6Vv aws s3 cp output/tableau-example s3://my-bucket/metadata/tableau-example --recursive ``` ## Crawl metadata in Atlan ‚Äã Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: - How to crawl Tableau Be sure you select **Offline** for the _Extraction method_. - Run tableau-extractor - (Optional) Review generated files - Upload generated files to S3 - Crawl metadata in Atlan",
    "metadata": {
      "topic": "Crawl on-premises Tableau",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-on-premises-tableau",
      "keywords": [
        "crawl",
        "premises",
        "tableau",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_onpremises_tableau_acce_0",
    "text": "On this page Who can do this You will need access to a machine that can run Docker on-premises You will also need your Tableau instance details, including credentials In some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata For example, this may happen when security requirements restrict access to sensitive, mission-critical data In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan This approach gives you full control over your resources and metadata transfer to Atlan. ## Prerequisites ‚Äã To extract metadata from your on-premises Tableau instance, you will need to use Atlan's tableau-extractor tool. **Did you know?** Atlan uses exactly the same tableau-extractor behind the scenes when it connects to Tableau in the cloud. ### Install Docker Compose ‚Äã Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? üòâ) To install Docker Compose: 1 Install Docker 2 Install Docker Compose **Did you know?** Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. ### Get the tableau-extractor tool ‚Äã To get the tableau-extractor tool: 1 Raise a support ticket to get the link to the latest version. 2 Download the image using the link provided by support. 3 Load the image to the server you'll use to crawl Tableau: ```codeBlockLines_e6Vv sudo docker load -i /path/to/tableau-extractor-master.tar ``` ## Get the compose file ‚Äã Atlan provides you with a Docker compose file for the tableau-extractor tool To get the compose file: 1 Download the latest compose file. 2 Save the file to an empty directory on the server you'll use to access your on-premises Tableau instance. 3 The file is `docker-compose.yaml`. ## Define Tableau connections ‚Äã The structure of the compose file includes three main sections: - `x-templates` contains configuration fragments You should ignore this section - do not make any changes to it. - `services` is where you will define your Tableau connections. - `volumes` contains mount information You should ignore this section as well - do not make any changes to it. ### Define services ‚Äã For each on-premises Tableau instance, define an entry under `services` in the compose file Each entry will have the following structure: ```codeBlockLines_e6Vv services: connection-name: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"\" volumes: - ./output/connection-name:/output/process ``` - Replace `connection-name` with the name of your connection. - `<<: *extract` tells the tableau-extractor tool to run. - `environment` contains all parameters for the tool. - `CERT_PATH` - if applicable, specify the SSL certificate path and store it as a new volume. - `volumes` specifies where to store results In this example, the extractor will store results in the `./output/connection-name` folder on the local file system",
    "metadata": {
      "topic": "Set up on-premises Tableau access",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access",
      "keywords": [
        "set",
        "premises",
        "tableau",
        "access",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_onpremises_tableau_acce_1",
    "text": "Each entry will have the following structure: ```codeBlockLines_e6Vv services: connection-name: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"\" volumes: - ./output/connection-name:/output/process ``` - Replace `connection-name` with the name of your connection. - `<<: *extract` tells the tableau-extractor tool to run. - `environment` contains all parameters for the tool. - `CERT_PATH` - if applicable, specify the SSL certificate path and store it as a new volume. - `volumes` specifies where to store results In this example, the extractor will store results in the `./output/connection-name` folder on the local file system You can add as many Tableau connections as you want. **Did you know?** Docker's documentation describes the `services` format in more detail. ## Provide credentials ‚Äã To define the credentials for your Tableau connections, you will need to provide a Tableau configuration file The Tableau configuration is a `.ini` file with the following format: ```codeBlockLines_e6Vv [TableauConfig] # Tableau instance URL Do not include /api/* in the URL. server_url=https://:<hostname>:<port> # Tableau site name Leaving this empty will select the default site. site_name=YourTableauSite # Tableau authentication type Options: basic, personal_access_token. auth_type=basic # Required only if auth_type is basic. [BasicAuth] username=YourTableauUsername password=YourTableauPassword # Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] token_name=YourTableauTokenName token_value=YourTableauTokenValue ``` danger For basic authentication, ensure that your password does not contain the special character `%` If the percent sign is included in your password, add another `%` to escape it. ## Secure credentials ‚Äã ### Using local files ‚Äã danger If you decide to keep Tableau credentials in plaintext files, we recommend you restrict access to the directory and the compose file For extra security, we recommend you use Docker secrets to store the sensitive passwords To specify the local files in your compose file: ```codeBlockLines_e6Vv secrets: tableau_config: file: ./tableau.ini ``` danger This `secrets` section is at the same top-level as the `services` section described earlier It is not a sub-section of the `services` section. ### Using Docker secrets ‚Äã To create and use Docker secrets: 1 Store the Tableau configuration file: ```codeBlockLines_e6Vv sudo docker secret create tableau_config path/to/tableau.ini ``` 2 At the top of your compose file, add a secrets element to access your secret: ```codeBlockLines_e6Vv secrets: tableau_config: external: true name: tableau_config ``` - The `name` should be the same one you used in the `docker secret create` command above. - Once stored as a Docker secret, you can remove the local Tableau configuration file. 3 Within the `service` section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. ## Example ‚Äã Let's explain in detail with an example: ```codeBlockLines_e6Vv secrets: tableau_config: external: true name: tableau_config x-templates: # ... services: my-tableau: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"/tmp/tab-cert.pem\" volumes: - ./output/my-tableau:/output/process - ./tab-cert.pem:/tmp/tab-cert.pem secrets: - tableau_config ``` 1 In this example, we've defined the secrets at the top of the file (you could also define them at the bottom)",
    "metadata": {
      "topic": "Set up on-premises Tableau access",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access",
      "keywords": [
        "set",
        "premises",
        "tableau",
        "access",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_onpremises_tableau_acce_2",
    "text": "Within the `service` section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. ## Example ‚Äã Let's explain in detail with an example: ```codeBlockLines_e6Vv secrets: tableau_config: external: true name: tableau_config x-templates: # ... services: my-tableau: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"/tmp/tab-cert.pem\" volumes: - ./output/my-tableau:/output/process - ./tab-cert.pem:/tmp/tab-cert.pem secrets: - tableau_config ``` 1 In this example, we've defined the secrets at the top of the file (you could also define them at the bottom) The `tableau_config` refers to an external Docker secret created using the `docker secret create` command. 2 The name of this service is `my-tableau` You can use any meaningful name you want. 3 The `<<: *tableau-defaults` sets the connection type to Tableau. 4. `EXCLUDE_PROJECTS_REGEX` tells the extractor to filter out all the projects whose names match the `Test1.*` and `Test2.*` regex patterns in the extracted metadata. 5. `CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS` tells the extractor to include all hidden or unpublished worksheets and dashboards that are part of a Tableau workbook in the extracted metadata. 6. `CRAWL_EMBEDDED_DASHBOARDS` tells the extractor to create relationships between Tableau dashboards used within another dashboard as a _Web Page_ item. 7 The `CERT_PATH` tells the extractor where to store the SSL certificate, if applicable In this example, the extractor will store results in the `./tab-cert.pem` directory on the local file system If the SSL certificate is not stored in the same folder as the compose file, you will need to specify the full path. 8 The `./output/my-tableau:/output/process` line tells the extractor where to store results In this example, the extractor will store results in the `./output/my-tableau` directory on the local file system We recommend you output the extracted metadata for different connections in separate directories. 9 The `secrets` section within `services` tells the extractor which secrets to use for this service Each of these refers to the name of a secret listed at the beginning of the compose file. - Prerequisites - Get the compose file - Define Tableau connections - Provide credentials - Secure credentials - Example",
    "metadata": {
      "topic": "Set up on-premises Tableau access",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access",
      "keywords": [
        "set",
        "premises",
        "tableau",
        "access",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_tableau_connec_0",
    "text": "On this page #### What are the known limitations of the Tableau connector? ‚Äã Atlan currently does not support the following: - Crawling Tableau flows when using the JWT bearer authentication method, due to limitations at source. - Crawling tags from Tableau. - Cataloging Tableau Pulse, stories, and views. - Parsing custom SQL queries, where all tables referenced (whether in the main query or subqueries) are considered as upstream lineage for the asset. #### Why does Atlan require the Site Administrator Explorer role in Tableau? ‚Äã Atlan requires the _Site Administrator Explorer_ role in Tableau to extract data source fields and calculated fields It is not possible to fetch data source fields and calculated fields with the _Viewer_ role in the current version of the Tableau Metadata API Atlan uses this data to generate granular column-level lineage across data sources and SQL assets To extract lineage for assets in Tableau, the user must have the _Site Administrator Explorer_ role. #### Is lineage available for Tableau custom SQL data sources? ‚Äã Yes, Atlan can parse custom SQL queries in Tableau to generate lineage between the data source and tables Lineage is available for tables from all SQL sources However, column-level lineage is currently not supported. #### Why is upstream lineage missing for Tableau data sources? ‚Äã If your Tableau data source is in a paused state, the Tableau Metadata API may fail to provide the requisite metadata on source databases and tables for Atlan to generate upstream lineage Restart your Tableau data source and ensure that it remains active while crawling Tableau This will allow Atlan to fetch the requisite metadata to generate upstream lineage for data sources. #### Why is there a discrepancy in asset count between Tableau and Atlan? ‚Äã - Dashboards - the Tableau UI does not display a unique count of dashboards Dashboards in Tableau are represented in collections of one or more views These may have same names as the views but are independent objects Hence, the total count of these views in Tableau does not match the dashboard count in Atlan Atlan sources the dashboard count from the Tableau API, which is the only reliable way to fetch the dashboard count. - Data sources - embedded data sources are not reported on the Tableau UI However, in Atlan, data sources can be filtered to show only published data sources, which should match the count of data sources on the Tableau UI. #### Can users who do not have access to a dashboard still see the preview? ‚Äã Users can only see asset previews if the following conditions are met: - They have the necessary permissions in both Tableau and Atlan. - They are logged into Atlan and Tableau on the same browser Therefore, if a user lacks the permission to view a dashboard in Tableau, they will not be able to view the dashboard preview in Atlan",
    "metadata": {
      "topic": "Troubleshooting Tableau connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity",
      "keywords": [
        "troubleshooting",
        "tableau",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_tableau_connec_1",
    "text": "However, in Atlan, data sources can be filtered to show only published data sources, which should match the count of data sources on the Tableau UI. #### Can users who do not have access to a dashboard still see the preview? ‚Äã Users can only see asset previews if the following conditions are met: - They have the necessary permissions in both Tableau and Atlan. - They are logged into Atlan and Tableau on the same browser Therefore, if a user lacks the permission to view a dashboard in Tableau, they will not be able to view the dashboard preview in Atlan Even if they do have the necessary permissions, they will need to be logged into Tableau on the same browser as their Atlan instance for asset previews to work. #### Why can I not see previews for my Tableau assets? ‚Äã - Your Tableau assets will be updated with previews during the next run of your Tableau workflow If you have run the workflow and still do not see the previews, we suggest you rerun the workflow Once you've rerun the workflow, the previews should be visible to all eligible users. - If you're using Tableau Server with clickjack protection enabled and your Tableau instance URL is of a different origin than the Atlan instance URL, the asset previews will not load due to a same-origin error from the browser You will need to disable clickjack protection to allow the Tableau asset previews to load. #### Is the certified status in Tableau mapped to the certificates field in Atlan? ‚Äã Yes, the isCertified status for published data sources in Tableau is mapped to the certificates field in Atlan. #### Is the owner field in Tableau mapped to the owners field in Atlan? ‚Äã No, the asset owner in Tableau is displayed as the source owner in the _Overview_ section of the asset sidebar in Atlan This is also only available for Tableau projects, flows, workbooks, and published data sources Tableau has retired metrics methods in API 3.22, hence source owner attribute for metrics is not supported in Atlan. #### Why am I getting a \"still creating the Metadata API store\" error? ‚Äã Error message: `Still creating the Metadata API Store Results from the query might be incomplete at this time BACKFILL-RUNNING` If your Tableau workflow is failing with the above error message, this is because the Tableau Metadata API is being re-indexed after a quarterly release The re-indexing of the Metadata API after quarterly releases can take up to a week, depending on the size of your instance Since Atlan uses the Tableau Metadata API to fetch metadata, your Tableau workflows in Atlan may fail if the re-indexing has not been completed You can check the backfill status of the Tableau Metadata API Store following this guide Learn more about common errors in your Metadata API query. #### How to debug test authentication and preflight check errors? ‚Äã **Incorrect hostname** `Unable to connect to the specified host",
    "metadata": {
      "topic": "Troubleshooting Tableau connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity",
      "keywords": [
        "troubleshooting",
        "tableau",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_tableau_connec_2",
    "text": "You can check the backfill status of the Tableau Metadata API Store following this guide Learn more about common errors in your Metadata API query. #### How to debug test authentication and preflight check errors? ‚Äã **Incorrect hostname** `Unable to connect to the specified host Please verify that the host details are correct and retry.` - Ensure that you have entered the hostname for your Tableau Online or Tableau Server instance correctly. - If you're using a domain name, verify that the DNS name correctly resolves to the corresponding IP address. **Connection timed out** `Unable to connect to Tableau instance Please verify server port or check if your server is up and running.` - Ensure that you're using the correct port number, especially if using a custom port for Tableau Server. - Verify network connectivity and DNS resolution - you can also test from a different network or device. **SSL error** `Unable to connect Please check your SSL setting.` - Ensure that the server URL uses `https` if SSL is enabled If the connection does not require an SSL, use `http` instead. `The ssl details provided are incorrect Please provide correct ssl certs.` - If your Tableau Server instance uses a self-signed or an internal CA SSL certificate, enter the SSL certificate correctly in the recommended format. **Incorrect port number** `Unable to connect to Tableau instance Please verify server port and retry.` - Ensure that you're using the correct port number, especially if using a custom port for Tableau Server. **Invalid personal access token** `The personal access token you provided is invalid Please check your PAT name and token value.` - Ensure that you have entered the token name correctly and it matches the token generated in Tableau: - Token name is case-sensitive. - Ensure that there are no extra spaces or characters. - If the token you provided is invalid, you can create a new token. **Incorrect site details** `The site details provided are incorrect Please provide correct site details.` - Confirm that the site name in the URL matches the exact case and spelling of the site you are trying to access Site names in Tableau are case-sensitive. **Incorrect username** `Provided username is incorrect Please check.` - Confirm that the username is present in Tableau Otherwise, you can add a new user for basic authentication. **Incorrect client ID** `The client id provided is incorrect or site is empty or connected app in tableau is deleted Please check and try again.` - Ensure that you have specified the site name if using JWT bearer authentication. - Ensure that the connected app is present in Tableau and verify the client ID. **Incorrect secret ID or value** `The secret id provided is incorrect or the secret value is deleted Please check and try again.` or `The secret value provided is incorrect",
    "metadata": {
      "topic": "Troubleshooting Tableau connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/troubleshooting/troubleshooting-tableau-connectivity",
      "keywords": [
        "troubleshooting",
        "tableau",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__0",
    "text": "On this page AWS PrivateLink creates a secure, private connection between services running in AWS This document describes the steps to set this up between PostgreSQL (RDS) and Atlan, when you use our Single Tenant SaaS deployment Who can do this You will need your AWS administrator involved - you may not have access to run these tasks yourself. ## Prerequisites ‚Äã You should already have the following: - Your own non-default VPC configured in AWS. - A PostgreSQL RDS instance running in AWS, linked to the non-default VPC. - Private subnets defined within the non-default VPC sufficient for availability. **Did you know?** You will also need Atlan's AWS account ID later in this process If you do not already have this, request it now from support. ## Setup network to RDS (in AWS) ‚Äã\") To setup the private network of your PostgreSQL instance, from within AWS: ### Copy network settings ‚Äã 1 Navigate to **Services**, then **Database**, then **RDS**. 2 On the left, under _Amazon RDS_, click on **Databases**. 3 From the _Databases_ table, click your instance's name under the _DB identifier_ column. 4 Under the _Connectivity & security_ tab, copy the following values: 1. _Endpoint_ and _Port_ values 2. _VPC_ value 3. _Subnet group_ value 5 On the left, click **Subnet groups**. 6 From the table, click the row whose _Name_ matches the subnet group copied above. 7 From the _Subnets_ table, copy each value under the _CIDR block_ column for private subnets. ### Create inbound rule ‚Äã To create an inbound rule allowing your private subnet access to your RDS instance: 1 On the left, under _Amazon RDS_, click on **Databases**. 2 From the _Databases_ table, click your instance's name under the _DB identifier_ column. 3 Under the _Connectivity & security_ tab, under the _Security_ column and the _VPC security groups_ heading click the link to your security group. 4 At the bottom of the screen, change to the **Inbound rules** tab and click the **Edit inbound rules** button. 5 At the bottom of the table, click the **Add rule** button and create the following rule: 1 For _Type_ use **PostgreSQL** if you are using the default port (5432), or use **Custom** and enter your port under _Port range_. 2 For _Source_ use **Custom** and enter your CIDR range (see Copy network settings). 3 Repeat these sub-steps for each of your CIDR ranges. 6 Below the table, click the **Save rules** button. ## (Optional) Create RDS proxy ‚Äã Create RDS proxy\") Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication This method uses a username and password to connect to the RDS database To create an RDS proxy for your RDS instance: 01 On the left, under _Amazon RDS_, click on **Proxies**. 02 In the upper right of the _Proxies_ table, click the **Create proxy** button. 03 Under _Proxy configuration_ enter the following details: 1 For _Engine family_ select **PostgreSQL**. 2",
    "metadata": {
      "topic": "Set up a private network link to PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__1",
    "text": "Under _Proxy configuration_ enter the following details: 1 For _Engine family_ select **PostgreSQL**. 2 For _Proxy identifier_ enter a meaningful name for your proxy. 04 Under _Target group configuration_ for _Database_ choose your RDS instance. 05 Under _Authentication_ for the _Secrets Manager secrets_: - If you have an existing secret for your RDS instance's database credentials, select it from the drop-down. - If not, click the **Create a new secret** link and enter these details in the new tab: 1 For _Secret type_ select **Credentials for Amazon RDS database**. 2 For _Credentials_ enter the _Username_ and _Password_ of the database user. 3 Under _Database_ select your RDS instance. 4 At the bottom of the form, click the **Next** button. 5 For _Secret name_ enter a name for the secret. 6 At the bottom of the form, click the **Next** button. 7 Leave the automatic secret rotation off and click the **Next** button. 8 Review the secret definition and click the **Store** button. 9 Return to the tab where you started creating the RDS proxy. 06 Under _Authentication_ for _IAM authentication_: - If IAM authentication is set to **Required**, Atlan will use an IAM role to connect to the RDS proxy. - If IAM authentication is set to **Not Allowed**, basic authentication will be enabled Atlan will use a username and password to connect to the RDS proxy. 07 Under _Connectivity_ expand the **Additional connectivity configuration**: 1 For _VPC security group_ select **Choose existing**. 2 For _Existing VPC security groups_ select the security group you edited with the inbound rules above. 08 At the bottom right of the form, click the **Create proxy** button. 09 From the _Proxies_ table, click the link for the proxy you just created. 10 Under _Proxy endpoints_ section, copy the hostname in the _Endpoint_ column. ## Create internal Network Load Balancer ‚Äã ### Retrieve IP address of the RDS ‚Äã From an EC2 instance in your AWS account, run the following command: ```codeBlockLines_e6Vv nslookup <endpoint> ``` - Replace `<endpoint>` with the fully-qualified endpoint hostname copied from the RDS proxy created above Copy the IP address that comes back from the command, under _Non-authoritative answer_ and to the right of _Address_. ### Start creating NLB ‚Äã To create an NLB, from within AWS: 1 Navigate to **Services**, then **Compute**, then **EC2**. 2 On the left, under _Load Balancing_, click on **Load Balancers**. 3 At the top of the screen, click the **Create Load Balancer** button. 4 Under the _Network Load Balancer_ option, click the **Create** button. 5 Enter the following _Basic configuration_ settings for the load balancer: 1 For _Load balancer name_ enter a unique name. 2 For _Scheme_ select **Internal**. 3 For _IP address type_ select **IPv4**. 6 Enter the following _Network mapping_ settings for the load balancer: 1 For _VPC_ select the VPC where the RDS instance is located (see Copy network settings). 2 For _Mappings_ select the availability zones with private subnets. 7",
    "metadata": {
      "topic": "Set up a private network link to PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__2",
    "text": "For _VPC_ select the VPC where the RDS instance is located (see Copy network settings). 2 For _Mappings_ select the availability zones with private subnets. 7 Enter the following _Listeners and routing_ settings for the load balancer: 1 For _Port_ enter **5432** (or the non-default port value from Copy network settings). 2 For _Default action_ click the **Create target group** link This will open the target group creation in a new browser tab. ### Create target group ‚Äã To create a target group for the NLB: 1 Enter the following _Basic configuration_ settings for the target group: 1 For _Choose target type_ select **IP addresses**. 2 For _Target group name_ enter a name. 3 For _Port_ enter **5432** (or the non-default port value from Copy network settings). 4 For _IP address type_ select **IPv4**. 5 For _VPC_ select the VPC where the RDS instance is located (see Copy network settings). 6 At the bottom of the form, click the **Next** button. 2 Enter the following _IP addresses_ settings for the target group: 1 For _Network_ select the VPC where the RDS instance is located (see Copy network settings). 2 For _IPv4 address_ enter the IP address returned by the _nslookup_ command (see Retrieve IP address of the RDS). 3 For _Ports_ enter **5432** (or the non-default port value from Copy network settings). 4 At the bottom of the _IP addresses_ section, click the **Include as pending below** button. 3 Confirm the following _Review targets_ settings for the target group: 1 Confirm _IP address_ matches the IP address returned by the _nslookup_ command. 2 Confirm _Port_ is 5432 (or the non-default port value used by your RDS instance). 4 At the bottom of the form, click the **Create target group** button. ### Finish creating NLB ‚Äã Return to the browser tab where you started the NLB creation, and continue: 1 Under _Listeners and routing_, click the refresh arrow to the far right of the _Default action_ drop-down box. 2 Select the target group you created above in the _Default action_ drop-down. 3 At the bottom of the form click the **Create load balancer** button. 4 In the resulting screen, click the **View load balancer** button. ### Verify target group is healthy ‚Äã To verify the target group is healthy: 1 From the EC2 menu on the left, under _Load Balancing_ click **Target Groups**. 2 From the _Target groups_ table, click the link to the target group you created above. 3 At the bottom of the screen, under the _Details_ tab, check that there is a 1 under both _Total targets_ and _Healthy_. ## Create endpoint service ‚Äã To create an endpoint service, from within AWS: 1 Navigate to **Services**, then **Networking & Content Delivery**, then **VPC**. 2 From the menu on the left, under _Virtual private cloud_ click **Endpoint services**. 3 At the top of the page, click the **Create endpoint service** button. 4 Enter the following _Endpoint service_ _settings_: 1 For _Name_ enter a meaningful name. 2",
    "metadata": {
      "topic": "Set up a private network link to PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__3",
    "text": "Enter the following _Endpoint service_ _settings_: 1 For _Name_ enter a meaningful name. 2 For _Load balancer type_ choose **Network**. 5 For _Available load balancers_ select the load balancer you created above in Create internal Network Load Balancer. 6 Enter the following _Additional settings_: 1 For _Require acceptance for endpoint_ enable **Acceptance required**. 2 For _Supported IP address types_ enable **IPv4**. 7 At the bottom of the form, click the **Create** button. **Did you know?** Under the _Details_ of the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format - `vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com` This is the hostname you will need to use to connect to the RDS instance from within Atlan. ## Allow Atlan account access ‚Äã To allow Atlan's account access to the service, from within the endpoint service screen: 1 At the bottom of the screen, change to the **Allow principals** tab. 2 At the top of the _Allow principals_ table, click the **Allow principals** button. 3 Under _Principals to add_ and _ARN_ enter the Atlan account ID. 4 At the bottom of the form, click the **Allow principals** button. ## Notify Atlan support ‚Äã Once all of the above steps are complete, contact Atlan support You will need to provide Atlan support: - The name of the endpoint service created in the previous step Go to _Endpoint Services_ and copy the \" **Service Name**\". - The RDS proxy or RDS endpoint DNS - if IAM authentication is enabled on your RDS proxy or RDS database, respectively Once this is done, there are additional steps that Atlan then needs to complete: - Creating a security group. - Creating an endpoint Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. ## Accept the consumer connection request ‚Äã To accept the consumer connection request, from within AWS: 1 Navigate to **Services**, then **Networking & Content Delivery**, then **VPC**. 2 From the menu on the left, under _Virtual private cloud_ click **Endpoint services**. 3 From the _Endpoint services_ table, select the endpoint service you created in Create endpoint service. 4 At the bottom of the screen, change to the **Endpoint connections** tab. 1 You should see a row in the _Endpoint connections_ table with a _State_ of _Pending acceptance_. 2 Select this row, and click the **Actions** button and then **Accept endpoint connection request**. 5 Wait for this to complete, it could take about 30 seconds. üòÖ The connection is now established You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl PostgreSQL in Atlan! üéâ - Prerequisites - Setup network to RDS (in AWS) - (Optional) Create RDS proxy - Create internal Network Load Balancer - Create endpoint service - Allow Atlan account access - Notify Atlan support - Accept the consumer connection request",
    "metadata": {
      "topic": "Set up a private network link to PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_postgresql_0",
    "text": "On this page Crawl PostgreSQL A Crawl PostgreSQL !Begin by adding a new workflow!The Postgres Assets package allows us to crawl metadata from PostgreSQL Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms Once you have configured the PostgreSQL user permissions, you can establish a connection between Atlan and PostgreSQL. (If you are using a private network for PostgreSQL, you will need to set that up first, too.) To crawl metadata from PostgreSQL, review the order of operations and then complete the following steps. ## Select the source ‚Äã To select PostgreSQL as your source: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the list of packages, select **Postgres Assets** and click on **Setup Workflow**. ## Provide credentials ‚Äã Choose your extraction method: - In **Direct** extraction, Atlan connects to your database and crawls metadata directly. - In **Offline** extraction, you need to first extract metadata yourself and make it available in S3. - In **Agent** extraction, Atlan's secure agent executes metadata extraction within the organization's environment. ### Direct extraction method ‚Äã To enter your PostgreSQL credentials: 1 For _Host_ enter the host for your PostgreSQL instance. 2 For _Port_ enter the port number of your PostgreSQL instance. 3 For _Authentication_ choose the method you configured when setting up the PostgreSQL user: - For **Basic** authentication, enter the _Username_ and _Password_ you configured in PostgreSQL. - For **IAM User** authentication, enter the _AWS Access Key_, _AWS Secret Key_, and database _Username_ you configured. - For **IAM Role** authentication, enter the _AWS Role ARN_ of the new role you created and database _Username_ you configured. (Optional) Enter the _AWS External ID_ only if you have not configured an external ID in the role definition. 4 For _Database_ enter the name of the database to crawl. 5 Click **Test Authentication** to confirm connectivity to PostgreSQL using these details. 6 When successful, at the bottom of the screen click **Next**. ### Offline extraction method ‚Äã Atlan also supports the offline extraction method for fetching metadata from PostgreSQL This method uses Atlan's metadata-extractor tool to fetch metadata You will need to first extract the metadata yourself and then make it available in S3 To enter your S3 details: 1 For _Bucket name_, enter the name of your S3 bucket or Atlan's bucket. 2 For _Bucket prefix_, enter the S3 prefix under which all the metadata files exist These include `database.json`, `columns-<database>.json`, and so on. 3 When complete, at the bottom of the screen click **Next**. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for fetching metadata from PostgreSQL To use a Secure Agent, follow these steps: 1 Select the **Agent** tab. 2 Configure the PostgreSQL data source by adding the secret keys for your secret store For details on the required fields, refer to the Direct extraction section. 3",
    "metadata": {
      "topic": "Crawl PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/crawl-postgresql",
      "keywords": [
        "crawl",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_postgresql_1",
    "text": "Configure the PostgreSQL data source by adding the secret keys for your secret store For details on the required fields, refer to the Direct extraction section. 3 Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. 4 Click **Next** after completing the configuration. ## Configure the connection ‚Äã To complete the PostgreSQL connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. 3 At the bottom of the screen, click **Next** to proceed. ## Configure the crawler ‚Äã Before running the PostgreSQL crawler, you can further configure it. (These options are only available when using the direct extraction method.) You can override the defaults for any of these options: - To select the assets you want to exclude from crawling, click **Exclude Metadata**. (This will default to no assets if none are specified.) - To select the assets you want to include in crawling, click **Include Metadata**. (This will default to all assets, if none are specified.) - To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the _Exclude regex for tables & views_ field. - For _Advanced Config_, keep _Default_ for the default configuration or click **Custom** to configure the crawler: - For _Enable Source Level Filtering_, click **True** to enable schema-level filtering at source or click **False** to disable it. - For _Use JDBC Internal Methods_, click **True** to enable JDBC internal methods for data extraction or click **False** to disable it. **Did you know?** If an asset appears in both the include and exclude filters, the exclude filter takes precedence. ## Run the crawler ‚Äã To run the PostgreSQL crawler, after completing the steps above: 1 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. 2 You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button Once the crawler has completed running, you will see the assets in Atlan's asset page! üéâ - Select the source - Provide credentials - Configure the connection - Configure the crawler - Run the crawler",
    "metadata": {
      "topic": "Crawl PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/crawl-postgresql",
      "keywords": [
        "crawl",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_postgresql_con_0",
    "text": "On this page #### Can Atlan crawl future tables created by any user? ‚Äã PostgreSQL does not provide a single command to grant access to future tables created by any user on a global level You will have to alter the default privileges of every current and future user that creates tables This is to ensure that the database role you created for integrating with Atlan has access to the tables created by those users by default For example: ```codeBlockLines_e6Vv ALTER DEFAULT PRIVILEGES FOR USER <USER_CREATING_TABLES> IN SCHEMA <SCHEMA> GRANT SELECT, REFERENCES ON TABLES TO atlan_user_role; ``` However, altering the default privileges of every current and future user may not be sustainable or controlled from a single place To automate the granting of permissions, you can: **Grant function** You can automate the granting of privileges to the database role you created for integrating with Atlan Note that the function below is located in the public schema You can use any schema you want to store this function: - Set custom conditions using PL/pgSQL to skip or allow only certain schemas or tables: ```codeBlockLines_e6Vv CREATE OR REPLACE FUNCTION public.grant_permissions_on_all_schemas() RETURNS void AS $$ DECLARE schema_name text; BEGIN FOR schema_name IN (SELECT nspname FROM pg_namespace WHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema') LOOP EXECUTE format('GRANT USAGE ON SCHEMA %I TO atlan_user_role', schema_name); -- grant access to all tables, including views, materialized views EXECUTE format('GRANT SELECT, REFERENCES ON ALL TABLES IN SCHEMA %I TO atlan_user_role', schema_name); END LOOP; END; $$ LANGUAGE plpgsql; ``` - Next, set up a periodic schedule and execute this function on a daily or hourly basis to ensure that the database role has access to all new schemas or tables: ```codeBlockLines_e6Vv select public.grant_permissions_on_all_schemas(); ``` **Event triggers** You can create an event trigger on any `CREATE SCHEMA` or `CREATE TABLE` command This automation will ensure minimal lag, and you will not have to set up a schedule to run the above grant function Note that the event trigger only listens to new create event triggers",
    "metadata": {
      "topic": "Troubleshooting PostgreSQL connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/troubleshooting/troubleshooting-postgresql-connectivity",
      "keywords": [
        "troubleshooting",
        "postgresql",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_postgresql_con_1",
    "text": "This automation will ensure minimal lag, and you will not have to set up a schedule to run the above grant function Note that the event trigger only listens to new create event triggers You will still need to run the grant function above to ensure that the database role has access to all current schemas or tables. ```codeBlockLines_e6Vv -- Function to grant permissions on a specific schema CREATE OR REPLACE FUNCTION public.grant_permissions_on_schema() RETURNS event_trigger AS $$ DECLARE obj record; BEGIN FOR obj IN SELECT * FROM pg_event_trigger_ddl_commands() WHERE command_tag = 'CREATE SCHEMA' LOOP EXECUTE format('GRANT USAGE ON SCHEMA %I TO atlan_user_role', obj.object_identity); RAISE NOTICE 'Granted USAGE on schema % to atlan_user_role', obj.object_identity; END LOOP; END; $$ LANGUAGE plpgsql; -- Event trigger for new schemas CREATE EVENT TRIGGER grant_permissions_on_new_schema ON ddl_command_end WHEN TAG IN ('CREATE SCHEMA') EXECUTE FUNCTION public.grant_permissions_on_schema(); -- Function to grant permissions on a specific table CREATE OR REPLACE FUNCTION public.grant_permissions_on_table() RETURNS event_trigger AS $$ DECLARE obj record; BEGIN FOR obj IN SELECT * FROM pg_event_trigger_ddl_commands() WHERE command_tag = 'CREATE TABLE' or command_tag = 'CREATE VIEW' or command_tag = 'CREATE TABLE AS' or command_tag = 'CREATE MATERIALIZED VIEW' LOOP EXECUTE format('GRANT SELECT, REFERENCES ON TABLE %s TO atlan_user_role', obj.object_identity); RAISE NOTICE 'Granted SELECT, REFERENCES on table % to atlan_user_role', obj.object_identity; END LOOP; END; $$ LANGUAGE plpgsql; -- Event trigger for new tables, views, mat views CREATE EVENT TRIGGER grant_permissions_on_new_table ON ddl_command_end WHEN TAG IN ('CREATE TABLE', 'CREATE VIEW', 'CREATE TABLE AS', 'CREATE MATERIALIZED VIEW') EXECUTE FUNCTION public.grant_permissions_on_table(); ```",
    "metadata": {
      "topic": "Troubleshooting PostgreSQL connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/troubleshooting/troubleshooting-postgresql-connectivity",
      "keywords": [
        "troubleshooting",
        "postgresql",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_postgresql_0",
    "text": "On this page Who can do this You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself. ## Create a database role ‚Äã To configure a database role for PostgreSQL, run the following commands: ```codeBlockLines_e6Vv CREATE role atlan_user_role; GRANT USAGE ON SCHEMA <schema> TO atlan_user_role; ``` - Replace `<schema>` with the schema to which the user should have access. danger You (or your administrator) will need to run these statements for each database and schema you want to crawl Atlan requires the following privileges: - `USAGE`: - Access a schema and fetch metadata By default, users cannot access any objects in schemas that they do not own The owner of a schema must grant the `USAGE` privilege on the schema to allow access. - Fetch the technical metadata persisted in the `INFORMATION_SCHEMA` These permissions enables Atlan to crawl metadat from PostgreSQL. ## (Optional) Grant permissions to query and preview data ‚Äã Grant permissions to query and preview data\") To grant permissions to query data and preview sample data: ```codeBlockLines_e6Vv GRANT SELECT, REFERENCES ON ALL TABLES IN SCHEMA schema_name TO atlan_user_role; ``` - Replace `schema_name`: Name of the schema you want Atlan to access. - Replace `atlan_user_role`: Role assigned to Atlan in your database The `SELECT` privilege is required to preview and query data from within Atlan. ## Choose authentication mechanism ‚Äã Atlan currently supports the following authentication mechanisms You will need to choose one and configure it according to the steps below. - Basic authentication - Identity and Access Management (IAM) authentication ## Basic authentication ‚Äã To create a username and password for basic authentication for PostgreSQL run the following commands: ```codeBlockLines_e6Vv CREATE USER atlan_user password '<pass>'; GRANT atlan_user_role TO atlan_user; ``` - Replace `<pass>` with the password for the `atlan_user` user you are creating. ## Identity and Access Management (IAM) authentication ‚Äã authentication\") To configure IAM authentication for PostgreSQL follow each of these steps. ### Enable IAM authentication ‚Äã To enable IAM authentication for your database instance follow the steps in the Amazon RDS documentation When given the option, apply the changes immediately and wait until they are complete. ### Create database user ‚Äã To create a database user with the necessary permissions run the following commands: 1 Connect to the database: ```codeBlockLines_e6Vv psql -h {{endpoint}} -U {{username}} -d {{database}} ``` - Replace `{{endpoint}}` with the database or cluster endpoint. - Replace `{{username}}` with the master username (admin account) for the database. - Replace `{{database}}` with the name of the database. 2 Create a database user: ```codeBlockLines_e6Vv CREATE USER {{db-username}} WITH LOGIN; GRANT atlan_user_role, rds_iam TO {{db-username}}; ``` - Replace `{{db-username}}` with the name for the database user to create. ### Create IAM policy ‚Äã To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide",
    "metadata": {
      "topic": "Set up PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-postgresql",
      "keywords": [
        "set",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_postgresql_1",
    "text": "Connect to the database: ```codeBlockLines_e6Vv psql -h {{endpoint}} -U {{username}} -d {{database}} ``` - Replace `{{endpoint}}` with the database or cluster endpoint. - Replace `{{username}}` with the master username (admin account) for the database. - Replace `{{database}}` with the name of the database. 2 Create a database user: ```codeBlockLines_e6Vv CREATE USER {{db-username}} WITH LOGIN; GRANT atlan_user_role, rds_iam TO {{db-username}}; ``` - Replace `{{db-username}}` with the name for the database user to create. ### Create IAM policy ‚Äã To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide Create the policy using the following JSON: ```codeBlockLines_e6Vv { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"rds-db:connect\" ], \"Resource\": [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } ``` - Replace `{{aws-region}}` with the AWS region of your database instance. - Replace `{{account-id}}` with your account ID. - Replace `{{resource-id}}` with the resource ID. - Replace `{{db-username}}` with the username created in the previous step. ### Attach IAM policy ‚Äã To attach the IAM policy for Atlan's use, you have two options: - **IAM role**: Create a new role in your AWS account and attach the policy to this role To create an AWS IAM role: 1 Follow the steps in the AWS Identity and Access Management User Guide. 2 When prompted for policies, attach the policy created in the previous step to this role. 3 Raise a support ticket to provide the AWS IAM role ARN to Atlan and get the ARN of the _Node Instance Role_ for your Atlan EKS cluster from Atlan. 4 When prompted, create a trust relationship for the role using the following trust policy. (Replace `<atlan_nodeinstance_role_arn>` with the ARN received from Atlan support.) ```codeBlockLines_e6Vv { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": {\"AWS\": \"<atlan_nodeinstance_role_arn>\"}, \"Action\": \"sts:AssumeRole\", } ] } ``` - **IAM user**: Create an AWS IAM user and attach the policy to this user To create an AWS IAM user: 1 Follow the steps in the AWS Identity and Access Management User Guide. 2 On the _Set permissions_ page, attach the policy created in the previous step to this user. 3 Once the user is created, view or download the user's _access key ID_ and _secret access key_. danger This will be your only opportunity to view or download the access keys You will not have access to them again after leaving the user creation screen. - Create a database role - (Optional) Grant permissions to query and preview data - Choose authentication mechanism - Basic authentication - Identity and Access Management (IAM) authentication",
    "metadata": {
      "topic": "Set up PostgreSQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-postgresql",
      "keywords": [
        "set",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_pos_0",
    "text": "On this page Atlan crawls and maps the following assets and properties from PostgreSQL Once you have crawled PostgreSQL, you can use connector-specific filters for quick asset discovery. ## Databases ‚Äã Atlan maps databases from PostgreSQL to its `Database` asset type. | Source property | Atlan property | | --- | --- | | `TABLE_CATALOG` | `name` | | `SCHEMA_COUNT` | `schemaCount` | ## Schemas ‚Äã Atlan maps schemas from PostgreSQL to its `Schema` asset type. | Source property | Atlan property | | --- | --- | | `TABLE_SCHEMA` | `name` | | `TABLE_COUNT` | `tableCount` | | `VIEW_COUNT` | `viewsCount` | | `TABLE_CATALOG` | `databaseName` | ## Tables ‚Äã Atlan maps tables from PostgreSQL to its `Table` asset type. | Source property | Atlan property | | --- | --- | | `TABLE_NAME` | `name` | | `REMARKS` | `description` | | `COLUMN_COUNT` | `columnCount` | | `ROW_COUNT` | `rowCount` | | `BYTES` | `sizeBytes` | | `TABLE_KIND (p)`, `TABLE_TYPE (PARTITIONED TABLE)` | `isPartitioned` | | `PARTITION_STRATEGY` | `partitionStrategy` | | `PARTITION_COUNT` | `partitionCount` | | `IS_INSERTABLE_INTO` | `is_insertable_into` | | `IS_TYPED` | `is_typed` | | `SELF_REFERENCING_COL_NAME` | `self_referencing_col_name` | | `REF_GENERATION` | `ref_generation` | | `IS_TRANSIENT` | `is_transient` | ## Table partitions ‚Äã Atlan maps table partitions from PostgreSQL to its `TablePartition` asset type. | Source property | Atlan property | | --- | --- | | `TABLE_NAME` | `name` | | `REMARKS` | `description` | | `COLUMN_COUNT` | `columnCount` | | `ROW_COUNT` | `rowCount` | | `BYTES` | `sizeBytes` | | `PARTITION_CONSTRAINT` | `constraint` | | `TABLE_KIND (p)`, `TABLE_TYPE (PARTITIONED TABLE)` | `isPartitioned` | | `PARTITION_STRATEGY` | `partitionStrategy` | | `PARTITION_COUNT` | `partitionCount` | | `IS_INSERTABLE_INTO` | `is_insertable_into` | | `IS_TYPED` | `is_typed` | | `SELF_REFERENCING_COL_NAME` | `self_referencing_col_name` | | `REF_GENERATION` | `ref_generation` | | `IS_TRANSIENT` | `is_transient` | ## Views ‚Äã Atlan maps views from PostgreSQL to its `View` asset type. | Source property | Atlan property | | --- | --- | | `TABLE_NAME` | `name` | | `REMARKS` | `description` | | `COLUMN_COUNT` | `columnCount` | | `VIEW_DEFINITION` | `definition` | | `IS_INSERTABLE_INTO` | `is_insertable_into` | | `IS_TYPED` | `is_typed` | | `SELF_REFERENCING_COL_NAME` | `self_referencing_col_name` | | `REF_GENERATION` | `ref_generation` | | `IS_TRANSIENT` | `is_transient` | ## Materialized views ‚Äã Atlan maps materialized views from PostgreSQL to its `MaterialisedView` asset type. | Source property | Atlan property | | --- | --- | | `TABLE_NAME` | `name` | | `REMARKS` | `description` | | `COLUMN_COUNT` | `columnCount` | | `ROW_COUNT` | `rowCount` | | `BYTES` | `sizeBytes` | | `VIEW_DEFINITION` | `definition` | ## Columns ‚Äã Atlan maps columns from PostgreSQL to its `Column` asset type. | Source property | Atlan property | | --- | --- | | `COLUMN_NAME` | `name` | | `REMARKS` | `description` | - Databases - Schemas - Tables - Table partitions - Views - Materialized views - Columns",
    "metadata": {
      "topic": "What does Atlan crawl from PostgreSQL?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/postgresql/references/what-does-atlan-crawl-from-postgresql",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "postgresql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__0",
    "text": "On this page Who can do this You will need your AWS administrator to complete these tasks - you may not have access yourself AWS PrivateLink creates a secure, private connection between services running in AWS This document describes the steps to set this up between MySQL (RDS) and Atlan. ## Prerequisites ‚Äã You should already have the following: - Your own non-default VPC configured in AWS. - A MySQL RDS instance running in AWS, linked to the non-default VPC. - Private subnets defined within the non-default VPC sufficient for availability. **Did you know?** You will also need Atlan's AWS account ID later in this process If you do not already have this, request it now from support. ## Setup network to RDS (in AWS) ‚Äã\") To setup the private network of your MySQL instance, from within AWS: ### Copy network settings ‚Äã 1 Navigate to **Services**, then **Database**, then **RDS**. 2 On the left, under _Amazon RDS_, click on **Databases**. 3 From the _Databases_ table, click your instance's name under the _DB identifier_ column. 4 Under the _Connectivity & security_ tab, copy the following values: 1. _Endpoint_ and _Port_ values 2. _VPC_ value 3. _Subnet group_ value 5 On the left, click **Subnet groups**. 6 From the table, click the row whose _Name_ matches the subnet group copied above. 7 From the _Subnets_ table, copy each value under the _CIDR block_ column for private subnets. ### Create inbound rule ‚Äã To create an inbound rule allowing your private subnet access to your RDS instance: 1 On the left, under _Amazon RDS_, click on **Databases**. 2 From the _Databases_ table, click your instance's name under the _DB identifier_ column. 3 Under the _Connectivity & security_ tab, under the _Security_ column and the _VPC security groups_ heading click the link to your security group. 4 At the bottom of the screen, change to the **Inbound rules** tab and click the **Edit inbound rules** button. 5 At the bottom of the table, click the **Add rule** button and create the following rule: 1 For _Type_ use **MySQL/Aurora** if you are using the default port (3306), or use **Custom** and enter your port under _Port range_. 2 For _Source_ use **Custom** and enter your CIDR range (see Copy network settings). 3 Repeat these sub-steps for each of your CIDR ranges. 6 Below the table, click the **Save rules** button. ## (Optional) Create RDS proxy ‚Äã Create RDS proxy\") Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication This method uses a username and password to connect to the RDS database To create an RDS proxy for your RDS instance: 01 On the left, under _Amazon RDS_, click on **Proxies**. 02 In the upper right of the _Proxies_ table, click the **Create proxy** button. 03 Under _Proxy configuration_ enter the following details: 1 For _Engine family_ select **MySQL**. 2 For _Proxy identifier_ enter a meaningful name for your proxy. 04",
    "metadata": {
      "topic": "Set up a private network link to MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__1",
    "text": "For _Engine family_ select **MySQL**. 2 For _Proxy identifier_ enter a meaningful name for your proxy. 04 Under _Target group configuration_ for _Database_ choose your RDS instance. 05 Under _Authentication_ for the _Secrets Manager secrets_: - If you have an existing secret for your RDS instance's database credentials, select it from the drop-down. - If not, click the **Create a new secret** link and enter these details in the new tab: 1 For _Secret type_ select **Credentials for Amazon RDS database**. 2 For _Credentials_ enter the _Username_ and _Password_ of the database user. 3 Under _Database_ select your RDS instance. 4 At the bottom of the form, click the **Next** button. 5 For _Secret name_ enter a name for the secret. 6 At the bottom of the form, click the **Next** button. 7 Leave the automatic secret rotation off and click the **Next** button. 8 Review the secret definition and click the **Store** button. 9 Return to the tab where you started creating the RDS proxy. 06 Under _Authentication_ for _IAM authentication_: - If IAM authentication is set to **Required**, Atlan will use an IAM role to connect to the RDS proxy. - If IAM authentication is set to **Not Allowed**, basic authentication will be enabled Atlan will use a username and password to connect to the RDS proxy. 07 Under _Connectivity_ expand the **Additional connectivity configuration**: 1 For _VPC security group_ select **Choose existing**. 2 For _Existing VPC security groups_ select the security group you edited with the inbound rules above. 08 At the bottom right of the form, click the **Create proxy** button. 09 From the _Proxies_ table, click the link for the proxy you just created. 10 Under _Proxy endpoints_ section, copy the hostname in the _Endpoint_ column. ## Create internal Network Load Balancer ‚Äã ### Retrieve IP address of the RDS ‚Äã From an EC2 instance in your AWS account, run the following command: ```codeBlockLines_e6Vv nslookup <endpoint> ``` - Replace `<endpoint>` with the fully-qualified endpoint hostname copied from the RDS endpoint or RDS proxy created above Copy the IP address that comes back from the command, under _Non-authoritative answer_ and to the right of _Address_. ### Start creating NLB ‚Äã To create an NLB, from within AWS: 1 Navigate to **Services**, then **Compute**, then **EC2**. 2 On the left, under _Load Balancing_, click on **Load Balancers**. 3 At the top of the screen, click the **Create Load Balancer** button. 4 Under the _Network Load Balancer_ option, click the **Create** button. 5 Enter the following _Basic configuration_ settings for the load balancer: 1 For _Load balancer name_ enter a unique name. 2 For _Scheme_ select **Internal**. 3 For _IP address type_ select **IPv4**. 6 Enter the following _Network mapping_ settings for the load balancer: 1 For _VPC_ select the VPC where the RDS instance is located (see Copy network settings). 2 For _Mappings_ select the availability zones with private subnets. 7 Enter the following _Listeners and routing_ settings for the load balancer: 1",
    "metadata": {
      "topic": "Set up a private network link to MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__2",
    "text": "For _Mappings_ select the availability zones with private subnets. 7 Enter the following _Listeners and routing_ settings for the load balancer: 1 For _Port_ enter **3306** (or the non-default port value from Copy network settings). 2 For _Default action_ click the **Create target group** link This will open the target group creation in a new browser tab. ### Create target group ‚Äã To create a target group for the NLB: 1 Enter the following _Basic configuration_ settings for the target group: 1 For _Choose target type_ select **IP addresses**. 2 For _Target group name_ enter a name. 3 For _Port_ enter **3306** (or the non-default port value from Copy network settings). 4 For _IP address type_ select **IPv4**. 5 For _VPC_ select the VPC where the RDS instance is located (see Copy network settings). 6 At the bottom of the form, click the **Next** button. 2 Enter the following _IP addresses_ settings for the target group: 1 For _Network_ select the VPC where the RDS instance is located (see Copy network settings). 2 For _IPv4 address_ enter the IP address returned by the _nslookup_ command (see Retrieve IP address of the RDS). 3 For _Ports_ enter **3306** (or the non-default port value from Copy network settings). 4 At the bottom of the _IP addresses_ section, click the **Include as pending below** button. 3 Confirm the following _Review targets_ settings for the target group: 1 Confirm _IP address_ matches the IP address returned by the _nslookup_ command. 2 Confirm _Port_ is 3306 (or the non-default port value used by your RDS instance). 4 At the bottom of the form, click the **Create target group** button. ### Finish creating NLB ‚Äã Return to the browser tab where you started the NLB creation, and continue: 1 Under _Listeners and routing_, click the refresh arrow to the far right of the _Default action_ drop-down box. 2 Select the target group you created above in the _Default action_ drop-down. 3 At the bottom of the form click the **Create load balancer** button. 4 In the resulting screen, click the **View load balancer** button. ### Verify target group is healthy ‚Äã To verify the target group is healthy: 1 From the EC2 menu on the left, under _Load Balancing_ click **Target Groups**. 2 From the _Target groups_ table, click the link to the target group you created above. 3 At the bottom of the screen, under the _Details_ tab, check that there is a 1 under both _Total targets_ and _Healthy_. ## Create endpoint service ‚Äã To create an endpoint service, from within AWS: 1 Navigate to **Services**, then **Networking & Content Delivery**, then **VPC**. 2 From the menu on the left, under _Virtual private cloud_ click **Endpoint services**. 3 At the top of the page, click the **Create endpoint service** button. 4 Enter the following _Endpoint service_ _settings_: 1 For _Name_ enter a meaningful name. 2 For _Load balancer type_ choose **Network**. 5",
    "metadata": {
      "topic": "Set up a private network link to MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_a_private_network_link__3",
    "text": "For _Name_ enter a meaningful name. 2 For _Load balancer type_ choose **Network**. 5 For _Available load balancers_ select the load balancer you created above in Create internal Network Load Balancer. 6 Enter the following _Additional settings_: 1 For _Require acceptance for endpoint_ enable **Acceptance required**. 2 For _Supported IP address types_ enable **IPv4**. 7 At the bottom of the form, click the **Create** button. **Did you know?** Under the _Details_ of the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format - `vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com` This is the hostname you will need to use to connect to the RDS instance from within Atlan. ## Allow Atlan account access ‚Äã To allow Atlan's account access to the service, from within the endpoint service screen: 1 At the bottom of the screen, change to the **Allow principals** tab. 2 At the top of the _Allow principals_ table, click the **Allow principals** button. 3 Under _Principals to add_ and _ARN_ enter the Atlan account ID and specified principal. 4 At the bottom of the form, click the **Allow principals** button. ## Notify Atlan support ‚Äã Once all of the above steps are complete, contact Atlan support You will need to provide Atlan support: - The RDS proxy or RDS endpoint DNS - if IAM authentication is enabled on your RDS proxy or RDS database, respectively Once this is done, there are additional steps that Atlan then needs to complete: - Creating a security group. - Creating an endpoint Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. ## Accept the consumer connection request ‚Äã To accept the consumer connection request, from within AWS: 1 Navigate to **Services**, then **Networking & Content Delivery**, then **VPC**. 2 From the menu on the left, under _Virtual private cloud_ click **Endpoint services**. 3 From the _Endpoint services_ table, select the endpoint service you created in Create endpoint service. 4 At the bottom of the screen, change to the **Endpoint connections** tab. 1 You should see a row in the _Endpoint connections_ table with a _State_ of _Pending acceptance_. 2 Select this row, and click the **Actions** button and then **Accept endpoint connection request**. 5 Wait for this to complete, it could take about 30 seconds. üòÖ The connection is now established You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl MySQL in Atlan! üéâ - Prerequisites - Setup network to RDS (in AWS) - (Optional) Create RDS proxy - Create internal Network Load Balancer - Create endpoint service - Allow Atlan account access - Notify Atlan support - Accept the consumer connection request",
    "metadata": {
      "topic": "Set up a private network link to MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
      "keywords": [
        "set",
        "private",
        "network",
        "link",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_mysql_0",
    "text": "On this page Once you have configured the MySQL user permissions, you can establish a connection between Atlan and MySQL. (If you are also using a private network for MySQL, you will need to set that up first, too.) Crawl MySQL A Crawl MySQL !Begin by adding a new workflow!The MySQL Assets package allows us to crawl metadata from MySQL Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms To crawl metadata from MySQL, review the order of operations and then complete the following steps. ## Select the source ‚Äã To select MySQL as your source: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the list of packages, select **MySQL Assets** and click on **Setup Workflow**. ## Provide credentials ‚Äã Choose your extraction method: - In **Direct** extraction, Atlan connects to your database and crawls metadata directly. - In **Offline** extraction, you need to first extract metadata yourself and make it available in S3. - In **Agent** extraction, Atlan's secure agent executes metadata extraction within the organization's environment. ### Direct extraction method ‚Äã To enter your MySQL credentials: 1 For _Host Name_ enter the host for your MySQL instance. 2 For _Port_ enter the port number of your MySQL instance. 3 For _Authentication_ choose the method you configured when setting up the MySQL user: - For **Basic** authentication, enter the _Username_ and _Password_ you configured in MySQL. - For **IAM User** authentication, enter the _AWS Access Key_, _AWS Secret Key_, and database _Username_ you configured. - For **IAM Role** authentication, enter the _AWS Role ARN_ of the new role you created and database _Username_ you configured. (Optional) Enter the _AWS External ID_ only if you have not configured an external ID in the role definition. 4 Click **Test Authentication** to confirm connectivity to MySQL using these details. info üí™ **Did you know?** If you get an `Error: 1129: Host ... is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts'`, ask your database admin to run the `FLUSH HOSTS;` command in the RDS instance, and then try again. 5 When successful, at the bottom of the screen click **Next**. ### Offline extraction method ‚Äã Atlan also supports the offline extraction method for fetching metadata from MySQL This method uses Atlan's metadata-extractor tool to fetch metadata You will need to first extract the metadata yourself and then make it available in S3 To enter your S3 details: 1 For _Bucket name_, enter the name of your S3 bucket or Atlan's bucket. 2 For _Bucket prefix_, enter the S3 prefix under which all the metadata files exist These include `databases.json`, `columns-<database>.json`, and so on. 3. (Optional) For _Bucket region_, enter the name of the S3 region. 4 When complete, at the bottom of the screen click **Next**. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for fetching metadata from MySQL To use a Secure Agent, follow these steps: 1",
    "metadata": {
      "topic": "Crawl MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/crawl-mysql",
      "keywords": [
        "crawl",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_mysql_1",
    "text": "When complete, at the bottom of the screen click **Next**. ### Agent extraction method ‚Äã Atlan supports using a Secure Agent for fetching metadata from MySQL To use a Secure Agent, follow these steps: 1 Select the **Agent** tab. 2 Configure the MySQL data source by adding the secret keys for your secret store For details on the required fields, refer to the Direct extraction section. 3 Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. 4 Click **Next** after completing the configuration. ## Configure the connection ‚Äã To complete the MySQL connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. 3. (Optional) To prevent users from querying any MySQL data, change _Allow SQL Query_ to **No**. 4. (Optional) To prevent users from previewing any MySQL data, change _Allow Data Preview_ to **No**. 5 At the bottom of the screen, click **Next** to proceed. ## Configure the crawler ‚Äã Before running the MySQL crawler, you can further configure it. (Some of the options may only be available when using the direct extraction method.) You can override the defaults for any of these options: - To select the assets you want to include in crawling, click **Include Metadata**. (This will default to all assets, if none are specified.) - To select the assets you want to exclude from crawling, click **Exclude Metadata**. (This will default to no assets if none are specified.) - To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the _Exclude regex for tables & views_ field. - For _Advanced Config_, keep _Default_ for the default configuration or click **Custom** to configure the crawler: - For _Enable Source Level Filtering_, click **True** to enable schema-level filtering at source or click **False** to disable it. - For _Use JDBC Internal Methods_, click **True** to enable JDBC internal methods for data extraction or click **False** to disable it. **Did you know?** If an asset appears in both the include and exclude filters, the exclude filter takes precedence. ## Run the crawler ‚Äã To run the MySQL crawler, after completing the steps above: 1 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. 2 You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button",
    "metadata": {
      "topic": "Crawl MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/crawl-mysql",
      "keywords": [
        "crawl",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_mys_0",
    "text": "Once you have crawled MySQL, you can use connector-specific filters for quick asset discovery. ## Databases ‚Äã Atlan maps databases from MySQL to its `Database` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_CATALOG` | `name` | asset preview and profile, overview sidebar | | `SCHEMA_COUNT` | `schemaCount` | asset filters | ## Schemas ‚Äã Atlan maps schemas from MySQL to its `Schema` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_SCHEMA` | `name` | asset preview and profile, overview sidebar | | `TABLE_COUNT` | `tableCount` | asset preview and profile | | `VIEW_COUNT` | `viewsCount` | asset preview and profile | | `TABLE_CATALOG` | `databaseName` | asset preview and profile, overview sidebar | ## Tables ‚Äã Atlan maps tables from MySQL to its `Table` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_NAME` | `name` | asset preview and profile, overview sidebar | | `REMARKS` | `description` | asset preview and profile, overview sidebar | | `COLUMN_COUNT` | `columnCount` | asset preview and profile, overview sidebar | | `ROW_COUNT` | `rowCount` | asset preview and profile, overview sidebar | | `BYTES` | `sizeBytes` | asset preview and profile, overview sidebar | | `TABLE_TYPE` | `subType` | API only | | `IS_PARTITION` | `isPartitioned` | API only | | `PARTITION_STRATEGY` | `partitionStrategy` | API only | | `PARTITION_COUNT` | `partitionCount` | API only | | `PARTITIONS` | `partitionList` | API only | | `CREATE_TIME` | `sourceCreatedAt` | asset preview and profile, properties sidebar | ## Views ‚Äã Atlan maps views from MySQL to its `View` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `TABLE_NAME` | `name` | asset preview and profile, overview sidebar | | `REMARKS` | `description` | asset preview and profile, overview sidebar | | `COLUMN_COUNT` | `columnCount` | asset preview and profile, overview sidebar | | `VIEW_DEFINITION` | `definition` | asset profile and overview sidebar | | `IS_PARTITION` | `isPartitioned` | API only | | `PARTITION_COUNT` | `partitionCount` | API only | | `CREATE_TIME` | `sourceCreatedAt` | asset preview and profile, properties sidebar | ## Columns ‚Äã Atlan maps columns from MySQL to its `Column` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `COLUMN_NAME` | `name` | asset profile and filter, overview sidebar | | `REMARKS` | `description` | asset profile and filter, overview sidebar | | `ORDINAL_POSITION` | `order` | parent asset profile | | `TYPE_NAME` | `dataType` | asset preview and filter, overview sidebar, parent asset profile | | `CONSTRAINT_TYPE (PRIMARY KEY)` | `isPrimary` | asset preview and filter, overview sidebar, parent asset profile | | `CONSTRAINT_TYPE (FOREIGN KEY)` | `isForeign` | asset preview and filter, overview sidebar, parent asset profile | | `IS_NULLABLE` | `isNullable` | API only | | `NUMERIC_SCALE` | `numericScale` | API only | | `CHARACTER_MAXIMUM_LENGTH` | `maxLength` | API only | ## Stored procedures ‚Äã Atlan maps stored procedures from MySQL to its `Procedure` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `PROCEDURE_NAME` | `name` | API only | | `REMARKS` | `description` | API only | | `PROCEDURE_TYPE` | `subType` | API only | | `ROUTINE_DEFINITION` | `definition` | API only | | `CREATED` | `sourceCreatedAt` | API only | | `LAST_ALTERED` | `sourceUpdatedAt` | API only | - Databases - Schemas - Tables - Views - Columns - Stored procedures",
    "metadata": {
      "topic": "What does Atlan crawl from MySQL?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/references/what-does-atlan-crawl-from-mysql",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_mysql_connecti_0",
    "text": "On this page #### Is using RDS Proxy recommended while connecting to RDS? ‚Äã Amazon RDS Proxy is a fully-managed, highly-available database proxy for Amazon Relational Database Service (RDS) It makes applications more scalable, secure, and resilient to database failures Amazon RDS Proxy sits between your application and relational database to efficiently manage connections to the database and improve scalability of the application When setting up a private network link to MySQL, Amazon RDS Proxy can help with: - Connection pooling - RDS Proxy helps manage database connections by pooling them and reusing them across multiple application connections This reduces the overhead of establishing new connections for each database request and improves performance. - Scalability - it automatically scales connection capacity based on demand, allowing applications to handle a high number of concurrent database connections without overwhelming the database instance It also helps distribute the workload across multiple database instances, enabling horizontal scalability. - Connection multiplexing - RDS Proxy multiplexes multiple database connections over a single secure connection This reduces the number of connections to the database, conserving system resources and reducing the chance of hitting connection limits. - Security - it integrates with AWS Identity and Access Management (IAM), allowing fine-grained control over who can access the proxy and underlying database It also supports Amazon virtual private cloud (VPC) endpoints, enhancing security through private connectivity For more questions, head over to Amazon RDS Proxy FAQs",
    "metadata": {
      "topic": "Troubleshooting MySQL connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/troubleshooting/troubleshooting-mysql-connectivity",
      "keywords": [
        "troubleshooting",
        "mysql",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_mysql_0",
    "text": "On this page Who can do this You will probably need your MySQL administrator to run these commands - you may not have access yourself. **Did you know?** Atlan supports both of the following AWS database engines - RDS MySQL and Aurora MySQL Currently we support the following authentication mechanisms You will need to choose one and configure it according to the steps below. - Basic authentication - Identity and Access Management (IAM) authentication ## Basic authentication ‚Äã To configure basic authentication for MySQL, run the following commands: ```codeBlockLines_e6Vv CREATE USER '{{db-username}}'@'%' IDENTIFIED BY '{{password}}'; GRANT SELECT,SHOW VIEW,EXECUTE ON *.* TO '{{db-username}}'@'%'; FLUSH PRIVILEGES; ``` - Replace `{{db-username}}` with the username you want to create. - Replace `{{password}}` with the password to be used for that username Atlan requires the following privileges to: - `SELECT`: - Fetch the technical metadata persisted in the `INFORMATION_SCHEMA`. `*.*` is required because `INFORMATION_SCHEMA` tables cannot be granted access directly Metadata is inferred from the access that the querying user has on the underlying tables. - Enable users to preview or query the underlying tables and views - this functionality can also be turned off. - `SHOW VIEW` enables the use of the `SHOW CREATE VIEW` statement to fetch view definitions for generating lineage. - `EXECUTE` is only required if using MySQL 5.7 and any earlier versions. ## Identity and Access Management (IAM) authentication ‚Äã authentication\") To configure IAM authentication for MySQL follow each of these steps. ### Enable IAM authentication ‚Äã To enable IAM authentication for your database instance: - For Amazon RDS, follow the steps in the Amazon RDS documentation. - For Aurora, follow the steps in the User Guide for Aurora documentation When given the option, apply the changes immediately and wait until they are complete. ### Create database user ‚Äã To create a database user with the necessary permissions run the following commands: ```codeBlockLines_e6Vv CREATE USER '{{db-username}}'@'%' IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS'; GRANT SELECT,SHOW VIEW,EXECUTE ON *.* TO '{{db-username}}'@'%'; FLUSH PRIVILEGES; ``` - Replace `{{db-username}}` with the username you want to create These permissions will allow you to crawl metadata, preview and query data from within Atlan. ### Create IAM policy ‚Äã To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide Create the policy using the following JSON: ```codeBlockLines_e6Vv { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"rds-db:connect\" ], \"Resource\": [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } ``` - Replace `{{aws-region}}` with the AWS region of your database instance. - Replace `{{account-id}}` with your account ID. - Replace `{{resource-id}}` with the resource ID. - Replace `{{db-username}}` with the username created in the previous step. ### Attach IAM policy ‚Äã To attach the IAM policy for Atlan's use, you have two options: - **IAM role**: Attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster",
    "metadata": {
      "topic": "Set up MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-mysql",
      "keywords": [
        "set",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_mysql_1",
    "text": "These permissions will allow you to crawl metadata, preview and query data from within Atlan. ### Create IAM policy ‚Äã To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide Create the policy using the following JSON: ```codeBlockLines_e6Vv { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"rds-db:connect\" ], \"Resource\": [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } ``` - Replace `{{aws-region}}` with the AWS region of your database instance. - Replace `{{account-id}}` with your account ID. - Replace `{{resource-id}}` with the resource ID. - Replace `{{db-username}}` with the username created in the previous step. ### Attach IAM policy ‚Äã To attach the IAM policy for Atlan's use, you have two options: - **IAM role**: Attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster Please raise a support ticket to use this option. - **IAM user**: Create an AWS IAM user and attach the policy to this user To create an AWS IAM user: 1 Follow the steps in the AWS Identity and Access Management User Guide. 2 On the _Set permissions_ page, attach the policy created in the previous step to this user. 3 Once the user is created, view or download the user's _access key ID_ and _secret access key_. danger This will be your only opportunity to view or download the access keys You will not have access to them again after leaving the user creation screen. - Basic authentication - Identity and Access Management (IAM) authentication",
    "metadata": {
      "topic": "Set up MySQL",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-mysql",
      "keywords": [
        "set",
        "mysql",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "troubleshooting_microsoft_powe_0",
    "text": "On this page #### What are the known limitations of the Microsoft Power BI connector? ‚Äã Atlan currently doesn't support the following: - Lineage between dataflows and reports - Microsoft Power BI Apps lineage support - Full lineage between workspaces and datasets or reports - Filtering hidden pages in reports - known limitation of PowerBI reports API - Crawling Microsoft Power BI tags - Crawling reports developed and published in Power BI Report Server #### What are the limitations of the Microsoft PowerBI Connector, when only admin APIs are used? ‚Äã When the Microsoft PowerBI Connector is configured to use only admin APIs, it results in reduced metadata extraction and limited lineage capabilities compared to using a combination of admin & non-admin APIs The following limitations apply as mentioned below: - PowerBI Pages (which are a part of PowerBI Report) won't be catalogued. - Downstream Lineage from PowerBI Table Column / Measure -> PowerBI Page won't be available. #### Can users who don't have access to a report still see the preview? ‚Äã Users can only see asset previews if the following conditions are met: - They have the necessary permissions in both Microsoft Power BI and Atlan. - They're logged into Atlan and Microsoft Power BI on the same browser Therefore, if a user lacks the permission to view a report in Microsoft Power BI, they won't be able to see the report preview in Atlan Even if they do have the necessary permissions, they need to be logged into Microsoft Power BI on the same browser as their Atlan instance for asset previews to work. #### Why can I not see previews for my Microsoft Power BI assets? ‚Äã Your Microsoft Power BI assets are updated with previews during the next run of your Microsoft Power BI workflow If you have run the workflow and still don't see the previews, rerun the workflow Once you've rerun the workflow, the previews are visible to all eligible users. #### How does Atlan calculate views for Microsoft Power BI reports and dashboards? ‚Äã Atlan calculates _Usage_ views based on the number of times users open a report or dashboard in Microsoft Power BI",
    "metadata": {
      "topic": "Troubleshooting Microsoft Power BI connectivity",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/troubleshooting/troubleshooting-microsoft-power-bi-connectivity",
      "keywords": [
        "troubleshooting",
        "microsoft",
        "power",
        "connectivity",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_microsoft_power_bi_0",
    "text": "On this page Who can do this Depending on the authentication method you choose, you may need a combination of your _Cloud Application Administrator_ or _Application Administrator_ for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and _Fabric Administrator_ ( formerly known as Power BI Administrator) for Microsoft Power BI to complete these tasks -> you may not have access yourself This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. ## Before you begin ‚Äã ### Register application in Microsoft Entra ID ‚Äã Who can do this You need your _Cloud Application Administrator_ or _Application Administrator_ to complete these steps‚Äî> you may not have access yourself This is required if the creation of registered applications isn't enabled for the entire organization To register a new application in Microsoft Entra ID: 01 Log in to the Azure portal. 02 Search for **Microsoft Entra ID** and select it. 03 Click **App registrations** from the left menu. 04 Click **+ New registration**. 05 Enter a name for your client application and click **Register**. 06 From the Overview screen, copy and securely store: - **Application (client) ID** - **Directory (tenant) ID** 07 Click **Certificates & secrets** from the left menu. 08 Under **Client secrets**, click **+ New client secret**. 09 Enter a description, select an expiry time, and click **Add**. 10 Copy and securely store the client secret **Value**. ### Create security group in Microsoft Entra ID ‚Äã Who can do this You need your _Cloud Application Administrator_ or _Application Administrator_ to complete these steps - you may not have access yourself To create a security group for your application: 1 Log in to the Azure portal. 2 Search for **Microsoft Entra ID** and select it. 3 Click **Groups** under the Manage section. 4 Click **New group**. 5 Set the Group type to **Security**. 6 Enter a Group name and optional description. 7 Click **No members selected**. 8 Add the appropriate member: - **For Delegated User authentication**: search for the user and select it. - **For Service Principal authentication**: search for the application registration created earlier and select it. 9 Click **Select** and then **Create** By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. ## Configure authentication options ‚Äã Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: ### Service principal authentication (recommended) ‚Äã\") When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage There are two supported options: #### Admin API only ‚Äã This option grants permissions that let the service principal to access only admin-level Power BI APIs In this mode, Atlan extracts metadata exclusively using administrative endpoints This option is recommended for stricter access control environments Who can do this",
    "metadata": {
      "topic": "Set up Microsoft Power BI",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi",
      "keywords": [
        "set",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_microsoft_power_bi_1",
    "text": "This option is recommended for stricter access control environments Who can do this You need your _Fabric Administrator_ ( formerly known as Power BI Administrator) to complete these tasks - you may not have access yourself To configure admin API access: 1 Log in to the Power BI admin portal. 2 Click **Tenant settings** under Admin portal. 3 Under **Admin API settings**: - Expand **Enable service principals to use read-only Power BI admin APIs** and set to **Enabled** - Add your security group under **Specific security groups** - Click **Apply** - Expand **Enhance admin APIs responses with detailed metadata** and set to **Enabled** - Add your security group - Click **Apply** - Expand **Enhance admin APIs responses with DAX and mashup expressions** and set to **Enabled** - Add your security group - Click **Apply** #### Admin and non-admin APIs ‚Äã This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. ##### Assign security group to Power BI workspaces in PowerBI service portal ‚Äã Who can do this You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself Make sure that you add the security group from the homepage and not the admin portal To assign a Microsoft Power BI workspace role to the security group: 1 Open the Microsoft Power BI homepage. 2 Open **Workspaces** and select the workspace you want to access from Atlan. 3 Click **Access**. 4 In the panel: - Enter the name of your security group where it says **Enter email addresses** - Choose one of the following roles: - **Viewer**: For workspaces without parameters - **Contributor**: For workspaces with semantic models containing parameters or to generate lineage for measures - **Member**: To generate lineage for dataflows - Click **Add**. ##### Configure admin and non-admin API access in PowerBI Service Portal ‚Äã Who can do this You need your _Fabric Administrator_ ( formerly known as Power BI Administrator) to complete these tasks - you may not have access yourself To enable both admin and non-admin API access: 1 Log in to the Power BI admin portal. 2 Click **Tenant settings** under Admin portal. 3 Under **Developer settings**: - Expand **Service principals can use Fabric APIs** and set to **Enabled** - Add your security group under **Specific security groups** - Click **Apply** 4",
    "metadata": {
      "topic": "Set up Microsoft Power BI",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi",
      "keywords": [
        "set",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "set_up_microsoft_power_bi_2",
    "text": "Click **Tenant settings** under Admin portal. 3 Under **Developer settings**: - Expand **Service principals can use Fabric APIs** and set to **Enabled** - Add your security group under **Specific security groups** - Click **Apply** 4 Under **Admin API settings**: - Expand **Enable service principals to use read-only Power BI admin APIs** and set to **Enabled** - Add your security group - Click **Apply** - Expand **Enhance admin APIs responses with detailed metadata** and set to **Enabled** - Add your security group - Click **Apply** - Expand **Enhance admin APIs responses with DAX and mashup expressions** and set to **Enabled** - Add your security group - Click **Apply** After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. ### Delegated user authentication ‚Äã info Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. #### Fabric administrator role assignment ‚Äã Who can do this You need your Microsoft 365 administrator to complete these steps - you may not have access yourself To assign the delegated user to the _Fabric Administrator_ role: 1 Open the Microsoft 365 admin portal. 2 Click **Users** and then **Active users** from the left menu. 3 Select the delegated user. 4 Under **Roles**, click **Manage roles**. 5 Expand **Show all by category**. 6 Under **Collaboration**, select **Fabric Administrator**. 7 Click **Save changes**. #### API permissions ‚Äã Who can do this You need your _Cloud Application Administrator_ or _Application Administrator_ to complete these steps, you may not have access yourself. danger The following permissions are only required for delegated user authentication If using service principal authentication, you don't need to configure any delegated permissions for a service principal‚Äîit's recommended that you avoid adding these permissions These are never used and can cause errors that may be hard to troubleshoot To add permissions for the registered application: 1 In your app registration, click **API permissions** under the Manage section. 2 Click **Add a permission**. 3 Search for and select **Power BI Service**. 4 Click **Delegated permissions** and select: - `Capacity.Read.All` - `Dataflow.Read.All` - `Dataset.Read.All` - `Report.Read.All` - `Tenant.Read.All` - `Workspace.Read.All` 5 Click **Grant Admin consent** (If you only see the _Add permissions_ button, you aren't an administrator). #### Admin API settings configuration ‚Äã Who can do this You need your _Fabric Administrator_ ( formerly known as Power BI Administrator) to complete these tasks, you may not have access yourself To enable the Microsoft Power BI admin API: 1 Log in to the Power BI admin portal. 2 Click **Tenant settings** under Admin portal. 3 Under **Admin API settings**: - Expand **Enhance admin APIs responses with detailed metadata** and set to **Enabled** - Add your security group - Click **Apply** - Expand **Enhance admin APIs responses with DAX and mashup expressions** and set to **Enabled** - Add your security group - Click **Apply**. - Before you begin - Configure authentication options",
    "metadata": {
      "topic": "Set up Microsoft Power BI",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi",
      "keywords": [
        "set",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "mine_microsoft_power_bi_0",
    "text": "On this page Once you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics To mine activity events from Microsoft Power BI, r Mine Power BI A Mine Power BI !Begin by adding a new workflow!Switch to the miner packages to find the miners more easily Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms eview the order of operations and then complete the following steps. ## Select the miner ‚Äã To select the Microsoft Power BI miner: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the filters along the top, click **Miner**. 3 From the list of packages, select **Power BI Miner** and then click **Setup Workflow**. ## Configure the miner ‚Äã To configure the Microsoft Power BI miner: 1 For _Connection_, select the connection to mine. (To select a connection, the crawler must have already run.) 2. (Optional) For _Advanced Config_, keep _Default_ for the default configuration or click **Advanced** to configure the miner: 1 For _Start time_, choose the earliest date from which to mine activity events. 2 For _Excluded Users_, type the names of users to be excluded while calculating usage metrics for Microsoft Power BI assets Press `enter` after each name to add more names. ## Run the miner ‚Äã To run the Microsoft Power BI miner, after completing the steps above: - To run the miner once immediately, at the bottom of the screen, click the **Run** button. - To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule & Run** button Once the miner has completed running, you will see usage metrics for Microsoft Power BI assets that were created in Microsoft Power BI between the start time and when the miner ran! üéâ - Select the miner - Configure the miner - Run the miner",
    "metadata": {
      "topic": "Mine Microsoft Power BI",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/mine-microsoft-power-bi",
      "keywords": [
        "mine",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_microsoft_power_bi_0",
    "text": "On this page Once you have configured the Microsoft Power BI user permissions, you can establish a connection between Atlan and Microsoft Power BI To crawl metadata from Microsoft Power BI, review Crawl Power BI A Crawl Power BI !Begin by adding a new workflow!The Power BI Assets package allows us to crawl metadata from Microsoft Power BI Interactive walkthrough reCAPTCHA Recaptcha requires verification Privacy - Terms protected by **reCAPTCHA** Privacy - Terms the order of operations and then complete the following steps. ## Select the source ‚Äã To select Microsoft Power BI as your source: 1 In the top right of any screen, navigate to **New** and then click **New Workflow**. 2 From the list of packages, select **Power BI Assets** and click on **Setup Workflow**. ## Provide credentials ‚Äã To enter your Microsoft Power BI credentials: 1 For _Authentication,_ choose the method you want to use to access Microsoft Power BI: - For **Service Principal** authentication, enter the _Tenant Id_, _Client Id_, and _Client Secret_ you configured when setting up Microsoft Power BI Use the **Enable Only Admin API Access** option to control how metadata is extracted When enabled, the crawler uses only admin APIs If disabled, both admin and non-admin APIs are used. - For **Delegated User** authentication, enter the _Username_, _Password_, _Tenant Id_, _Client Id_, and _Client Secret_ you configured when setting up Microsoft Power BI. 2 At the bottom of the form, click the **Test Authentication** button to confirm connectivity to Microsoft Power BI using these details. 3 Once successful, at the bottom of the screen click the **Next** button. ## Configure connection ‚Äã To complete the Microsoft Power BI connection configuration: 1 Provide a _Connection Name_ that represents your source environment For example, you might want to use values like `production`, `development`, `gold`, or `analytics`. 2. (Optional) To change the users able to manage this connection, change the users or groups listed under _Connection Admins_. danger If you don't specify any user or group, nobody can manage the connection - not even admins. 3 At the bottom of the screen, click the **Next** button to proceed. ## Configure the crawler ‚Äã Before running the Microsoft Power BI crawler, configure metadata extraction and advanced options You can override the default settings for the following fields. ### Configure metadata ‚Äã - **Include Workspaces**: Select Microsoft Power BI workspaces to include Defaults to all workspaces when left blank Use **Advanced Search** to filter workspaces using the following options: - **Contains**: Matches workspaces that contain the given substring. - **Starts with**: Matches workspaces that begin with the specified text. - **Ends with**: Matches workspaces that end with the specified text. - **Regex pattern**: Matches workspaces based on a regular expression. _All selected filters apply using an `AND` condition._ - **Exclude Workspaces**: Select workspaces to exclude",
    "metadata": {
      "topic": "Crawl Microsoft Power BI",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi",
      "keywords": [
        "crawl",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "crawl_microsoft_power_bi_1",
    "text": "Defaults to all workspaces when left blank Use **Advanced Search** to filter workspaces using the following options: - **Contains**: Matches workspaces that contain the given substring. - **Starts with**: Matches workspaces that begin with the specified text. - **Ends with**: Matches workspaces that end with the specified text. - **Regex pattern**: Matches workspaces based on a regular expression. _All selected filters apply using an `AND` condition._ - **Exclude Workspaces**: Select workspaces to exclude No workspaces are excluded by default. **Advanced Search** is also available for exclusion, with the same filtering options as mentioned previously. - **Include Dashboard and Reports Regex**: Use a regular expression to include dashboards and reports based on naming patterns Includes all by default. - **Exclude Dashboard and Reports Regex**: Use a regular expression to exclude dashboards and reports based on naming patterns Excludes none by default. - **Attach Endorsements from Power BI**: Automatically certify assets endorsed in Power BI To manually review before applying, change this setting to **Send a Request** For more details, see What does Atlan crawl from Microsoft Power BI? ### Configure advanced settings ‚Äã - **Source Connections**: When your tenant has multiple connections available for the same source system that share the similar metadata, confirm the advanced options and choose the correct connections from the `Source Connections` list drop down to avoid creating duplicate lineage to such connections. - **Enable ODBC DSN Connectivity Mapping**: Power BI provides multiple ways of connecting to a SQL source, including ODBC connectivity for building Reports and Dashboards When datasets are populated using ODBC, provide a mapping of the DSN ( Data Source Name ) names to their appropriate database qualified names after enabling this toggle. **Did you know?** If a workspace appears in both the include and exclude filters, the exclude filter takes precedence. ## Run the crawler ‚Äã To run the Microsoft Power BI crawler: 1 To check for any permissions or other configuration issues before running the crawler, click **Preflight checks**. 2 You can either: - To run the crawler once immediately, at the bottom of the screen, click the **Run** button. - To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the **Schedule Run** button Once the crawler has completed running, you can see the assets in Atlan's asset page! üéâ - Select the source - Provide credentials - Configure connection - Configure the crawler - Run the crawler",
    "metadata": {
      "topic": "Crawl Microsoft Power BI",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi",
      "keywords": [
        "crawl",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source",
        "connection"
      ]
    }
  },
  {
    "id": "what_lineage_does_atlan_extrac_0",
    "text": "On this page Atlan currently supports the following lineage for Microsoft Power BI: - Lineage between Microsoft Power BI assets crawled in Atlan - Upstream lineage to SQL warehouse assets, includes table- and column-level lineage for the following supported SQL sources: - Amazon Redshift - Databricks - Google BigQuery - Microsoft Azure Synapse Analytics - Microsoft SQL Server - MySQL - Oracle - Atlan generates lineage for the following methods of Oracle connectivity: - connection string - for example, `<host_name>:<port>/<service_name>` - connect descriptor - for example, `(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=<host_name>)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=<service_name>)))` - Lineage generation for TNS Alias connectivity is currently not supported. - SAP HANA - Snowflake - Teradata This document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to ensure seamless lineage generation. ## Lineage generation ‚Äã Atlan generates lineage for your Microsoft Power BI assets as follows: - You connect to a SQL data source such as Snowflake and extract relevant SQL tables to create a table in Microsoft Power BI for analysis. - Once the data has been loaded, you can perform Microsoft Power BI native operations as required. - Each table created in Microsoft Power BI and part of a dataset has a Power Query expression associated with it For example: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",\"EXAMPLE_WAREHOUSE\",[Role=\"EXAMPLE_ROLE\"]), EXAMPLE_DB = Source{[Name=\"EXAMPLE_DATABASE_NAME\",Kind=\"Database\"]}[Data], EXAMPLE_Sch = EXAMPLE_DB{[Name=\"EXAMPLE_SCHEMA_NAME\",Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=\"EXAMPLE_TABLE_NAME\",Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` - Atlan retrieves the Power Query expression as a plain string from the Microsoft Power BI API response. - Atlan's custom query parser then parses the Power Query expression to derive lineage between the upstream SQL tables and Microsoft Power BI table asset However, note that the Power Query expression can be modified in the Power Query Editor of the Power BI Desktop application These modifications may involve using parameter substitutes and variable naming patterns in the Power Query expression These modifications may lead to inconsistent behavior in Atlan's query parser This is because the latter is built on the standard format of a Power Query expression, without any modifications. ## Limitations of query parser ‚Äã To ensure seamless lineage generation, Atlan recommends the following when building tables in Microsoft Power BI. ### ODBC-based data sources ‚Äã Upstream lineage is not supported for ODBC-based data sources. ### Using parameters ‚Äã The Power Query expression associated with a table can be manually modified to serve different use cases For example, if you're creating multiple tables using data from the same database and schema at source, you may want to use dynamic M query parameters to substitute common values in Power Query expressions",
    "metadata": {
      "topic": "What lineage does Atlan extract from Microsoft Power BI?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-lineage-does-atlan-extract-from-microsoft-power-bi",
      "keywords": [
        "what",
        "lineage",
        "does",
        "atlan",
        "extract",
        "from",
        "microsoft",
        "power",
        "connectors",
        "integration"
      ]
    }
  },
  {
    "id": "what_lineage_does_atlan_extrac_1",
    "text": "This is because the latter is built on the standard format of a Power Query expression, without any modifications. ## Limitations of query parser ‚Äã To ensure seamless lineage generation, Atlan recommends the following when building tables in Microsoft Power BI. ### ODBC-based data sources ‚Äã Upstream lineage is not supported for ODBC-based data sources. ### Using parameters ‚Äã The Power Query expression associated with a table can be manually modified to serve different use cases For example, if you're creating multiple tables using data from the same database and schema at source, you may want to use dynamic M query parameters to substitute common values in Power Query expressions Atlan recommends the following: - Avoid using the following words to define your parameter names: - `Database` - `Schema` - `Table` - `View` - `Warehouse` - `Role` - Avoid including any spaces in your parameter names - for example, `( Example : Example DB )` For example, Atlan's query parser does not support the following: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",WarehouseName,[Role=\"EXAMPLE_ROLE\"]), DatabaseName = Source{[Name=DatabaseName,Kind=\"Database\"]}[Data], EXAMPLE_Sch = DatabaseName{[Name=SchemaName,Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=TableName,Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` - The above example includes `WarehouseName`, `DatabaseName`, `SchemaName`, and `TableName` as parameters, which are not supported in the query parser. ### Parameter syntax ‚Äã There are different formats for the syntax used in parameter names for Power Query expressions For example, `param_name`, `#‚Äùparam_name‚Äù`, or `#\"param name‚Äù` Atlan recommends the following for parameter names: - Use plain text format - Avoid any special characters - for example, `#`, `\"`, and more For example, Atlan's query parser does not support the following: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",\"EXAMPLE_WAREHOUSE\",[Role=\"EXAMPLE_ROLE\"]), DatabaseName = Source{[Name=#\"DatabaseName\",Kind=\"Database\"]}[Data], EXAMPLE_Sch = DatabaseName{[Name=\"EXAMPLE_SCHEMA_NAME\",Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=\"EXAMPLE_TABLE_NAME\",Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` - The above example includes `#\"DatabaseName\"` as parameter name, which is not supported in the query parser. ### Variable names ‚Äã While using parameters in Power Query expressions, ensure that the variable names do not match the parameter names For example, Atlan's query parser does not support the following: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",\"EXAMPLE_WAREHOUSE\",[Role=\"EXAMPLE_ROLE\"]), DatabaseName = Source{[Name=DatabaseName,Kind=\"Database\"]}[Data], EXAMPLE_Sch = DatabaseName{[Name=\"EXAMPLE_SCHEMA_NAME\",Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=\"EXAMPLE_TABLE_NAME\",Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` - In the above example, `DatabaseName` is used as both a parameter name and variable name, which is not supported in the query parser. ### User-defined expressions ‚Äã Parts of a Power Query expression can be parameterized and cross-referenced in other Power Query expressions Atlan's query parser currently only parses standard forms of Power Query expressions, hence these user-defined expressions are not supported",
    "metadata": {
      "topic": "What lineage does Atlan extract from Microsoft Power BI?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-lineage-does-atlan-extract-from-microsoft-power-bi",
      "keywords": [
        "what",
        "lineage",
        "does",
        "atlan",
        "extract",
        "from",
        "microsoft",
        "power",
        "connectors",
        "integration"
      ]
    }
  },
  {
    "id": "what_lineage_does_atlan_extrac_2",
    "text": "For example, Atlan's query parser does not support the following: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",\"EXAMPLE_WAREHOUSE\",[Role=\"EXAMPLE_ROLE\"]), DatabaseName = Source{[Name=DatabaseName,Kind=\"Database\"]}[Data], EXAMPLE_Sch = DatabaseName{[Name=\"EXAMPLE_SCHEMA_NAME\",Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=\"EXAMPLE_TABLE_NAME\",Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` - In the above example, `DatabaseName` is used as both a parameter name and variable name, which is not supported in the query parser. ### User-defined expressions ‚Äã Parts of a Power Query expression can be parameterized and cross-referenced in other Power Query expressions Atlan's query parser currently only parses standard forms of Power Query expressions, hence these user-defined expressions are not supported Example of a supported Power Query expression: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",\"EXAMPLE_WAREHOUSE\",[Role=\"EXAMPLE_ROLE\"]), EXAMPLE_DB = Source{[Name=\"EXAMPLE_DATABASE_NAME\",Kind=\"Database\"]}[Data], EXAMPLE_Sch = EXAMPLE_DB{[Name=\"EXAMPLE_SCHEMA_NAME\",Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=\"TBL_AGG_SALES_HT_POS_BEER\",Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` Example of an unsupported Power Query expression: ```codeBlockLines_e6Vv let Source = db_source, EXAMPLE_Sch = db_source{[Name=\"EXAMPLE_SCHEMA_NAME\",Kind=\"Schema\"]}[Data], EXAMPLE_Table_Var = EXAMPLE_Sch{[Name=\"EXAMPLE_TABLE_NAME\",Kind=\"Table\"]}[Data] in EXAMPLE_Table_Var ``` Example of a reference expression, parameterized as `db_source`: ```codeBlockLines_e6Vv let Source = Snowflake.Databases(\"example.snowflakecomputing.com\",\"EXAMPLE_WAREHOUSE\",[Role=\"EXAMPLE_ROLE\"]), EXAMPLE_DB = Source{[Name=\"EXAMPLE_DATABASE_NAME\",Kind=\"Database\"]}[Data] in EXAMPLE_DB ``` ### Table functions ‚Äã Atlan's custom query parser currently does not support parsing expressions with table functions. ### Power Query functions ‚Äã Upstream lineage is not supported when the data source expression involves the use of certain built-in Power Query functions The following functions are not supported: - `Csv.Document` - `DateTime.LocalNow` - `Excel.Workbook` - `Folder.Files` - `Json.Document` - `List.Dates` - `SharePoint.Files` - `SharePoint.Tables` - `UsageMetricsDataConnector.GetMetricsData` - `Value.NativeQuery` - `Xml.Tables` - Lineage generation - Limitations of query parser",
    "metadata": {
      "topic": "What lineage does Atlan extract from Microsoft Power BI?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-lineage-does-atlan-extract-from-microsoft-power-bi",
      "keywords": [
        "what",
        "lineage",
        "does",
        "atlan",
        "extract",
        "from",
        "microsoft",
        "power",
        "connectors",
        "integration"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_mic_0",
    "text": "Once you've crawled Microsoft Power BI, you can use connector-specific filters for quick asset discovery The following filters are currently supported for these assets: - Measures - External measure filter danger Currently Atlan only represents the assets marked with üîÄ in lineage For your Microsoft Power BI reports, Atlan also provides asset previews to help with quick discovery and give you the context you need. ## Apps Private Preview ‚Äã Atlan maps **Apps** from Microsoft Power BI to its `PowerBIApp` asset type. | Source Property | Atlan Property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `id` | `powerBIAppId` | properties sidebar | | `description` | `description` | asset profile and overview sidebar | | `publishedBy` | `sourceOwners` | overview sidebar | | `users ( displayName, appUserAccessRight )` | `powerBIAppUsers` | asset profile | | `groups ( displayName, appUserAccessRight )` | `powerBIAppGroups` | asset profile | ## Workspaces ‚Äã Atlan maps workspaces from Microsoft Power BI to its `PowerBIWorkspace` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `reportCount` | `reportCount` | asset preview and profile | | `dashboardCount` | `dashboardCount` | asset profile | | `datasetCount` | `datasetCount` | asset profile | | `dataflowCount` | `dataflowCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | ## Dashboards üîÄ ‚Äã Atlan maps dashboards from Microsoft Power BI to its `PowerBIDashboard` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `displayName` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `tileCount` | `tileCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | ## Data sources ‚Äã Atlan maps data sources from Microsoft Power BI to its `PowerBIDatasource` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `connectionDetails` | `connectionDetails` | overview sidebar | ## Datasets üîÄ ‚Äã Atlan maps datasets from Microsoft Power BI to its `PowerBIDataset` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `webUrl` | `sourceURL` | overview sidebar | | `configuredBy` | `sourceOwners` | asset profile and properties sidebar | | `createdDate` | `sourceCreatedAt` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | ## Dataflows üîÄ ‚Äã Atlan maps dataflows from Microsoft Power BI to its `PowerBIDataflow` asset type",
    "metadata": {
      "topic": "What does Atlan crawl from Microsoft Power BI?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-does-atlan-crawl-from-microsoft-power-bi",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_mic_1",
    "text": "The following filters are currently supported for these assets: - Measures - External measure filter danger Currently Atlan only represents the assets marked with üîÄ in lineage For your Microsoft Power BI reports, Atlan also provides asset previews to help with quick discovery and give you the context you need. ## Apps Private Preview ‚Äã Atlan maps **Apps** from Microsoft Power BI to its `PowerBIApp` asset type. | Source Property | Atlan Property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `id` | `powerBIAppId` | properties sidebar | | `description` | `description` | asset profile and overview sidebar | | `publishedBy` | `sourceOwners` | overview sidebar | | `users ( displayName, appUserAccessRight )` | `powerBIAppUsers` | asset profile | | `groups ( displayName, appUserAccessRight )` | `powerBIAppGroups` | asset profile | ## Workspaces ‚Äã Atlan maps workspaces from Microsoft Power BI to its `PowerBIWorkspace` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `reportCount` | `reportCount` | asset preview and profile | | `dashboardCount` | `dashboardCount` | asset profile | | `datasetCount` | `datasetCount` | asset profile | | `dataflowCount` | `dataflowCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | ## Dashboards üîÄ ‚Äã Atlan maps dashboards from Microsoft Power BI to its `PowerBIDashboard` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `displayName` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `tileCount` | `tileCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | ## Data sources ‚Äã Atlan maps data sources from Microsoft Power BI to its `PowerBIDatasource` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `connectionDetails` | `connectionDetails` | overview sidebar | ## Datasets üîÄ ‚Äã Atlan maps datasets from Microsoft Power BI to its `PowerBIDataset` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `webUrl` | `sourceURL` | overview sidebar | | `configuredBy` | `sourceOwners` | asset profile and properties sidebar | | `createdDate` | `sourceCreatedAt` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | ## Dataflows üîÄ ‚Äã Atlan maps dataflows from Microsoft Power BI to its `PowerBIDataflow` asset type Atlan currently only supports dataflow lineage for the following SQL sources: - Microsoft SQL Server - Oracle - SAP HANA - Snowflake | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `webUrl` | `sourceURL` | overview sidebar | | `configuredBy` | `sourceOwners` | asset profile and properties sidebar | | `modifiedDateTime` | `sourceUpdatedAt` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | | `modifiedBy` | `sourceUpdatedBy` | asset profile and properties sidebar | | `days` | `powerBIDataflowRefreshScheduleFrequency` | properties sidebar | | `times` | `powerBIDataflowRefreshScheduleTimes` | properties sidebar | | `localTimeZoneId` | `powerBIDataflowRefreshScheduleTimeZone` | properties sidebar | ## Dataflow entity columns üîÄ ‚Äã Atlan maps attributes of dataflow entities from Microsoft Power BI to its `PowerBIDataflowEntityColumn` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `attrbutes.name` | `name` | asset profile and overview sidebar | | `entities.name` | `powerBIDataflowEntityName` | overview sidebar | | `attributes.$type` | `powerBIDataflowEntityColumnDataType` | asset profile and overview sidebar | ## Reports üîÄ ‚Äã Atlan maps reports from Microsoft Power BI to its `PowerBIReport` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `pageCount` | `pageCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | | `createdDateTime` | `sourceCreatedAt` | asset profile and properties sidebar | | `modifiedDateTime` | `sourceUpdatedAt` | asset profile and properties sidebar | | `createdBy` | `sourceCreatedBy`, `sourceOwners` | asset profile and properties sidebar | | `modifiedBy` | `sourceUpdatedBy` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | ## Pages üîÄ ‚Äã Atlan maps pages from Microsoft Power BI to its `PowerBIPage` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `displayName` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `report_name` | `reportName` | API only | ## Tiles üîÄ ‚Äã Atlan maps tiles from Microsoft Power BI to its `PowerBITile` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `title` | `name` | asset profile and overview sidebar | | `subTitle` | `description` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `dashboard_name` | `dashboardName` | API only | ## Tables üîÄ ‚Äã Atlan maps tables from Microsoft Power BI to its `PowerBITable` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `isHidden` | `isHidden` | API only | | `sourceExpressions` | `powerBITableSourceExpressions` | API only | | `columnCount` | `powerBITableColumnCount` | asset profile and preview | | `measureCount` | `powerBITableMeasureCount` | asset profile and preview | ## Columns üîÄ ‚Äã Atlan maps columns from Microsoft Power BI to its `PowerBIColumn` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `isHidden` | `powerBIIsHidden` | API only | | `dataCategory` | `powerBIColumnDataCategory` | API only | | `dataType` | `powerBIColumnDataType` | asset preview and overview sidebar | | `formatString` | `powerBIFormatString` | API only | | `sortByColumn` | `powerBISortByColumn` | API only | | `summarizeBy` | `powerBIColumnSummarizeBy` | API only | ## Measures üîÄ ‚Äã Atlan maps measures from Microsoft Power BI to its `PowerBIMeasure` asset type",
    "metadata": {
      "topic": "What does Atlan crawl from Microsoft Power BI?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-does-atlan-crawl-from-microsoft-power-bi",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source"
      ]
    }
  },
  {
    "id": "what_does_atlan_crawl_from_mic_2",
    "text": "For your Microsoft Power BI reports, Atlan also provides asset previews to help with quick discovery and give you the context you need. ## Apps Private Preview ‚Äã Atlan maps **Apps** from Microsoft Power BI to its `PowerBIApp` asset type. | Source Property | Atlan Property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `id` | `powerBIAppId` | properties sidebar | | `description` | `description` | asset profile and overview sidebar | | `publishedBy` | `sourceOwners` | overview sidebar | | `users ( displayName, appUserAccessRight )` | `powerBIAppUsers` | asset profile | | `groups ( displayName, appUserAccessRight )` | `powerBIAppGroups` | asset profile | ## Workspaces ‚Äã Atlan maps workspaces from Microsoft Power BI to its `PowerBIWorkspace` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `reportCount` | `reportCount` | asset preview and profile | | `dashboardCount` | `dashboardCount` | asset profile | | `datasetCount` | `datasetCount` | asset profile | | `dataflowCount` | `dataflowCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | ## Dashboards üîÄ ‚Äã Atlan maps dashboards from Microsoft Power BI to its `PowerBIDashboard` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `displayName` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `tileCount` | `tileCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | ## Data sources ‚Äã Atlan maps data sources from Microsoft Power BI to its `PowerBIDatasource` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `connectionDetails` | `connectionDetails` | overview sidebar | ## Datasets üîÄ ‚Äã Atlan maps datasets from Microsoft Power BI to its `PowerBIDataset` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `webUrl` | `sourceURL` | overview sidebar | | `configuredBy` | `sourceOwners` | asset profile and properties sidebar | | `createdDate` | `sourceCreatedAt` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | ## Dataflows üîÄ ‚Äã Atlan maps dataflows from Microsoft Power BI to its `PowerBIDataflow` asset type Atlan currently only supports dataflow lineage for the following SQL sources: - Microsoft SQL Server - Oracle - SAP HANA - Snowflake | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `webUrl` | `sourceURL` | overview sidebar | | `configuredBy` | `sourceOwners` | asset profile and properties sidebar | | `modifiedDateTime` | `sourceUpdatedAt` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | | `modifiedBy` | `sourceUpdatedBy` | asset profile and properties sidebar | | `days` | `powerBIDataflowRefreshScheduleFrequency` | properties sidebar | | `times` | `powerBIDataflowRefreshScheduleTimes` | properties sidebar | | `localTimeZoneId` | `powerBIDataflowRefreshScheduleTimeZone` | properties sidebar | ## Dataflow entity columns üîÄ ‚Äã Atlan maps attributes of dataflow entities from Microsoft Power BI to its `PowerBIDataflowEntityColumn` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `attrbutes.name` | `name` | asset profile and overview sidebar | | `entities.name` | `powerBIDataflowEntityName` | overview sidebar | | `attributes.$type` | `powerBIDataflowEntityColumnDataType` | asset profile and overview sidebar | ## Reports üîÄ ‚Äã Atlan maps reports from Microsoft Power BI to its `PowerBIReport` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `description` | `description` | asset profile and overview sidebar | | `pageCount` | `pageCount` | asset profile | | `webUrl` | `sourceURL` | overview sidebar | | `createdDateTime` | `sourceCreatedAt` | asset profile and properties sidebar | | `modifiedDateTime` | `sourceUpdatedAt` | asset profile and properties sidebar | | `createdBy` | `sourceCreatedBy`, `sourceOwners` | asset profile and properties sidebar | | `modifiedBy` | `sourceUpdatedBy` | asset profile and properties sidebar | | `endorsementDetails` | `certificateStatus (VERIFIED)` | asset profile and overview sidebar | | `endorsementDetails (endorsement)` | `certificateStatusMessage`, `powerBIEndorsement` | asset profile and overview sidebar | | `endorsementDetails (certifiedBy)` | `certificateUpdatedBy` | asset profile and overview sidebar | ## Pages üîÄ ‚Äã Atlan maps pages from Microsoft Power BI to its `PowerBIPage` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `displayName` | `name` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `report_name` | `reportName` | API only | ## Tiles üîÄ ‚Äã Atlan maps tiles from Microsoft Power BI to its `PowerBITile` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `title` | `name` | asset profile and overview sidebar | | `subTitle` | `description` | asset profile and overview sidebar | | `workspace_name` | `workspaceName` | API only | | `dashboard_name` | `dashboardName` | API only | ## Tables üîÄ ‚Äã Atlan maps tables from Microsoft Power BI to its `PowerBITable` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `isHidden` | `isHidden` | API only | | `sourceExpressions` | `powerBITableSourceExpressions` | API only | | `columnCount` | `powerBITableColumnCount` | asset profile and preview | | `measureCount` | `powerBITableMeasureCount` | asset profile and preview | ## Columns üîÄ ‚Äã Atlan maps columns from Microsoft Power BI to its `PowerBIColumn` asset type. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `isHidden` | `powerBIIsHidden` | API only | | `dataCategory` | `powerBIColumnDataCategory` | API only | | `dataType` | `powerBIColumnDataType` | asset preview and overview sidebar | | `formatString` | `powerBIFormatString` | API only | | `sortByColumn` | `powerBISortByColumn` | API only | | `summarizeBy` | `powerBIColumnSummarizeBy` | API only | ## Measures üîÄ ‚Äã Atlan maps measures from Microsoft Power BI to its `PowerBIMeasure` asset type Atlan supports PowerBI Measures for downstream lineage to a PowerBI Page. | Source property | Atlan property | Where in Atlan | | --- | --- | --- | | `name` | `name` | asset profile and overview sidebar | | `description` | `description` | asset profile and overview sidebar | | `isHidden` | `powerBIIsHidden` | API only | | `expression` | `powerBIMeasureExpression` | overview sidebar | | `isExternalMeasure` | `powerBIIsExternalMeasure` | asset filter | | `formatString` | `powerBIFormatString` | API only | - Apps - Workspaces - Dashboards üîÄ - Data sources - Datasets üîÄ - Dataflows üîÄ - Dataflow entity columns üîÄ - Reports üîÄ - Pages üîÄ - Tiles üîÄ - Tables üîÄ - Columns üîÄ - Measures üîÄ",
    "metadata": {
      "topic": "What does Atlan crawl from Microsoft Power BI?",
      "category": "Connectors",
      "source_url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/references/what-does-atlan-crawl-from-microsoft-power-bi",
      "keywords": [
        "what",
        "does",
        "atlan",
        "crawl",
        "from",
        "microsoft",
        "power",
        "connectors",
        "integration",
        "data source"
      ]
    }
  }
]
